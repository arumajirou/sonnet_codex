# è©³ç´°è¦ä»¶å®šç¾©æ›¸
**Detailed Requirements Specification for Time Series Forecasting System**

---

## ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±

| é …ç›® | å†…å®¹ |
|-----|------|
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«** | æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆãƒ»è©•ä¾¡ãƒ»å†å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  è©³ç´°è¦ä»¶å®šç¾©æ›¸ |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** | v1.0.0 (è©³ç´°ç‰ˆ) |
| **ä½œæˆæ—¥** | 2025-11-03 |
| **æœ€çµ‚æ›´æ–°æ—¥** | 2025-11-03 |
| **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹** | æ‰¿èªå¾…ã¡ |
| **å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ ** | NeuralForecast Auto Runner + Time Series Forecasting System |
| **ã‚¹ã‚³ãƒ¼ãƒ—** | ãƒ‡ãƒ¼ã‚¿å–å¾— â†’ ç‰¹å¾´é‡ç”Ÿæˆ â†’ å­¦ç¿’ â†’ è©•ä¾¡ â†’ åˆ†æï¼ˆç›¸é–¢/å› æœ/å¯„ä¸åº¦ï¼‰ â†’ å†å­¦ç¿’ â†’ äºˆæ¸¬ â†’ å¯è¦–åŒ– â†’ é‹ç”¨ |
| **æƒ³å®šç’°å¢ƒ** | ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆPostgreSQL, File System, GPU/CPUï¼‰ã€ãƒ­ã‚°çµ±åˆï¼ˆW&B, MLflow, Ray, Lightning, Optunaï¼‰ |

---

## ç›®æ¬¡

1. [ç›®çš„ã¨èƒŒæ™¯](#1-ç›®çš„ã¨èƒŒæ™¯)
2. [ç”¨èªå®šç¾©](#2-ç”¨èªå®šç¾©)
3. [ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¨å½¹å‰²](#3-ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¨å½¹å‰²)
4. [å‰ææ¡ä»¶ã¨åˆ¶ç´„](#4-å‰ææ¡ä»¶ã¨åˆ¶ç´„)
5. [æ©Ÿèƒ½è¦ä»¶è©³ç´°](#5-æ©Ÿèƒ½è¦ä»¶è©³ç´°)
6. [éæ©Ÿèƒ½è¦ä»¶è©³ç´°](#6-éæ©Ÿèƒ½è¦ä»¶è©³ç´°)
7. [APIä»•æ§˜è©³ç´°](#7-apiä»•æ§˜è©³ç´°)
8. [ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒè©³ç´°](#8-ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒè©³ç´°)
9. [å“è³ªå±æ€§](#9-å“è³ªå±æ€§)
10. [åˆ¶ç´„äº‹é …](#10-åˆ¶ç´„äº‹é …)
11. [ãƒªã‚¹ã‚¯ã¨å¯¾ç­–](#11-ãƒªã‚¹ã‚¯ã¨å¯¾ç­–)
12. [ä»˜éŒ²](#12-ä»˜éŒ²)

---

## 1. ç›®çš„ã¨èƒŒæ™¯

### 1.1 ç›®çš„

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®**ç”Ÿæˆã€è©•ä¾¡ã€å†å­¦ç¿’ã€äºˆæ¸¬**ã‚’è‡ªå‹•åŒ–ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¾ã™ï¼š

#### ä¸»è¦ç›®çš„

1. **é«˜ç²¾åº¦ãªäºˆæ¸¬**: è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¢ç´¢ã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
2. **å†ç¾æ€§ã®ç¢ºä¿**: ã™ã¹ã¦ã®å®Ÿé¨“ã‚’å®Œå…¨ã«å†ç¾å¯èƒ½ãªçŠ¶æ…‹ã§è¨˜éŒ²
3. **åŠ¹ç‡çš„ãªé‹ç”¨**: é‡è¤‡å®Ÿè¡Œã®è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ã€ä¸¦åˆ—å®Ÿè¡Œã«ã‚ˆã‚‹é«˜é€ŸåŒ–
4. **åŒ…æ‹¬çš„ãªåˆ†æ**: ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ã€ç›¸é–¢ã€å› æœé–¢ä¿‚ã€å¯„ä¸åº¦ã‚’çµ±è¨ˆçš„ã«åˆ†æ
5. **æ‹¡å¼µæ€§ã®ç¢ºä¿**: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å®¹æ˜“ã«è¿½åŠ å¯èƒ½

---

### 1.2 èƒŒæ™¯

#### ç¾çŠ¶ã®èª²é¡Œ

1. **æ‰‹å‹•é‹ç”¨ã®è² æ‹…**: ãƒ¢ãƒ‡ãƒ«é¸æŠã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã€è©•ä¾¡ãŒæ‰‹å‹•
2. **å†ç¾æ€§ã®æ¬ å¦‚**: å®Ÿé¨“çµæœã‚’æ­£ç¢ºã«å†ç¾ã§ããªã„
3. **é‡è¤‡å®Ÿè¡Œã®ãƒ ãƒ€**: åŒã˜æ¡ä»¶ã§ã®å®Ÿé¨“ãŒç¹°ã‚Šè¿”ã•ã‚Œã‚‹
4. **åˆ†æã®å›°é›£**: ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã‚„å¯„ä¸åº¦ã®æŠŠæ¡ãŒä¸ååˆ†
5. **æ‹¡å¼µã®å›°é›£**: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ ã«å¤šå¤§ãªå·¥æ•°

#### è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

1. **è‡ªå‹•åŒ–**: ãƒ¢ãƒ‡ãƒ«æ¤œå‡ºã€å®Ÿé¨“è¨ˆç”»ã€å®Ÿè¡Œã€è©•ä¾¡ã‚’è‡ªå‹•åŒ–
2. **æ¨™æº–åŒ–**: å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆAdapterï¼‰ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«çµ±ä¸€
3. **ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°**: PostgreSQL + MLflow/W&Bã«ã‚ˆã‚‹åŒ…æ‹¬çš„è¨˜éŒ²
4. **ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ**: Fingerprintã«ã‚ˆã‚‹é‡è¤‡æ¤œå‡ºã¨ã‚¹ã‚­ãƒƒãƒ—
5. **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆ**: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

---

### 1.3 ã‚¹ã‚³ãƒ¼ãƒ—

#### å«ã¾ã‚Œã‚‹ç¯„å›²ï¼ˆIn Scopeï¼‰

- âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æº–å‚™ï¼ˆCSV scraping â†’ æ­£è¦åŒ–ï¼‰
- âœ… ç‰¹å¾´é‡ç”Ÿæˆãƒ»è©•ä¾¡ï¼ˆLag, MA, Seasonal, Exogï¼‰
- âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æ¢ç´¢ãƒ»è©•ä¾¡ï¼ˆNeuralForecast + æ‹¡å¼µå¯èƒ½ï¼‰
- âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆMAE, RMSE, sMAPE, MASE, MAPEï¼‰
- âœ… å› æœãƒ»ç›¸é–¢ãƒ»å¯„ä¸åº¦åˆ†æï¼ˆGranger, Permutation Importanceï¼‰
- âœ… å®Ÿé¨“ç®¡ç†ï¼ˆExperiment/Runæ§‹é€ ã€é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—ï¼‰
- âœ… å†å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå®šæœŸ/ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ï¼‰
- âœ… äºˆæ¸¬é…ä¿¡ï¼ˆãƒãƒƒãƒ/ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰
- âœ… ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»ï¼‰
- âœ… å¯è¦–åŒ–UIï¼ˆãƒªã‚½ãƒ¼ã‚¹è¡¨ç¤ºã€å­¦ç¿’ã€åˆ†æã€è©•ä¾¡ï¼‰
- âœ… ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆPostgreSQLå¿…é ˆã€MLflow/W&Bã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### å«ã¾ã‚Œãªã„ç¯„å›²ï¼ˆOut of Scopeï¼‰

- âŒ å¤–éƒ¨ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ»å¤§è¦æ¨¡åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®å¿…é ˆæ¡ç”¨
- âŒ çµ„ç¹”æ¨ªæ–­çš„ãªMLOpsã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰ï¼ˆå°†æ¥æ‹¡å¼µå¯èƒ½ï¼‰
- âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°äºˆæ¸¬ï¼ˆãƒãƒƒãƒä¸­å¿ƒï¼‰
- âŒ AutoMLå…¨èˆ¬ï¼ˆæ™‚ç³»åˆ—äºˆæ¸¬ã«ç‰¹åŒ–ï¼‰

---

## 2. ç”¨èªå®šç¾©

### 2.1 åŸºæœ¬ç”¨èª

| ç”¨èª | å®šç¾© | ä¾‹ |
|-----|------|---|
| **Run** | 1å›ã®å­¦ç¿’ï½è©•ä¾¡å®Ÿè¡Œ | å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ« + ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®å­¦ç¿’ |
| **Experiment** | åŒä¸€ç›®çš„ã®Runé›†åˆ | åŒã˜ãƒ‡ãƒ¼ã‚¿ãƒ»è©•ä¾¡è¨­è¨ˆã§ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ |
| **Adapter** | ãƒ¢ãƒ‡ãƒ«å®Ÿè£…ã®å·®ç•°ã‚’å¸åã™ã‚‹å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ | NeuralForecast, Darts, GluonTSç”¨ã®Adapter |
| **Registry** | ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»ç®¡ç† | Staging â†’ Production ã¸ã®æ˜‡æ ¼ |
| **Fingerprint** | å®Ÿè¡Œæ¡ä»¶ã®ä¸€æ„è­˜åˆ¥å­ï¼ˆãƒãƒƒã‚·ãƒ¥å€¤ï¼‰ | `run_fingerprint = hash(model, params, data, ...)` |
| **Artifact** | å®Ÿé¨“ã§ç”Ÿæˆã•ã‚Œã‚‹æˆæœç‰© | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã€äºˆæ¸¬çµæœã€ã‚°ãƒ©ãƒ•ã€ãƒ­ã‚° |
| **Backtest** | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã®äº¤å·®æ¤œè¨¼ | Rolling-origin, expanding window |
| **Horizon (h)** | äºˆæ¸¬æœŸé–“ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼‰ | h=7ï¼ˆ7ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§äºˆæ¸¬ï¼‰ |
| **Exogenous Variable** | å¤–ç”Ÿå¤‰æ•°ï¼ˆäºˆæ¸¬å¯¾è±¡ä»¥å¤–ã®èª¬æ˜å¤‰æ•°ï¼‰ | å¤©æ°—ã€ã‚¤ãƒ™ãƒ³ãƒˆã€ä¼‘æ—¥ãƒ•ãƒ©ã‚° |

---

### 2.2 ãƒ‡ãƒ¼ã‚¿é–¢é€£ç”¨èª

| ç”¨èª | å®šç¾© | ã‚¹ã‚­ãƒ¼ãƒ |
|-----|------|---------|
| **unique_id** | æ™‚ç³»åˆ—ã®è­˜åˆ¥å­ | TEXT NOT NULL |
| **ds** | æ—¥æ™‚ï¼ˆDateStampï¼‰ | TIMESTAMP/DATE NOT NULL |
| **y** | ç›®çš„å¤‰æ•°ï¼ˆäºˆæ¸¬å¯¾è±¡ï¼‰ | NUMERIC NOT NULL |
| **futr_exog** | æœªæ¥ã®å¤–ç”Ÿå¤‰æ•°ï¼ˆäºˆæ¸¬æ™‚ã«åˆ©ç”¨å¯èƒ½ï¼‰ | NUMERIC/TEXT/BOOL |
| **hist_exog** | éå»ã®å¤–ç”Ÿå¤‰æ•°ï¼ˆäºˆæ¸¬æ™‚ã«åˆ©ç”¨ä¸å¯ï¼‰ | NUMERIC/TEXT/BOOL |
| **stat_exog** | é™çš„ãªå¤–ç”Ÿå¤‰æ•°ï¼ˆæ™‚é–“ã«ä¾å­˜ã—ãªã„ï¼‰ | NUMERIC/TEXT/BOOL |

---

### 2.3 ãƒ¢ãƒ‡ãƒ«é–¢é€£ç”¨èª

| ç”¨èª | å®šç¾© | ä¾‹ |
|-----|------|---|
| **AutoModel** | NeuralForecastã®Auto*ã‚¯ãƒ©ã‚¹ | AutoNHITS, AutoLSTM, AutoTFT |
| **Loss Function** | æå¤±é–¢æ•° | MAE, MSE, QuantileLoss, Huber |
| **Scaler** | ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–æ‰‹æ³• | StandardScaler, MinMaxScaler, RobustScaler |
| **Search Algorithm** | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | Optuna, Ray Tune, Grid Search |
| **Backend** | è¨ˆç®—ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | CPU, CUDA, MPS |

---

### 2.4 è©•ä¾¡é–¢é€£ç”¨èª

| ç”¨èª | å®šç¾© | è¨ˆç®—å¼ |
|-----|------|--------|
| **MAE** | Mean Absolute Error | `mean(abs(y_true - y_pred))` |
| **RMSE** | Root Mean Squared Error | `sqrt(mean((y_true - y_pred)^2))` |
| **sMAPE** | Symmetric Mean Absolute Percentage Error | `mean(2 * abs(y_true - y_pred) / (abs(y_true) + abs(y_pred)))` |
| **MASE** | Mean Absolute Scaled Error | MAEã‚’ãƒŠã‚¤ãƒ¼ãƒ–äºˆæ¸¬ã®MAEã§ã‚¹ã‚±ãƒ¼ãƒ« |
| **MAPE** | Mean Absolute Percentage Error | `mean(abs((y_true - y_pred) / y_true))` |

---

## 3. ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¨å½¹å‰²

### 3.1 ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ä¸€è¦§

| å½¹å‰² | è²¬å‹™ | é–¢å¿ƒäº‹ | æ¨©é™ |
|-----|------|--------|------|
| **Product Owner (PO)** | KPIè¨­å®šã€ãƒªãƒªãƒ¼ã‚¹åˆ¤æ–­ | ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã€ROI | æ‰¿èªã€å„ªå…ˆé †ä½æ±ºå®š |
| **Data Scientist (DS)** | ç‰¹å¾´é‡è¨­è¨ˆã€è©•ä¾¡è¨­è¨ˆã€åˆ†æ | ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã€è§£é‡ˆæ€§ | å®Ÿé¨“è¨­è¨ˆã€åˆ†ææ‰‹æ³•é¸æŠ |
| **Machine Learning Engineer (MLE)** | ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè£…ã€Adapteré–‹ç™º | æ€§èƒ½ã€å¯ç”¨æ€§ã€ä¿å®ˆæ€§ | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®šã€æŠ€è¡“é¸å®š |
| **SRE/Infra** | ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã€ç›£è¦–ã€é‹ç”¨ | å®‰å®šæ€§ã€ã‚³ã‚¹ãƒˆ | ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆã€ç›£è¦–è¨­å®š |
| **User (Analyst/Ops)** | UIçµŒç”±ã§ã®æ“ä½œã€ãƒ¬ãƒãƒ¼ãƒˆé–²è¦§ | ä½¿ã„ã‚„ã™ã•ã€çµæœã®ä¿¡é ¼æ€§ | å®Ÿé¨“å®Ÿè¡Œã€ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ |

---

### 3.2 RACI ãƒãƒˆãƒªã‚¯ã‚¹

| ã‚¿ã‚¹ã‚¯ | PO | DS | MLE | SRE | User |
|-------|----|----|-----|-----|------|
| è¦ä»¶å®šç¾© | **A** | **R** | C | C | I |
| ç‰¹å¾´é‡è¨­è¨ˆ | C | **A/R** | C | I | I |
| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ | I | C | **A/R** | C | I |
| å®Ÿè£… | I | C | **A/R** | C | I |
| ã‚¤ãƒ³ãƒ•ãƒ©æ§‹ç¯‰ | I | I | C | **A/R** | I |
| ãƒ†ã‚¹ãƒˆ | I | C | **R** | C | I |
| ãƒ‡ãƒ—ãƒ­ã‚¤ | **A** | I | **R** | **R** | I |
| é‹ç”¨ | I | I | C | **A/R** | **R** |
| åˆ†æ | I | **A/R** | C | I | C |

**å‡¡ä¾‹**:
- **R** (Responsible): å®Ÿè¡Œè²¬ä»»è€…
- **A** (Accountable): èª¬æ˜è²¬ä»»è€…
- **C** (Consulted): ç›¸è«‡å…ˆ
- **I** (Informed): å ±å‘Šå…ˆ

---

## 4. å‰ææ¡ä»¶ã¨åˆ¶ç´„

### 4.1 æŠ€è¡“çš„å‰ææ¡ä»¶

#### 4.1.1 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢

| é …ç›® | è¦ä»¶ | æ¨å¥¨ | å‚™è€ƒ |
|-----|------|------|------|
| **CPU** | x86_64, 4ã‚³ã‚¢ä»¥ä¸Š | 16ã‚³ã‚¢ä»¥ä¸Š | ä¸¦åˆ—å®Ÿè¡Œæ™‚ |
| **ãƒ¡ãƒ¢ãƒª** | 16GBä»¥ä¸Š | 32GBä»¥ä¸Š | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ™‚ |
| **GPU** | CUDA 11.0+å¯¾å¿œ | RTX 3060ä»¥ä¸Š | PyTorchå¯¾å¿œGPU |
| **VRAM** | 8GBä»¥ä¸Š | 16GBä»¥ä¸Š | ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | 100GBä»¥ä¸Š | 500GBä»¥ä¸Š SSD | ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜ç”¨ |

---

#### 4.1.2 ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢

| é …ç›® | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | å¿…é ˆ/æ¨å¥¨ | å‚™è€ƒ |
|-----|-----------|----------|------|
| **Python** | 3.11+ | å¿…é ˆ | Type Hintså®Œå…¨å¯¾å¿œ |
| **PostgreSQL** | 14+ | å¿…é ˆ | ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ |
| **CUDA** | 11.0+ | æ¨å¥¨ | GPUä½¿ç”¨æ™‚ |
| **Docker** | 20.10+ | æ¨å¥¨ | ã‚³ãƒ³ãƒ†ãƒŠå®Ÿè¡Œæ™‚ |
| **Git** | 2.30+ | å¿…é ˆ | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† |

---

#### 4.1.3 Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒª

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ç”¨é€” |
|----------|-----------|------|
| **neuralforecast** | 1.6+ | æ™‚ç³»åˆ—äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« |
| **pytorch** | 2.0+ | ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ |
| **lightning** | 2.0+ | å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ |
| **ray** | 2.5+ | ä¸¦åˆ—å®Ÿè¡Œ |
| **optuna** | 3.2+ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– |
| **pandas** | 2.0+ | ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œ |
| **numpy** | 1.24+ | æ•°å€¤è¨ˆç®— |
| **scikit-learn** | 1.3+ | å‰å‡¦ç†ã€è©•ä¾¡ |
| **mlflow** | 2.5+ | å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ |
| **wandb** | 0.15+ | å®Ÿé¨“å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ |
| **psycopg2** | 2.9+ | PostgreSQLæ¥ç¶š |
| **sqlalchemy** | 2.0+ | ORM |
| **pydantic** | 2.0+ | ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ |
| **hydra-core** | 1.3+ | è¨­å®šç®¡ç† |
| **structlog** | 23.1+ | æ§‹é€ åŒ–ãƒ­ã‚° |

---

### 4.2 åˆ¶ç´„äº‹é …

#### 4.2.1 æŠ€è¡“çš„åˆ¶ç´„

| åˆ¶ç´„ | å†…å®¹ | å½±éŸ¿ | å›é¿ç­– |
|-----|------|------|--------|
| **ãƒ¡ãƒ¢ãƒªåˆ¶é™** | å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿æ™‚ã®OOM | ä¸¦åˆ—æ•°åˆ¶é™ | ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| **GPU VRAMåˆ¶é™** | ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™ | å­¦ç¿’é€Ÿåº¦ä½ä¸‹ | Gradient Accumulation, Mixed Precision |
| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¾å­˜** | å¤–éƒ¨APIï¼ˆMLflow, W&Bï¼‰ã®å¯ç”¨æ€§ | ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°å¤±æ•— | PostgreSQLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ |
| **ä¸¦åˆ—åº¦åˆ¶é™** | CPUã‚³ã‚¢æ•°ã€GPUæ•° | å®Ÿè¡Œæ™‚é–“å¢—åŠ  | å„ªå…ˆåº¦ä»˜ã‘ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° |

---

#### 4.2.2 ãƒ“ã‚¸ãƒã‚¹åˆ¶ç´„

| åˆ¶ç´„ | å†…å®¹ | å½±éŸ¿ |
|-----|------|------|
| **äºˆç®—åˆ¶é™** | ã‚¯ãƒ©ã‚¦ãƒ‰ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ä¸å¯ | ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œå¿…é ˆ |
| **ç´æœŸåˆ¶ç´„** | 3ãƒ¶æœˆä»¥å†…ã®ãƒªãƒªãƒ¼ã‚¹ | MVPæ©Ÿèƒ½ã«çµã‚‹ |
| **ã‚¹ã‚­ãƒ«åˆ¶ç´„** | DS/MLEã®ã‚¹ã‚­ãƒ«ãƒ¬ãƒ™ãƒ« | è¤‡é›‘ãªæ©Ÿèƒ½ã¯å¾Œå›ã— |
| **ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹** | PIIå‡¦ç†ä¸å¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ | ãƒ‡ãƒ¼ã‚¿åŒ¿ååŒ–ã¯åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ |

---

#### 4.2.3 ãƒ‡ãƒ¼ã‚¿åˆ¶ç´„

| åˆ¶ç´„ | å†…å®¹ | å¯¾å¿œ |
|-----|------|------|
| **ãƒ‡ãƒ¼ã‚¿å½¢å¼** | CSVå½¢å¼å¿…é ˆ | Parquetå¯¾å¿œã¯å°†æ¥æ‹¡å¼µ |
| **ã‚¹ã‚­ãƒ¼ãƒ** | `unique_id, ds, y`å¿…é ˆ | è‡ªå‹•æ¤œè¨¼ |
| **æ¬ æå€¤** | è¨±å®¹ã™ã‚‹ãŒè­¦å‘Š | è£œå®Œæ–¹æ³•ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠ |
| **æ™‚é–“ç²’åº¦** | æ—¥æ¬¡ï½æœˆæ¬¡ã‚’æƒ³å®š | ç§’å˜ä½ã¯æœªå¯¾å¿œ |

---

## 5. æ©Ÿèƒ½è¦ä»¶è©³ç´°

### 5.1 ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»ç®¡ç† (FR-DATA)

#### 5.1.1 ãƒ‡ãƒ¼ã‚¿å–å¾— (FR-DATA-001)

**æ¦‚è¦**: CSVå½¢å¼ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’èª­ã¿è¾¼ã¿ã€æ¨™æº–ã‚¹ã‚­ãƒ¼ãƒã«æ­£è¦åŒ–

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**è©³ç´°ä»•æ§˜**:

| é …ç›® | ä»•æ§˜ |
|-----|------|
| **å…¥åŠ›å½¢å¼** | CSVï¼ˆUTF-8, BOMç„¡ã—ï¼‰ |
| **å¿…é ˆã‚«ãƒ©ãƒ ** | `unique_id`, `ds`, `y` |
| **ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ ** | `futr_*`, `hist_*`, `stat_*` |
| **æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º** | 1GBï¼ˆãã‚Œä»¥ä¸Šã¯ãƒãƒ£ãƒ³ã‚¯èª­ã¿ï¼‰ |
| **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°** | UTF-8, Shift-JIS, EUC-JPè‡ªå‹•æ¤œå‡º |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class DataLoader:
    def load_csv(
        self,
        file_path: Path,
        *,
        encoding: Optional[str] = None,
        chunksize: Optional[int] = None,
        date_format: Optional[str] = None,
        parse_dates: bool = True,
        infer_datetime_format: bool = True,
    ) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ¨™æº–ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›
        
        Args:
            file_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            encoding: æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆNoneã§è‡ªå‹•æ¤œå‡ºï¼‰
            chunksize: ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºï¼ˆè¡Œæ•°ï¼‰
            date_format: æ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¾‹: '%Y-%m-%d'ï¼‰
            parse_dates: æ—¥æ™‚ã‚«ãƒ©ãƒ ã‚’è‡ªå‹•ãƒ‘ãƒ¼ã‚¹
            infer_datetime_format: æ—¥æ™‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ¨å®š
            
        Returns:
            pd.DataFrame: æ­£è¦åŒ–ã•ã‚ŒãŸDataFrame
            
        Raises:
            FileNotFoundError: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„
            ValueError: ã‚¹ã‚­ãƒ¼ãƒä¸æ­£ï¼ˆå¿…é ˆã‚«ãƒ©ãƒ æ¬ å¦‚ï¼‰
            EncodingError: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºå¤±æ•—
            
        Example:
            >>> loader = DataLoader()
            >>> df = loader.load_csv(Path("data.csv"))
            >>> df.columns
            Index(['unique_id', 'ds', 'y', 'futr_temp', 'hist_sales'])
        """
```

---

#### 5.1.2 ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ (FR-DATA-002)

**æ¦‚è¦**: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ¤œè¨¼ã—ã€å•é¡Œã‚’æ¤œå‡º

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**æ¤œè¨¼é …ç›®**:

| æ¤œè¨¼é …ç›® | æ¤œè¨¼å†…å®¹ | ã‚¨ãƒ©ãƒ¼/è­¦å‘Š | å¯¾å‡¦ |
|---------|---------|-----------|------|
| **ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼** | å¿…é ˆã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª | ã‚¨ãƒ©ãƒ¼ | å®Ÿè¡Œåœæ­¢ |
| **å‹æ¤œè¨¼** | `unique_id:str`, `ds:datetime`, `y:float` | ã‚¨ãƒ©ãƒ¼ | å‹å¤‰æ›è©¦è¡Œ |
| **æ¬ æå€¤æ¤œè¨¼** | NULLå€¤ã®å­˜åœ¨ç¢ºèª | è­¦å‘Š | ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› |
| **é‡è¤‡æ¤œè¨¼** | `(unique_id, ds)`ã®é‡è¤‡ | è­¦å‘Š | æœ€æ–°è¡Œã‚’ä¿æŒ |
| **æ™‚ç³»åˆ—é †åºæ¤œè¨¼** | `ds`ãŒæ˜‡é †ã‹ç¢ºèª | è­¦å‘Š | è‡ªå‹•ã‚½ãƒ¼ãƒˆ |
| **å¤–ã‚Œå€¤æ¤œè¨¼** | IQRæ³•ã§å¤–ã‚Œå€¤æ¤œå‡º | è­¦å‘Š | ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› |
| **æ™‚é–“é–“éš”æ¤œè¨¼** | `ds`ã®é–“éš”ãŒä¸€å®šã‹ | è­¦å‘Š | ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class DataValidator:
    def validate_schema(
        self,
        df: pd.DataFrame,
        *,
        required_columns: List[str] = ["unique_id", "ds", "y"],
        allow_extra_columns: bool = True,
    ) -> ValidationResult:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¹ã‚­ãƒ¼ãƒã‚’æ¤œè¨¼
        
        Args:
            df: æ¤œè¨¼å¯¾è±¡DataFrame
            required_columns: å¿…é ˆã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ
            allow_extra_columns: è¿½åŠ ã‚«ãƒ©ãƒ ã‚’è¨±å¯
            
        Returns:
            ValidationResult: æ¤œè¨¼çµæœ
                - is_valid: bool
                - errors: List[ValidationError]
                - warnings: List[ValidationWarning]
                
        Example:
            >>> validator = DataValidator()
            >>> result = validator.validate_schema(df)
            >>> if not result.is_valid:
            ...     raise ValueError(result.errors)
        """
    
    def detect_outliers(
        self,
        df: pd.DataFrame,
        *,
        column: str = "y",
        method: Literal["iqr", "zscore", "isolation_forest"] = "iqr",
        threshold: float = 1.5,
    ) -> pd.Series:
        """
        å¤–ã‚Œå€¤ã‚’æ¤œå‡º
        
        Args:
            df: DataFrame
            column: æ¤œè¨¼å¯¾è±¡ã‚«ãƒ©ãƒ 
            method: æ¤œå‡ºæ‰‹æ³•
                - "iqr": IQRæ³•ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
                - "zscore": Z-scoreæ³•
                - "isolation_forest": Isolation Forest
            threshold: é–¾å€¤ï¼ˆIQR: 1.5, Z-score: 3.0æ¨å¥¨ï¼‰
            
        Returns:
            pd.Series: å¤–ã‚Œå€¤ãƒ•ãƒ©ã‚°ï¼ˆTrue=å¤–ã‚Œå€¤ï¼‰
            
        Example:
            >>> outliers = validator.detect_outliers(df, method="iqr")
            >>> df_clean = df[~outliers]
        """
```

---

#### 5.1.3 ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° (FR-DATA-003)

**æ¦‚è¦**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ä¸€æ„ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³IDã‚’ä»˜ä¸ã—ã€å†ç¾æ€§ã‚’ç¢ºä¿

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°æ–¹å¼**:

```python
# ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®è¨ˆç®—
dataset_version = hashlib.sha256(
    sorted_rows_without_leaky_cols.to_csv(index=False).encode()
).hexdigest()[:16]
```

**é™¤å¤–ã‚«ãƒ©ãƒ ** (ãƒªãƒ¼ã‚¯é˜²æ­¢):
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ ï¼ˆ`created_at`, `updated_at`, `id`ãªã©ï¼‰
- äºˆæ¸¬çµæœã‚«ãƒ©ãƒ ï¼ˆ`y_pred`, `residual`ãªã©ï¼‰

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class DataVersionManager:
    def compute_version(
        self,
        df: pd.DataFrame,
        *,
        exclude_columns: Optional[List[str]] = None,
        sort_by: Optional[List[str]] = None,
    ) -> str:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        
        Args:
            df: DataFrame
            exclude_columns: é™¤å¤–ã‚«ãƒ©ãƒ ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰
            sort_by: ã‚½ãƒ¼ãƒˆåŸºæº–ã‚«ãƒ©ãƒ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: unique_id, dsï¼‰
            
        Returns:
            str: 16æ¡ã®ãƒãƒƒã‚·ãƒ¥å€¤
            
        Example:
            >>> manager = DataVersionManager()
            >>> version = manager.compute_version(df)
            >>> version
            'a3f8c921d4e6b5f2'
        """
    
    def register_version(
        self,
        version: str,
        df: pd.DataFrame,
        *,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’DBã«ç™»éŒ²
        
        Args:
            version: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚·ãƒ¥
            df: DataFrame
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆè¡Œæ•°ã€ã‚«ãƒ©ãƒ æ•°ã€çµ±è¨ˆãªã©ï¼‰
            
        Raises:
            DuplicateVersionError: åŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒæ—¢ã«å­˜åœ¨
            
        Example:
            >>> manager.register_version(
            ...     version="a3f8c921d4e6b5f2",
            ...     df=df,
            ...     metadata={"source": "scraping_2025-11-03.csv"}
            ... )
        """
```

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ**:

```sql
CREATE TABLE datasets (
    id SERIAL PRIMARY KEY,
    dataset_version VARCHAR(16) UNIQUE NOT NULL,
    file_path TEXT NOT NULL,
    file_hash VARCHAR(64) NOT NULL,
    row_count INTEGER NOT NULL,
    column_count INTEGER NOT NULL,
    unique_id_count INTEGER NOT NULL,
    date_range_start TIMESTAMP,
    date_range_end TIMESTAMP,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_dataset_version (dataset_version)
);
```

---

#### 5.1.4 ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆãƒ»ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° (FR-DATA-004)

**æ¦‚è¦**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—ã—ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

**å„ªå…ˆåº¦**: ä¸­

**çµ±è¨ˆé …ç›®**:

| ã‚«ãƒ†ã‚´ãƒª | é …ç›® | è¨ˆç®—æ–¹æ³• |
|---------|------|----------|
| **åŸºæœ¬çµ±è¨ˆ** | è¡Œæ•°ã€ã‚«ãƒ©ãƒ æ•°ã€unique_idæ•° | `len(df)`, `len(df.columns)`, `df['unique_id'].nunique()` |
| **æ¬ æå€¤** | ã‚«ãƒ©ãƒ ã”ã¨ã®æ¬ æç‡ | `df.isnull().mean()` |
| **æ•°å€¤çµ±è¨ˆ** | å¹³å‡ã€ä¸­å¤®å€¤ã€æ¨™æº–åå·®ã€min, max | `df.describe()` |
| **æ™‚ç³»åˆ—çµ±è¨ˆ** | æœ€å°æ—¥æ™‚ã€æœ€å¤§æ—¥æ™‚ã€æœŸé–“ | `df['ds'].min()`, `df['ds'].max()` |
| **å‘¨æœŸæ€§** | æ¨å®šå‘¨æœŸï¼ˆæ—¥æ¬¡ã€é€±æ¬¡ã€æœˆæ¬¡ãªã©ï¼‰ | FFT, ACF |
| **ãƒˆãƒ¬ãƒ³ãƒ‰** | ãƒˆãƒ¬ãƒ³ãƒ‰æœ‰ç„¡ã€å­£ç¯€æ€§æœ‰ç„¡ | STLåˆ†è§£ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class DataProfiler:
    def profile(
        self,
        df: pd.DataFrame,
        *,
        compute_correlations: bool = True,
        detect_seasonality: bool = True,
        n_samples: Optional[int] = None,
    ) -> DataProfile:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        
        Args:
            df: DataFrame
            compute_correlations: ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
            detect_seasonality: å­£ç¯€æ€§ã‚’æ¤œå‡º
            n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
            
        Returns:
            DataProfile: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœ
                - basic_stats: Dict[str, Any]
                - missing_values: pd.Series
                - numeric_stats: pd.DataFrame
                - temporal_stats: Dict[str, Any]
                - correlations: Optional[pd.DataFrame]
                - seasonality: Optional[Dict[str, Any]]
                
        Example:
            >>> profiler = DataProfiler()
            >>> profile = profiler.profile(df)
            >>> print(f"è¡Œæ•°: {profile.basic_stats['row_count']}")
            >>> print(f"æ¬ æç‡: {profile.missing_values['y']:.2%}")
        """
```

---

### 5.2 ç‰¹å¾´é‡ç”Ÿæˆãƒ»è©•ä¾¡ (FR-FEATURE)

#### 5.2.1 ç‰¹å¾´é‡ç”Ÿæˆ (FR-FEATURE-001)

**æ¦‚è¦**: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«ç‰¹å¾´é‡ã‚’ç”Ÿæˆ

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—**:

| ã‚¿ã‚¤ãƒ— | èª¬æ˜ | ç”Ÿæˆä¾‹ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|-------|------|--------|-----------|
| **Lag Features** | éå»NæœŸã®ãƒ©ã‚°ç‰¹å¾´é‡ | `y_lag_1`, `y_lag_7` | `lags: List[int]` |
| **Rolling Statistics** | ç§»å‹•çµ±è¨ˆé‡ | `y_rolling_mean_7`, `y_rolling_std_7` | `windows: List[int]`, `functions: List[str]` |
| **Seasonal Dummies** | å­£ç¯€ãƒ€ãƒŸãƒ¼å¤‰æ•° | `month_1`, `weekday_0`, `hour_12` | `freq: str`, `drop_first: bool` |
| **Holiday Features** | ä¼‘æ—¥ãƒ•ãƒ©ã‚° | `is_holiday`, `is_weekend` | `country: str`, `holidays: List[str]` |
| **Exogenous Features** | å¤–ç”Ÿå¤‰æ•° | `temperature`, `promotion_flag` | ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®š |
| **Fourier Features** | ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ç‰¹å¾´é‡ | `sin_annual_1`, `cos_annual_1` | `periods: List[float]`, `orders: List[int]` |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class FeatureGenerator:
    def generate_lag_features(
        self,
        df: pd.DataFrame,
        *,
        target_col: str = "y",
        lags: List[int] = [1, 2, 3, 7, 14, 21],
        group_by: Optional[str] = "unique_id",
    ) -> pd.DataFrame:
        """
        ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
        
        Args:
            df: DataFrame
            target_col: å¯¾è±¡ã‚«ãƒ©ãƒ 
            lags: ãƒ©ã‚°æœŸé–“ãƒªã‚¹ãƒˆ
            group_by: ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã‚«ãƒ©ãƒ 
            
        Returns:
            pd.DataFrame: ãƒ©ã‚°ç‰¹å¾´é‡ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
            
        Example:
            >>> generator = FeatureGenerator()
            >>> df_with_lags = generator.generate_lag_features(
            ...     df, lags=[1, 7, 14]
            ... )
            >>> df_with_lags.columns
            Index(['unique_id', 'ds', 'y', 'y_lag_1', 'y_lag_7', 'y_lag_14'])
        """
    
    def generate_rolling_features(
        self,
        df: pd.DataFrame,
        *,
        target_col: str = "y",
        windows: List[int] = [7, 14, 30],
        functions: List[str] = ["mean", "std", "min", "max"],
        group_by: Optional[str] = "unique_id",
        min_periods: Optional[int] = None,
    ) -> pd.DataFrame:
        """
        ç§»å‹•çµ±è¨ˆé‡ã‚’ç”Ÿæˆ
        
        Args:
            df: DataFrame
            target_col: å¯¾è±¡ã‚«ãƒ©ãƒ 
            windows: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãƒªã‚¹ãƒˆ
            functions: çµ±è¨ˆé–¢æ•°ãƒªã‚¹ãƒˆ
                - "mean": å¹³å‡
                - "std": æ¨™æº–åå·®
                - "min": æœ€å°å€¤
                - "max": æœ€å¤§å€¤
                - "median": ä¸­å¤®å€¤
                - "sum": åˆè¨ˆ
            group_by: ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã‚«ãƒ©ãƒ 
            min_periods: æœ€å°æœŸé–“æ•°
            
        Returns:
            pd.DataFrame: ç§»å‹•çµ±è¨ˆé‡ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
            
        Example:
            >>> df_with_rolling = generator.generate_rolling_features(
            ...     df, windows=[7, 30], functions=["mean", "std"]
            ... )
        """
    
    def generate_seasonal_features(
        self,
        df: pd.DataFrame,
        *,
        date_col: str = "ds",
        features: List[str] = ["month", "weekday", "day", "week"],
        cyclical_encoding: bool = True,
        drop_first: bool = False,
    ) -> pd.DataFrame:
        """
        å­£ç¯€æ€§ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
        
        Args:
            df: DataFrame
            date_col: æ—¥æ™‚ã‚«ãƒ©ãƒ 
            features: ç”Ÿæˆã™ã‚‹ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
                - "month": æœˆï¼ˆ1-12ï¼‰
                - "weekday": æ›œæ—¥ï¼ˆ0-6ï¼‰
                - "day": æ—¥ï¼ˆ1-31ï¼‰
                - "week": é€±ç•ªå·ï¼ˆ1-52ï¼‰
                - "quarter": å››åŠæœŸï¼ˆ1-4ï¼‰
                - "hour": æ™‚ï¼ˆ0-23ï¼‰
                - "minute": åˆ†ï¼ˆ0-59ï¼‰
            cyclical_encoding: å¾ªç’°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆsin/cosï¼‰
            drop_first: ãƒ€ãƒŸãƒ¼å¤‰æ•°ã®æœ€åˆã‚’å‰Šé™¤
            
        Returns:
            pd.DataFrame: å­£ç¯€æ€§ç‰¹å¾´é‡ãŒè¿½åŠ ã•ã‚ŒãŸDataFrame
            
        Example:
            >>> df_with_seasonal = generator.generate_seasonal_features(
            ...     df, features=["month", "weekday"], cyclical_encoding=True
            ... )
            >>> df_with_seasonal.columns
            Index([..., 'month_sin', 'month_cos', 'weekday_sin', 'weekday_cos'])
        """
```

---

#### 5.2.2 ç‰¹å¾´é‡é‡è¦åº¦è©•ä¾¡ (FR-FEATURE-002)

**æ¦‚è¦**: ç”Ÿæˆã—ãŸç‰¹å¾´é‡ã®å¯„ä¸åº¦ã‚’è©•ä¾¡

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**è©•ä¾¡æ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | é©ç”¨å ´é¢ | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|-----|------|---------|-----------|
| **Permutation Importance** | ç‰¹å¾´é‡ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ç²¾åº¦ä½ä¸‹ã‚’æ¸¬å®š | ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ« | é«˜ |
| **Feature Correlation** | ç›®çš„å¤‰æ•°ã¨ã®ç›¸é–¢ä¿‚æ•° | ç·šå½¢é–¢ä¿‚ã®æŠŠæ¡ | ä½ |
| **Mutual Information** | ç›¸äº’æƒ…å ±é‡ | éç·šå½¢é–¢ä¿‚ã®æŠŠæ¡ | ä¸­ |
| **SHAP Values** | ãƒ¢ãƒ‡ãƒ«éä¾å­˜ã®å¯„ä¸åº¦ | è§£é‡ˆæ€§é‡è¦– | éå¸¸ã«é«˜ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class FeatureEvaluator:
    def compute_permutation_importance(
        self,
        model: Any,
        X: pd.DataFrame,
        y: pd.Series,
        *,
        n_repeats: int = 10,
        random_state: int = 42,
        scoring: str = "neg_mean_squared_error",
        n_jobs: int = -1,
    ) -> pd.DataFrame:
        """
        Permutation Importanceã‚’è¨ˆç®—
        
        Args:
            model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            X: ç‰¹å¾´é‡DataFrame
            y: ç›®çš„å¤‰æ•°Series
            n_repeats: ã‚·ãƒ£ãƒƒãƒ•ãƒ«å›æ•°
            random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            scoring: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é–¢æ•°
            n_jobs: ä¸¦åˆ—æ•°
            
        Returns:
            pd.DataFrame: é‡è¦åº¦çµæœ
                - feature: ç‰¹å¾´é‡å
                - importance_mean: å¹³å‡é‡è¦åº¦
                - importance_std: æ¨™æº–åå·®
                
        Example:
            >>> evaluator = FeatureEvaluator()
            >>> importance = evaluator.compute_permutation_importance(
            ...     model=fitted_model,
            ...     X=X_test,
            ...     y=y_test,
            ...     n_repeats=10
            ... )
            >>> importance.sort_values('importance_mean', ascending=False)
        """
    
    def compute_correlation(
        self,
        df: pd.DataFrame,
        *,
        target_col: str = "y",
        method: Literal["pearson", "spearman", "kendall"] = "spearman",
        min_periods: int = 30,
    ) -> pd.Series:
        """
        ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã®ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
        
        Args:
            df: DataFrame
            target_col: ç›®çš„å¤‰æ•°ã‚«ãƒ©ãƒ 
            method: ç›¸é–¢ä¿‚æ•°æ‰‹æ³•
                - "pearson": ãƒ”ã‚¢ã‚½ãƒ³ï¼ˆç·šå½¢ï¼‰
                - "spearman": ã‚¹ãƒ”ã‚¢ãƒãƒ³ï¼ˆé †ä½ï¼‰
                - "kendall": ã‚±ãƒ³ãƒ‰ãƒ¼ãƒ«ï¼ˆé †ä½ï¼‰
            min_periods: æœ€å°æœŸé–“æ•°
            
        Returns:
            pd.Series: ç›¸é–¢ä¿‚æ•°ï¼ˆç‰¹å¾´é‡åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹ï¼‰
            
        Example:
            >>> correlation = evaluator.compute_correlation(
            ...     df, method="spearman"
            ... )
            >>> correlation.abs().sort_values(ascending=False)
        """
```

---

#### 5.2.3 ç‰¹å¾´é‡é¸æŠ (FR-FEATURE-003)

**æ¦‚è¦**: é‡è¦åº¦ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’è‡ªå‹•é¸æŠ

**å„ªå…ˆåº¦**: ä¸­

**é¸æŠåŸºæº–**:

| åŸºæº– | èª¬æ˜ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|-----|------|-----------|
| **é‡è¦åº¦é–¾å€¤** | é‡è¦åº¦ãŒé–¾å€¤ä»¥ä¸Šã®ç‰¹å¾´é‡ã‚’é¸æŠ | `threshold: float` |
| **Top-K** | ä¸Šä½Kå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ | `k: int` |
| **ç´¯ç©å¯„ä¸ç‡** | ç´¯ç©å¯„ä¸ç‡ãŒN%ã«ãªã‚‹ã¾ã§é¸æŠ | `cumulative_ratio: float` |
| **ç›¸é–¢é™¤å»** | é«˜ç›¸é–¢ç‰¹å¾´é‡ã‚’é™¤å» | `correlation_threshold: float` |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class FeatureSelector:
    def select_by_importance(
        self,
        importance: pd.DataFrame,
        *,
        method: Literal["threshold", "top_k", "cumulative"] = "threshold",
        threshold: Optional[float] = 0.01,
        k: Optional[int] = None,
        cumulative_ratio: Optional[float] = None,
    ) -> List[str]:
        """
        é‡è¦åº¦ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’é¸æŠ
        
        Args:
            importance: é‡è¦åº¦DataFrame
            method: é¸æŠæ‰‹æ³•
                - "threshold": é–¾å€¤ä»¥ä¸Š
                - "top_k": ä¸Šä½Kå€‹
                - "cumulative": ç´¯ç©å¯„ä¸ç‡
            threshold: é‡è¦åº¦é–¾å€¤ï¼ˆmethod="threshold"æ™‚ï¼‰
            k: é¸æŠæ•°ï¼ˆmethod="top_k"æ™‚ï¼‰
            cumulative_ratio: ç´¯ç©å¯„ä¸ç‡ï¼ˆmethod="cumulative"æ™‚ã€0.95æ¨å¥¨ï¼‰
            
        Returns:
            List[str]: é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
            
        Example:
            >>> selector = FeatureSelector()
            >>> selected = selector.select_by_importance(
            ...     importance, method="top_k", k=10
            ... )
            >>> X_selected = X[selected]
        """
    
    def remove_correlated_features(
        self,
        df: pd.DataFrame,
        *,
        correlation_threshold: float = 0.95,
        method: Literal["pearson", "spearman"] = "spearman",
        keep: Literal["first", "last", "highest_importance"] = "first",
    ) -> List[str]:
        """
        é«˜ç›¸é–¢ç‰¹å¾´é‡ã‚’é™¤å»
        
        Args:
            df: DataFrame
            correlation_threshold: ç›¸é–¢é–¾å€¤ï¼ˆã“ã‚Œä»¥ä¸Šã§é™¤å»ï¼‰
            method: ç›¸é–¢è¨ˆç®—æ‰‹æ³•
            keep: ä¿æŒåŸºæº–
                - "first": æœ€åˆã®ç‰¹å¾´é‡ã‚’ä¿æŒ
                - "last": æœ€å¾Œã®ç‰¹å¾´é‡ã‚’ä¿æŒ
                - "highest_importance": é‡è¦åº¦ãŒé«˜ã„æ–¹ã‚’ä¿æŒ
                
        Returns:
            List[str]: é™¤å»ã™ã‚‹ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
            
        Example:
            >>> to_drop = selector.remove_correlated_features(
            ...     df, correlation_threshold=0.95
            ... )
            >>> df_reduced = df.drop(columns=to_drop)
        ```

---

### 5.3 å®Ÿé¨“ç®¡ç†ãƒ»å†ç¾æ€§ (FR-EXPERIMENT)

#### 5.3.1 å®Ÿé¨“æ§‹é€  (FR-EXPERIMENT-001)

**æ¦‚è¦**: Experimentï¼ˆå®Ÿé¨“ï¼‰ã¨Runï¼ˆå®Ÿè¡Œï¼‰ã®2å±¤æ§‹é€ ã§ç®¡ç†

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**æ§‹é€ å®šç¾©**:

```
Experiment (å®Ÿé¨“)
â”œâ”€â”€ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ experiment_id: UUID
â”‚   â”œâ”€â”€ name: str
â”‚   â”œâ”€â”€ objective: str
â”‚   â”œâ”€â”€ created_at: datetime
â”‚   â””â”€â”€ tags: Dict[str, str]
â””â”€â”€ Runs (å®Ÿè¡Œ) []
    â”œâ”€â”€ Run 1
    â”‚   â”œâ”€â”€ run_id: UUID
    â”‚   â”œâ”€â”€ run_fingerprint: str (16æ¡)
    â”‚   â”œâ”€â”€ status: Enum
    â”‚   â”œâ”€â”€ parameters: Dict
    â”‚   â”œâ”€â”€ metrics: Dict
    â”‚   â”œâ”€â”€ artifacts: List[Path]
    â”‚   â””â”€â”€ metadata: Dict
    â”œâ”€â”€ Run 2
    â””â”€â”€ ...
```

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class ExperimentManager:
    def create_experiment(
        self,
        name: str,
        *,
        objective: str,
        description: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
    ) -> Experiment:
        """
        æ–°ã—ã„å®Ÿé¨“ã‚’ä½œæˆ
        
        Args:
            name: å®Ÿé¨“å
            objective: å®Ÿé¨“ç›®çš„
            description: è©³ç´°èª¬æ˜
            tags: ã‚¿ã‚°è¾æ›¸ï¼ˆä¾‹: {"dataset": "v1", "model_type": "lstm"}ï¼‰
            
        Returns:
            Experiment: å®Ÿé¨“ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Raises:
            DuplicateExperimentError: åŒåã®å®Ÿé¨“ãŒæ—¢ã«å­˜åœ¨
            
        Example:
            >>> manager = ExperimentManager()
            >>> exp = manager.create_experiment(
            ...     name="baseline_comparison_2025-11",
            ...     objective="Compare AutoNHITS vs AutoLSTM",
            ...     tags={"dataset": "sales_v2", "phase": "baseline"}
            ... )
            >>> exp.experiment_id
            UUID('f47ac10b-58cc-4372-a567-0e02b2c3d479')
        """
    
    def get_experiment(
        self,
        experiment_id: Optional[UUID] = None,
        *,
        name: Optional[str] = None,
    ) -> Experiment:
        """
        å®Ÿé¨“ã‚’å–å¾—
        
        Args:
            experiment_id: å®Ÿé¨“IDï¼ˆå„ªå…ˆï¼‰
            name: å®Ÿé¨“åï¼ˆexperiment_idãŒç„¡ã„å ´åˆï¼‰
            
        Returns:
            Experiment: å®Ÿé¨“ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Raises:
            ExperimentNotFoundError: å®Ÿé¨“ãŒå­˜åœ¨ã—ãªã„
            
        Example:
            >>> exp = manager.get_experiment(name="baseline_comparison_2025-11")
        """
```

---

#### 5.3.2 Run Fingerprint (FR-EXPERIMENT-002)

**æ¦‚è¦**: å®Ÿè¡Œæ¡ä»¶ã®ä¸€æ„è­˜åˆ¥å­ã‚’ç”Ÿæˆã—ã€é‡è¤‡å®Ÿè¡Œã‚’æ¤œå‡º

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**Fingerprintè¨ˆç®—å¼**:

```python
run_fingerprint = hashlib.sha256(
    json.dumps({
        "model_adapter_name": "AutoNHITS",
        "model_version_tag": "neuralforecast==1.6.0",
        "hyperparameters": {
            "input_size": 14,
            "h": 7,
            "loss": "MAE",
            "scaler": "standard",
            # ... ãã®ä»–ã™ã¹ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        },
        "dataset_version": "a3f8c921d4e6b5f2",
        "feature_set_id": "lag_7_14_rolling_mean",
        "training_window_spec": {
            "start_date": "2023-01-01",
            "end_date": "2024-12-31",
            "split_method": "rolling_origin",
            "n_splits": 5,
        },
        "code_revision": "abc123def",
        "random_seed": 42,
        "objective_config": {
            "metric": "sMAPE",
            "weight": 1.0,
        }
    }, sort_keys=True).encode()
).hexdigest()[:16]
```

**é‡è¤‡åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯**:

```python
if fingerprint_exists_in_db(run_fingerprint):
    logger.info(f"Skip: Run with fingerprint {run_fingerprint} already exists")
    return existing_run
else:
    execute_new_run()
```

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class FingerprintManager:
    def compute_fingerprint(
        self,
        *,
        model_name: str,
        model_version: str,
        hyperparameters: Dict[str, Any],
        dataset_version: str,
        feature_set_id: str,
        training_window: Dict[str, Any],
        code_revision: str,
        random_seed: int,
        objective_config: Dict[str, Any],
    ) -> str:
        """
        Run Fingerprintã‚’è¨ˆç®—
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: "AutoNHITS"ï¼‰
            model_version: ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆä¾‹: "neuralforecast==1.6.0"ï¼‰
            hyperparameters: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸
            dataset_version: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³
            feature_set_id: ç‰¹å¾´é‡ã‚»ãƒƒãƒˆID
            training_window: å­¦ç¿’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨­å®š
            code_revision: Gitã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥
            random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            objective_config: ç›®çš„é–¢æ•°è¨­å®š
            
        Returns:
            str: 16æ¡ã®Fingerprint
            
        Example:
            >>> manager = FingerprintManager()
            >>> fp = manager.compute_fingerprint(
            ...     model_name="AutoNHITS",
            ...     model_version="neuralforecast==1.6.0",
            ...     hyperparameters={"input_size": 14, "h": 7},
            ...     dataset_version="a3f8c921",
            ...     feature_set_id="lag_7_14",
            ...     training_window={"start": "2023-01-01", "end": "2024-12-31"},
            ...     code_revision="abc123",
            ...     random_seed=42,
            ...     objective_config={"metric": "sMAPE"}
            ... )
            >>> fp
            '7d8e3f1a2b9c4e5f'
        """
    
    def check_duplicate(
        self,
        fingerprint: str,
        *,
        experiment_id: Optional[UUID] = None,
    ) -> Optional[Run]:
        """
        é‡è¤‡Runã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            fingerprint: Run Fingerprint
            experiment_id: å®Ÿé¨“IDï¼ˆæŒ‡å®šã—ãŸå ´åˆã€åŒä¸€å®Ÿé¨“å†…ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼‰
            
        Returns:
            Optional[Run]: æ—¢å­˜ã®Runï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰ã€ãªã‘ã‚Œã°None
            
        Example:
            >>> existing = manager.check_duplicate("7d8e3f1a2b9c4e5f")
            >>> if existing:
            ...     print(f"Skip: Run {existing.run_id} already exists")
            ... else:
            ...     execute_new_run()
        """
```

---

### 5.4 å­¦ç¿’ãƒ»æ¢ç´¢ãƒ»è©•ä¾¡ (FR-TRAINING)

#### 5.4.1 ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ (FR-TRAINING-001)

**æ¦‚è¦**: AdapterçµŒç”±ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**å­¦ç¿’ãƒ•ãƒ­ãƒ¼**:

```
1. ãƒ‡ãƒ¼ã‚¿æº–å‚™
   â†“
2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
   â†“
3. å­¦ç¿’/æ¤œè¨¼åˆ†å‰²
   â†“
4. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
   â†“
5. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
   â†“
6. å­¦ç¿’å®Ÿè¡Œ
   â†“
7. ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
   â†“
8. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
```

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class ModelTrainer:
    def train(
        self,
        model: BaseModel,
        train_df: pd.DataFrame,
        *,
        val_df: Optional[pd.DataFrame] = None,
        hyperparameters: Optional[Dict[str, Any]] = None,
        callbacks: Optional[List[Callback]] = None,
        early_stopping: bool = True,
        early_stopping_patience: int = 10,
        checkpoint_dir: Optional[Path] = None,
    ) -> TrainingResult:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        
        Args:
            model: å­¦ç¿’å¯¾è±¡ãƒ¢ãƒ‡ãƒ«
            train_df: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            val_df: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ï¼ˆNoneã®å ´åˆã€train_dfã‹ã‚‰è‡ªå‹•åˆ†å‰²ï¼‰
            hyperparameters: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸
            callbacks: ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒªã‚¹ãƒˆ
            early_stopping: Early Stoppingæœ‰åŠ¹åŒ–
            early_stopping_patience: Early Stopping patience
            checkpoint_dir: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å…ˆ
            
        Returns:
            TrainingResult: å­¦ç¿’çµæœ
                - model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
                - history: å­¦ç¿’å±¥æ­´ï¼ˆloss, metricsï¼‰
                - best_epoch: æœ€è‰¯ã‚¨ãƒãƒƒã‚¯
                - training_time: å­¦ç¿’æ™‚é–“ï¼ˆç§’ï¼‰
                - metadata: ãã®ä»–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                
        Raises:
            TrainingError: å­¦ç¿’å¤±æ•—
            ResourceExhaustedError: ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ï¼ˆOOMã€CUDA OOMï¼‰
            
        Example:
            >>> trainer = ModelTrainer()
            >>> result = trainer.train(
            ...     model=AutoNHITS(),
            ...     train_df=train,
            ...     val_df=val,
            ...     hyperparameters={"max_steps": 1000, "lr": 0.001}
            ... )
            >>> print(f"Training time: {result.training_time:.2f}s")
            >>> print(f"Best epoch: {result.best_epoch}")
        """
```

---

#### 5.4.2 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ (FR-TRAINING-002)

**æ¦‚è¦**: Optuna/Ray Tuneã«ã‚ˆã‚‹è‡ªå‹•æ¢ç´¢

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | èª¬æ˜ | é©ç”¨å ´é¢ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|------------|------|---------|-----------|
| **TPE (Tree-structured Parzen Estimator)** | ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®ä¸€ç¨® | ä¸­å°è¦æ¨¡æ¢ç´¢ | `n_trials`, `n_startup_trials` |
| **CMA-ES** | é€²åŒ–æˆ¦ç•¥ | é€£ç¶šå€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | `sigma0`, `n_trials` |
| **Random Search** | ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | `n_trials` |
| **Grid Search** | å…¨æ¢ç´¢ | å°è¦æ¨¡æ¢ç´¢ | `param_grid` |
| **ASHA** | éåŒæœŸãƒãƒ«ãƒ“ãƒ³ã‚° | å¤§è¦æ¨¡ä¸¦åˆ—æ¢ç´¢ | `max_t`, `grace_period` |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class HyperparameterTuner:
    def tune(
        self,
        model_class: Type[BaseModel],
        train_df: pd.DataFrame,
        val_df: pd.DataFrame,
        *,
        param_space: Dict[str, Any],
        n_trials: int = 100,
        timeout: Optional[float] = None,
        algorithm: Literal["tpe", "cmaes", "random", "grid", "asha"] = "tpe",
        metric: str = "val_loss",
        direction: Literal["minimize", "maximize"] = "minimize",
        n_jobs: int = 1,
        pruning: bool = True,
        seed: int = 42,
    ) -> TuningResult:
        """
        ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¢ç´¢
        
        Args:
            model_class: ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
            train_df: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
            val_df: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
            param_space: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“
                ä¾‹: {
                    "input_size": ("int", 7, 30),
                    "hidden_size": ("categorical", [64, 128, 256]),
                    "lr": ("loguniform", 1e-5, 1e-2)
                }
            n_trials: è©¦è¡Œå›æ•°
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
            algorithm: æ¢ç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
            metric: æœ€é©åŒ–å¯¾è±¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            direction: æœ€é©åŒ–æ–¹å‘
            n_jobs: ä¸¦åˆ—æ•°
            pruning: Pruningæœ‰åŠ¹åŒ–
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            
        Returns:
            TuningResult: æ¢ç´¢çµæœ
                - best_params: æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                - best_value: æœ€è‰¯å€¤
                - best_trial: æœ€è‰¯Trial
                - study: Optunaã‚¹ã‚¿ãƒ‡ã‚£ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
                - trials_df: å…¨TrialçµæœDataFrame
                
        Example:
            >>> tuner = HyperparameterTuner()
            >>> result = tuner.tune(
            ...     model_class=AutoNHITS,
            ...     train_df=train,
            ...     val_df=val,
            ...     param_space={
            ...         "input_size": ("int", 7, 30),
            ...         "hidden_size": ("categorical", [64, 128, 256]),
            ...         "lr": ("loguniform", 1e-5, 1e-2)
            ...     },
            ...     n_trials=50,
            ...     algorithm="tpe"
            ... )
            >>> print(f"Best params: {result.best_params}")
            >>> print(f"Best {metric}: {result.best_value}")
        """
```

---

#### 5.4.3 è©•ä¾¡ (FR-TRAINING-003)

**æ¦‚è¦**: Rolling-origin backtest ã«ã‚ˆã‚‹æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**Backtestè¨­è¨ˆ**:

```
ãƒ‡ãƒ¼ã‚¿: 2023-01-01 ~ 2024-12-31 (730æ—¥)

Rolling-Origin Backtest (5 splits, h=7):

Split 1:
  Train: 2023-01-01 ~ 2024-06-30 (547æ—¥)
  Test:  2024-07-01 ~ 2024-07-07 (7æ—¥)

Split 2:
  Train: 2023-01-01 ~ 2024-07-31 (577æ—¥)
  Test:  2024-08-01 ~ 2024-08-07 (7æ—¥)

Split 3:
  Train: 2023-01-01 ~ 2024-08-31 (608æ—¥)
  Test:  2024-09-01 ~ 2024-09-07 (7æ—¥)

Split 4:
  Train: 2023-01-01 ~ 2024-09-30 (638æ—¥)
  Test:  2024-10-01 ~ 2024-10-07 (7æ—¥)

Split 5:
  Train: 2023-01-01 ~ 2024-10-31 (669æ—¥)
  Test:  2024-11-01 ~ 2024-11-07 (7æ—¥)
```

**è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æ•°å¼ | ç¯„å›² | è§£é‡ˆ |
|----------|------|------|------|
| **MAE** | `mean(abs(y_true - y_pred))` | [0, âˆ) | ä½ã„ã»ã©è‰¯ã„ã€å˜ä½ã¯yã¨åŒã˜ |
| **RMSE** | `sqrt(mean((y_true - y_pred)^2))` | [0, âˆ) | ä½ã„ã»ã©è‰¯ã„ã€å¤–ã‚Œå€¤ã«æ•æ„Ÿ |
| **sMAPE** | `mean(2 * abs(y_true - y_pred) / (abs(y_true) + abs(y_pred)))` | [0, 2] | ä½ã„ã»ã©è‰¯ã„ã€å¯¾ç§°çš„ |
| **MASE** | `MAE / naive_MAE` | [0, âˆ) | <1ã§è‰¯å¥½ã€ãƒŠã‚¤ãƒ¼ãƒ–äºˆæ¸¬ã‚ˆã‚Šè‰¯ã„ |
| **MAPE** | `mean(abs((y_true - y_pred) / y_true))` | [0, âˆ) | ä½ã„ã»ã©è‰¯ã„ã€y_true=0ã§ã‚¨ãƒ©ãƒ¼ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class ModelEvaluator:
    def backtest(
        self,
        model: BaseModel,
        df: pd.DataFrame,
        *,
        n_splits: int = 5,
        h: int = 7,
        gap: int = 0,
        expanding_window: bool = True,
        metrics: List[str] = ["mae", "rmse", "smape", "mase"],
        return_predictions: bool = True,
    ) -> BacktestResult:
        """
        Rolling-origin backtestã‚’å®Ÿè¡Œ
        
        Args:
            model: è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«
            df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            n_splits: åˆ†å‰²æ•°
            h: Horizonï¼ˆäºˆæ¸¬æœŸé–“ï¼‰
            gap: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®é–“éš”
            expanding_window: True=Expanding, False=Sliding
            metrics: è¨ˆç®—ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆ
            return_predictions: äºˆæ¸¬çµæœã‚’è¿”ã™ã‹
            
        Returns:
            BacktestResult: Backtestçµæœ
                - fold_results: List[FoldResult]
                    - fold_id: int
                    - train_period: (start, end)
                    - test_period: (start, end)
                    - metrics: Dict[str, float]
                    - predictions: Optional[pd.DataFrame]
                - aggregated_metrics: Dict[str, float]
                    - {metric}_mean
                    - {metric}_std
                    - {metric}_min
                    - {metric}_max
                - metadata: Dict[str, Any]
                
        Example:
            >>> evaluator = ModelEvaluator()
            >>> result = evaluator.backtest(
            ...     model=fitted_model,
            ...     df=df,
            ...     n_splits=5,
            ...     h=7,
            ...     metrics=["mae", "rmse", "smape"]
            ... )
            >>> print(f"Mean MAE: {result.aggregated_metrics['mae_mean']:.2f}")
            >>> print(f"Std MAE: {result.aggregated_metrics['mae_std']:.2f}")
        """
    
    def compute_metrics(
        self,
        y_true: pd.Series,
        y_pred: pd.Series,
        *,
        metrics: List[str] = ["mae", "rmse", "smape", "mase", "mape"],
    ) -> Dict[str, float]:
        """
        äºˆæ¸¬ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        
        Args:
            y_true: çœŸå€¤
            y_pred: äºˆæ¸¬å€¤
            metrics: è¨ˆç®—ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict[str, float]: ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾æ›¸
            
        Example:
            >>> metrics = evaluator.compute_metrics(
            ...     y_true=y_test,
            ...     y_pred=predictions
            ... )
            >>> print(f"MAE: {metrics['mae']:.2f}")
            >>> print(f"sMAPE: {metrics['smape']:.2%}")
        """
```

---

### 5.5 å› æœãƒ»ç›¸é–¢ãƒ»å¯„ä¸åº¦åˆ†æ (FR-ANALYSIS)

#### 5.5.1 ç›¸é–¢åˆ†æ (FR-ANALYSIS-001)

**æ¦‚è¦**: ç‰¹å¾´é‡é–“ãŠã‚ˆã³ãƒ©ã‚°ç›¸é–¢ã‚’åˆ†æ

**å„ªå…ˆåº¦**: ä¸­

**åˆ†ææ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | é©ç”¨å ´é¢ |
|-----|------|---------|
| **Pearsonç›¸é–¢** | ç·šå½¢ç›¸é–¢ | æ­£è¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ |
| **Spearmanç›¸é–¢** | é †ä½ç›¸é–¢ | éç·šå½¢ãƒ»å¤–ã‚Œå€¤ã«é ‘å¥ |
| **Kendallç›¸é–¢** | é †ä½ç›¸é–¢ï¼ˆã‚ˆã‚Šé ‘å¥ï¼‰ | å°ã‚µãƒ³ãƒ—ãƒ« |
| **Lagç›¸é–¢ (ACF)** | è‡ªå·±ç›¸é–¢é–¢æ•° | æ™‚ç³»åˆ—ã®å‘¨æœŸæ€§æ¤œå‡º |
| **Partialç›¸é–¢ (PACF)** | åè‡ªå·±ç›¸é–¢é–¢æ•° | ARãƒ¢ãƒ‡ãƒ«æ¬¡æ•°æ±ºå®š |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class CorrelationAnalyzer:
    def compute_correlation_matrix(
        self,
        df: pd.DataFrame,
        *,
        method: Literal["pearson", "spearman", "kendall"] = "spearman",
        min_periods: int = 30,
    ) -> pd.DataFrame:
        """
        ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
        
        Args:
            df: DataFrame
            method: ç›¸é–¢æ‰‹æ³•
            min_periods: æœ€å°æœŸé–“æ•°
            
        Returns:
            pd.DataFrame: ç›¸é–¢è¡Œåˆ—
            
        Example:
            >>> analyzer = CorrelationAnalyzer()
            >>> corr_matrix = analyzer.compute_correlation_matrix(df)
            >>> import seaborn as sns
            >>> sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
        """
    
    def compute_lag_correlation(
        self,
        series: pd.Series,
        *,
        lags: int = 40,
        alpha: float = 0.05,
    ) -> LagCorrelationResult:
        """
        ãƒ©ã‚°ç›¸é–¢ï¼ˆACF/PACFï¼‰ã‚’è¨ˆç®—
        
        Args:
            series: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
            lags: æœ€å¤§ãƒ©ã‚°æ•°
            alpha: æœ‰æ„æ°´æº–
            
        Returns:
            LagCorrelationResult:
                - acf: np.ndarray (è‡ªå·±ç›¸é–¢)
                - pacf: np.ndarray (åè‡ªå·±ç›¸é–¢)
                - acf_confint: np.ndarray (ä¿¡é ¼åŒºé–“)
                - pacf_confint: np.ndarray (ä¿¡é ¼åŒºé–“)
                
        Example:
            >>> result = analyzer.compute_lag_correlation(df['y'], lags=40)
            >>> plt.stem(result.acf)
            >>> plt.fill_between(
            ...     range(len(result.acf)),
            ...     result.acf_confint[:, 0],
            ...     result.acf_confint[:, 1],
            ...     alpha=0.2
            ... )
        """
```

---

#### 5.5.2 å› æœåˆ†æ (FR-ANALYSIS-002)

**æ¦‚è¦**: Grangerå› æœæ€§ãƒ†ã‚¹ãƒˆãŠã‚ˆã³ä»‹å…¥åˆ†æ

**å„ªå…ˆåº¦**: ä¸­ï¼ˆæ³¨æ„äº‹é …ã‚ã‚Šï¼‰

**è­¦å‘Š**:
- Grangerå› æœæ€§ã¯ã€Œçµ±è¨ˆçš„å› æœã€ã§ã‚ã‚Šã€çœŸã®å› æœé–¢ä¿‚ã‚’ä¿è¨¼ã—ãªã„
- ä»‹å…¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿DiD/Synthetic Controlã‚’é©ç”¨å¯èƒ½
- çµæœã¯ã€Œå‚è€ƒå€¤ã€ã¨ã—ã¦æ‰±ã„ã€è§£é‡ˆã«ã¯æ³¨æ„ãŒå¿…è¦

**åˆ†ææ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | ãƒ‡ãƒ¼ã‚¿è¦ä»¶ | é©ç”¨å ´é¢ |
|-----|------|-----------|---------|
| **Grangerå› æœæ€§ãƒ†ã‚¹ãƒˆ** | Xâ†’Yã®äºˆæ¸¬æ”¹å–„ã‚’æ¤œå®š | æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ | å…ˆè¡Œé–¢ä¿‚ã®æ¤œè¨¼ |
| **DiD (Difference-in-Differences)** | ä»‹å…¥åŠ¹æœã®æ¨å®š | ä»‹å…¥ç¾¤/å¯¾ç…§ç¾¤ã€ä»‹å…¥å‰å¾Œ | æ”¿ç­–åŠ¹æœæ¤œè¨¼ |
| **Synthetic Control** | åˆæˆå¯¾ç…§ç¾¤ã®æ§‹ç¯‰ | ä»‹å…¥å‰ãƒ‡ãƒ¼ã‚¿ã€è¤‡æ•°å¯¾ç…§ç¾¤ | å˜ä¸€ä»‹å…¥ã®åŠ¹æœæ¨å®š |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class CausalAnalyzer:
    def granger_causality_test(
        self,
        df: pd.DataFrame,
        *,
        target_col: str,
        predictor_cols: List[str],
        max_lag: int = 10,
        test: Literal["ssr_ftest", "ssr_chi2test", "lrtest", "params_ftest"] = "ssr_ftest",
        alpha: float = 0.05,
    ) -> GrangerTestResult:
        """
        Grangerå› æœæ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        
        Args:
            df: DataFrame
            target_col: ç›®çš„å¤‰æ•°ã‚«ãƒ©ãƒ 
            predictor_cols: èª¬æ˜å¤‰æ•°ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ
            max_lag: æœ€å¤§ãƒ©ã‚°æ•°
            test: ãƒ†ã‚¹ãƒˆæ‰‹æ³•
            alpha: æœ‰æ„æ°´æº–
            
        Returns:
            GrangerTestResult:
                - results: Dict[str, Dict[int, GrangerResult]]
                    - predictor_col:
                        - lag: GrangerResult
                            - f_statistic: float
                            - p_value: float
                            - is_significant: bool
                - summary_df: pd.DataFrameï¼ˆè¦ç´„ï¼‰
                
        Raises:
            ValueError: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€ãƒ©ã‚°æ•°ä¸æ­£
            
        Example:
            >>> causal = CausalAnalyzer()
            >>> result = causal.granger_causality_test(
            ...     df,
            ...     target_col="y",
            ...     predictor_cols=["temperature", "promotion"],
            ...     max_lag=7
            ... )
            >>> for predictor, lag_results in result.results.items():
            ...     for lag, gr in lag_results.items():
            ...         if gr.is_significant:
            ...             print(f"{predictor} (lag={lag}): p={gr.p_value:.4f}")
        """
    
    def difference_in_differences(
        self,
        df: pd.DataFrame,
        *,
        outcome_col: str,
        treatment_col: str,
        time_col: str,
        intervention_time: Any,
        control_variables: Optional[List[str]] = None,
    ) -> DIDResult:
        """
        Difference-in-Differencesåˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            df: DataFrame
            outcome_col: ã‚¢ã‚¦ãƒˆã‚«ãƒ å¤‰æ•°
            treatment_col: å‡¦ç½®ç¾¤ãƒ•ãƒ©ã‚°ï¼ˆ0/1ï¼‰
            time_col: æ™‚é–“å¤‰æ•°
            intervention_time: ä»‹å…¥æ™‚ç‚¹
            control_variables: å…±å¤‰é‡ãƒªã‚¹ãƒˆ
            
        Returns:
            DIDResult:
                - treatment_effect: float (å‡¦ç½®åŠ¹æœæ¨å®šå€¤)
                - std_error: float
                - t_statistic: float
                - p_value: float
                - conf_interval: Tuple[float, float]
                - regression_summary: str
                
        Example:
            >>> result = causal.difference_in_differences(
            ...     df,
            ...     outcome_col="sales",
            ...     treatment_col="is_store_group_A",
            ...     time_col="ds",
            ...     intervention_time="2024-06-01"
            ... )
            >>> print(f"Treatment Effect: {result.treatment_effect:.2f}")
            >>> print(f"P-value: {result.p_value:.4f}")
        """
```

---

#### 5.5.3 å¯„ä¸åº¦åˆ†æ (FR-ANALYSIS-003)

**æ¦‚è¦**: ç‰¹å¾´é‡ã®äºˆæ¸¬ã¸ã®å¯„ä¸åº¦ã‚’å®šé‡åŒ–

**å„ªå…ˆåº¦**: ä¸­

**åˆ†ææ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | é©ç”¨ãƒ¢ãƒ‡ãƒ« | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|-----|------|-----------|-----------|
| **Permutation Importance** | ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ã‚ˆã‚‹ç²¾åº¦ä½ä¸‹ | å…¨ãƒ¢ãƒ‡ãƒ« | ä¸­ |
| **SHAP (SHapley Additive exPlanations)** | Shapleyå€¤ã«ã‚ˆã‚‹å¯„ä¸åº¦ | Treeç³»ã€NNç³» | é«˜ |
| **LIME (Local Interpretable Model-agnostic Explanations)** | å±€æ‰€ç·šå½¢è¿‘ä¼¼ | å…¨ãƒ¢ãƒ‡ãƒ« | ä¸­ |
| **Feature Ablation** | ç‰¹å¾´é‡é™¤å»ã«ã‚ˆã‚‹ç²¾åº¦å¤‰åŒ– | å…¨ãƒ¢ãƒ‡ãƒ« | é«˜ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class ContributionAnalyzer:
    def compute_shap_values(
        self,
        model: BaseModel,
        X: pd.DataFrame,
        *,
        background_samples: int = 100,
        max_evals: int = 1000,
        check_additivity: bool = False,
    ) -> SHAPResult:
        """
        SHAPå€¤ã‚’è¨ˆç®—
        
        Args:
            model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            X: èª¬æ˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            background_samples: èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°
            max_evals: æœ€å¤§è©•ä¾¡å›æ•°
            check_additivity: åŠ æ³•æ€§ãƒã‚§ãƒƒã‚¯
            
        Returns:
            SHAPResult:
                - shap_values: np.ndarray (N x D)
                - expected_value: float
                - feature_importance: pd.Series
                - summary_plot_data: Dict
                
        Example:
            >>> contrib = ContributionAnalyzer()
            >>> shap_result = contrib.compute_shap_values(
            ...     model=fitted_model,
            ...     X=X_test.sample(100),
            ...     background_samples=50
            ... )
            >>> import shap
            >>> shap.summary_plot(
            ...     shap_result.shap_values,
            ...     X_test.sample(100)
            ... )
        """
    
    def compute_feature_contribution_over_time(
        self,
        model: BaseModel,
        df: pd.DataFrame,
        *,
        window_size: int = 30,
        step: int = 7,
        method: Literal["permutation", "shap"] = "permutation",
    ) -> pd.DataFrame:
        """
        æ™‚é–“çµŒéã«ä¼´ã†ç‰¹å¾´é‡å¯„ä¸åº¦ã®å¤‰åŒ–ã‚’è¨ˆç®—
        
        Args:
            model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            df: DataFrame
            window_size: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆæ—¥æ•°ï¼‰
            step: ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºï¼ˆæ—¥æ•°ï¼‰
            method: å¯„ä¸åº¦è¨ˆç®—æ‰‹æ³•
            
        Returns:
            pd.DataFrame: æ™‚ç³»åˆ—ã®å¯„ä¸åº¦
                - columns: ç‰¹å¾´é‡å
                - index: æ™‚åˆ»
                - values: å¯„ä¸åº¦
                
        Example:
            >>> contrib_time = contrib.compute_feature_contribution_over_time(
            ...     model=fitted_model,
            ...     df=df,
            ...     window_size=30,
            ...     step=7,
            ...     method="permutation"
            ... )
            >>> contrib_time.plot(figsize=(12, 6))
        ```

---

### 5.6 ãƒ­ã‚®ãƒ³ã‚°ãƒ»ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° (FR-LOGGING)

#### 5.6.1 æ§‹é€ åŒ–ãƒ­ã‚° (FR-LOGGING-001)

**æ¦‚è¦**: JSONå½¢å¼ã®æ§‹é€ åŒ–ãƒ­ã‚°ã‚’å‡ºåŠ›

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«**:

| ãƒ¬ãƒ™ãƒ« | ç”¨é€” | ä¾‹ |
|-------|------|---|
| **DEBUG** | é–‹ç™ºæ™‚ã®è©³ç´°æƒ…å ± | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã€ä¸­é–“è¨ˆç®—çµæœ |
| **INFO** | é€šå¸¸ã®å‹•ä½œæƒ…å ± | å®Ÿè¡Œé–‹å§‹/çµ‚äº†ã€é€²æ— |
| **WARNING** | è­¦å‘Šï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰ | æ¬ æå€¤æ¤œå‡ºã€åæŸã—ãªã„ |
| **ERROR** | ã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†å¤±æ•—ï¼‰ | ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—ã€OOM |
| **CRITICAL** | ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ | ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ |

**ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**:

```json
{
  "timestamp": "2025-11-03T12:34:56.789Z",
  "level": "INFO",
  "logger": "nf_auto_runner.training",
  "message": "Training completed",
  "run_id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  "experiment_id": "a1b2c3d4-e5f6-4789-90ab-cdef12345678",
  "model_name": "AutoNHITS",
  "duration_seconds": 123.45,
  "metrics": {
    "train_loss": 0.123,
    "val_loss": 0.145,
    "mae": 1.23,
    "rmse": 1.56
  },
  "context": {
    "hostname": "ml-server-01",
    "pid": 12345,
    "thread_id": 67890,
    "gpu_id": 0
  }
}
```

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class StructuredLogger:
    def __init__(
        self,
        name: str,
        *,
        level: str = "INFO",
        log_file: Optional[Path] = None,
        log_to_console: bool = True,
        log_to_file: bool = True,
        add_context: bool = True,
    ):
        """
        æ§‹é€ åŒ–ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            name: ãƒ­ã‚¬ãƒ¼å
            level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
            log_file: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            log_to_console: ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
            log_to_file: ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            add_context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è‡ªå‹•è¿½åŠ 
            
        Example:
            >>> logger = StructuredLogger(
            ...     "nf_auto_runner.training",
            ...     level="INFO",
            ...     log_file=Path("logs/training.jsonl")
            ... )
        """
    
    def log(
        self,
        level: str,
        message: str,
        *,
        run_id: Optional[UUID] = None,
        experiment_id: Optional[UUID] = None,
        extra: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> None:
        """
        ãƒ­ã‚°ã‚’è¨˜éŒ²
        
        Args:
            level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
            message: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            run_id: Run ID
            experiment_id: Experiment ID
            extra: è¿½åŠ æƒ…å ±è¾æ›¸
            **kwargs: ãã®ä»–ã® Key-Value
            
        Example:
            >>> logger.log(
            ...     "INFO",
            ...     "Training started",
            ...     run_id=run.run_id,
            ...     model_name="AutoNHITS",
            ...     hyperparameters={"input_size": 14, "h": 7}
            ... )
        """
    
    def info(self, message: str, **kwargs) -> None:
        """INFOãƒ¬ãƒ™ãƒ«ãƒ­ã‚°"""
    
    def warning(self, message: str, **kwargs) -> None:
        """WARNINGãƒ¬ãƒ™ãƒ«ãƒ­ã‚°"""
    
    def error(self, message: str, **kwargs) -> None:
        """ERRORãƒ¬ãƒ™ãƒ«ãƒ­ã‚°"""
```

---

#### 5.6.2 MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° (FR-LOGGING-002)

**æ¦‚è¦**: MLflowã¸ã®å®Ÿé¨“è¨˜éŒ²ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

**å„ªå…ˆåº¦**: ä¸­ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

**è¨˜éŒ²é …ç›®**:

| ã‚«ãƒ†ã‚´ãƒª | é …ç›® | ä¾‹ |
|---------|------|---|
| **Parameters** | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | `input_size=14`, `h=7`, `lr=0.001` |
| **Metrics** | è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | `train_loss`, `val_loss`, `mae`, `rmse` |
| **Artifacts** | ãƒ•ã‚¡ã‚¤ãƒ«æˆæœç‰© | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã€äºˆæ¸¬çµæœCSVã€ã‚°ãƒ©ãƒ•PNG |
| **Tags** | ã‚¿ã‚° | `model_type=AutoNHITS`, `dataset=sales_v2` |
| **Models** | ãƒ¢ãƒ‡ãƒ«ç™»éŒ² | `models:/AutoNHITS/production` |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class MLflowBridge:
    def __init__(
        self,
        tracking_uri: Optional[str] = None,
        *,
        experiment_name: str = "default",
        artifact_location: Optional[str] = None,
    ):
        """
        MLflow Bridgeã‚’åˆæœŸåŒ–
        
        Args:
            tracking_uri: MLflow Tracking URI
            experiment_name: å®Ÿé¨“å
            artifact_location: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜å…ˆ
            
        Example:
            >>> bridge = MLflowBridge(
            ...     tracking_uri="http://localhost:5000",
            ...     experiment_name="neuralforecast_experiments"
            ... )
        """
    
    def start_run(
        self,
        run_id: UUID,
        *,
        run_name: Optional[str] = None,
        tags: Optional[Dict[str, str]] = None,
    ) -> mlflow.ActiveRun:
        """
        MLflow Runã‚’é–‹å§‹
        
        Args:
            run_id: ã‚·ã‚¹ãƒ†ãƒ ã®Run ID
            run_name: MLflow Runå
            tags: ã‚¿ã‚°è¾æ›¸
            
        Returns:
            mlflow.ActiveRun: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªRun
            
        Example:
            >>> with bridge.start_run(run.run_id, tags={"model": "AutoNHITS"}):
            ...     bridge.log_params({"input_size": 14, "h": 7})
            ...     bridge.log_metrics({"mae": 1.23, "rmse": 1.56})
        """
    
    def log_params(self, params: Dict[str, Any]) -> None:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
    
    def log_metrics(
        self,
        metrics: Dict[str, float],
        *,
        step: Optional[int] = None,
    ) -> None:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
    
    def log_artifact(
        self,
        local_path: Path,
        *,
        artifact_path: Optional[str] = None,
    ) -> None:
        """ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’è¨˜éŒ²"""
    
    def log_model(
        self,
        model: BaseModel,
        artifact_path: str,
        *,
        registered_model_name: Optional[str] = None,
    ) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²ãƒ»ç™»éŒ²"""
```

---

### 5.7 äºˆæ¸¬é‹ç”¨ãƒ»å†å­¦ç¿’ (FR-FORECAST)

#### 5.7.1 1ã‚¹ãƒ†ãƒƒãƒ—å…ˆäºˆæ¸¬ (FR-FORECAST-001)

**æ¦‚è¦**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§1ã‚¹ãƒ†ãƒƒãƒ—å…ˆäºˆæ¸¬ã‚’å®Ÿè¡Œ

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**äºˆæ¸¬ãƒ•ãƒ­ãƒ¼**:

```
1. æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
   â†“
2. ç‰¹å¾´é‡ç”Ÿæˆ
   â†“
3. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
   â†“
4. äºˆæ¸¬å®Ÿè¡Œ
   â†“
5. çµæœä¿å­˜
   â†“
6. é…ä¿¡ï¼ˆAPI/ãƒ•ã‚¡ã‚¤ãƒ«/DBï¼‰
```

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class Forecaster:
    def predict_one_step(
        self,
        model: BaseModel,
        df: pd.DataFrame,
        *,
        horizon: int = 1,
        include_history: bool = False,
        return_quantiles: bool = False,
        quantiles: List[float] = [0.1, 0.5, 0.9],
    ) -> ForecastResult:
        """
        1ã‚¹ãƒ†ãƒƒãƒ—å…ˆäºˆæ¸¬ã‚’å®Ÿè¡Œ
        
        Args:
            model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            df: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…è¦ãªãƒ©ã‚°æœŸé–“ã‚’å«ã‚€ï¼‰
            horizon: äºˆæ¸¬æœŸé–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ=1ï¼‰
            include_history: å±¥æ­´ã‚’å«ã‚ã‚‹ã‹
            return_quantiles: åˆ†ä½ç‚¹ã‚’è¿”ã™ã‹
            quantiles: åˆ†ä½ç‚¹ãƒªã‚¹ãƒˆ
            
        Returns:
            ForecastResult:
                - predictions: pd.DataFrame
                    - unique_id: str
                    - ds: datetime
                    - y_pred: float
                    - [y_pred_q10, y_pred_q50, y_pred_q90]: Optional[float]
                - metadata: Dict[str, Any]
                    - model_name: str
                    - prediction_time: datetime
                    - input_rows: int
                    - output_rows: int
                    
        Example:
            >>> forecaster = Forecaster()
            >>> result = forecaster.predict_one_step(
            ...     model=production_model,
            ...     df=latest_data,
            ...     horizon=7,
            ...     return_quantiles=True
            ... )
            >>> result.predictions
               unique_id         ds  y_pred  y_pred_q10  y_pred_q50  y_pred_q90
            0      store_1 2025-11-04   123.4       110.2       123.4       136.6
            1      store_1 2025-11-05   125.7       112.0       125.7       139.4
        """
```

---

#### 5.7.2 å†å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° (FR-FORECAST-002)

**æ¦‚è¦**: å®šæœŸçš„ã¾ãŸã¯æ¡ä»¶ä»˜ãã§ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**å†å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼**:

| ãƒˆãƒªã‚¬ãƒ¼ | èª¬æ˜ | è¨­å®šä¾‹ |
|---------|------|--------|
| **æ™‚é–“ãƒ™ãƒ¼ã‚¹** | å®šæœŸçš„ã«å†å­¦ç¿’ | æ¯é€±æœˆæ›œæ—¥00:00 |
| **ãƒ‡ãƒ¼ã‚¿é‡ãƒ™ãƒ¼ã‚¹** | æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒé–¾å€¤è¶…é | æ–°è¦100è¡Œã§å†å­¦ç¿’ |
| **ç²¾åº¦åŠ£åŒ–ãƒ™ãƒ¼ã‚¹** | ç²¾åº¦ãŒé–¾å€¤ã‚’ä¸‹å›ã‚‹ | sMAPE > 15%ã§å†å­¦ç¿’ |
| **ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºãƒ™ãƒ¼ã‚¹** | ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¤‰åŒ–ã‚’æ¤œå‡º | KSæ¤œå®š p<0.05ã§å†å­¦ç¿’ |
| **æ‰‹å‹•ãƒˆãƒªã‚¬ãƒ¼** | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«å®Ÿè¡Œ | UIã‹ã‚‰ã€Œå†å­¦ç¿’ã€ãƒœã‚¿ãƒ³ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class RetrainingScheduler:
    def schedule_retrain(
        self,
        model_id: UUID,
        *,
        trigger_type: Literal["time", "data_volume", "accuracy", "drift", "manual"],
        trigger_config: Dict[str, Any],
        retrain_config: Dict[str, Any],
    ) -> RetrainingJob:
        """
        å†å­¦ç¿’ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
        
        Args:
            model_id: ãƒ¢ãƒ‡ãƒ«ID
            trigger_type: ãƒˆãƒªã‚¬ãƒ¼ã‚¿ã‚¤ãƒ—
            trigger_config: ãƒˆãƒªã‚¬ãƒ¼è¨­å®š
                - time: {"cron": "0 0 * * MON"}
                - data_volume: {"threshold": 100, "unit": "rows"}
                - accuracy: {"metric": "smape", "threshold": 0.15}
                - drift: {"method": "ks_test", "alpha": 0.05}
                - manual: {}
            retrain_config: å†å­¦ç¿’è¨­å®š
                - hyperparameters: Dict
                - dataset_window: str ("last_30_days", "all")
                - notify_on_completion: bool
                
        Returns:
            RetrainingJob: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–
            
        Example:
            >>> scheduler = RetrainingScheduler()
            >>> job = scheduler.schedule_retrain(
            ...     model_id=model.id,
            ...     trigger_type="time",
            ...     trigger_config={"cron": "0 0 * * MON"},
            ...     retrain_config={
            ...         "hyperparameters": {"max_steps": 1000},
            ...         "dataset_window": "last_90_days"
            ...     }
            ... )
            >>> job.job_id
            UUID('...')
        """
    
    def check_and_trigger(self) -> List[RetrainingJob]:
        """
        å…¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€æ¡ä»¶ã‚’æº€ãŸã™ã‚‚ã®ã‚’ãƒˆãƒªã‚¬ãƒ¼
        
        Returns:
            List[RetrainingJob]: ãƒˆãƒªã‚¬ãƒ¼ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–ãƒªã‚¹ãƒˆ
            
        Example:
            >>> triggered_jobs = scheduler.check_and_trigger()
            >>> for job in triggered_jobs:
            ...     print(f"Triggered: {job.model_id} due to {job.trigger_type}")
        """
```

---

#### 5.7.3 ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º (FR-FORECAST-003)

**æ¦‚è¦**: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¤‰åŒ–ã‚’çµ±è¨ˆçš„ã«æ¤œå‡º

**å„ªå…ˆåº¦**: ä¸­

**æ¤œå‡ºæ‰‹æ³•**:

| æ‰‹æ³• | èª¬æ˜ | çµ±è¨ˆé‡ | é–¾å€¤ |
|-----|------|--------|------|
| **Kolmogorov-Smirnovæ¤œå®š** | ç´¯ç©åˆ†å¸ƒé–¢æ•°ã®å·® | KSçµ±è¨ˆé‡ | p < 0.05 |
| **Chi-squaredæ¤œå®š** | ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã®å·® | Ï‡Â²çµ±è¨ˆé‡ | p < 0.05 |
| **Population Stability Index (PSI)** | åˆ†å¸ƒã®å®‰å®šæ€§æŒ‡æ¨™ | PSIå€¤ | >0.2ã§ãƒ‰ãƒªãƒ•ãƒˆ |
| **ADWIN** | é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ | å¤‰åŒ–æ¤œå‡º | è‡ªå‹• |
| **Maximum Mean Discrepancy (MMD)** | åˆ†å¸ƒé–“è·é›¢ | MMDçµ±è¨ˆé‡ | p < 0.05 |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class DriftDetector:
    def detect_drift(
        self,
        reference_data: pd.DataFrame,
        current_data: pd.DataFrame,
        *,
        columns: Optional[List[str]] = None,
        method: Literal["ks", "chi2", "psi", "adwin", "mmd"] = "ks",
        alpha: float = 0.05,
        psi_threshold: float = 0.2,
    ) -> DriftResult:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œå‡º
        
        Args:
            reference_data: å‚ç…§ãƒ‡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
            current_data: ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿
            columns: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆï¼ˆNoneã§å…¨æ•°å€¤ã‚«ãƒ©ãƒ ï¼‰
            method: æ¤œå‡ºæ‰‹æ³•
            alpha: æœ‰æ„æ°´æº–ï¼ˆçµ±è¨ˆçš„æ¤œå®šç”¨ï¼‰
            psi_threshold: PSIé–¾å€¤
            
        Returns:
            DriftResult:
                - has_drift: bool
                - drift_columns: List[str]
                - statistics: Dict[str, Any]
                    - column:
                        - statistic: float
                        - p_value: Optional[float]
                        - is_drifted: bool
                - summary: str
                
        Example:
            >>> detector = DriftDetector()
            >>> result = detector.detect_drift(
            ...     reference_data=train_df,
            ...     current_data=latest_df,
            ...     method="ks",
            ...     alpha=0.05
            ... )
            >>> if result.has_drift:
            ...     print(f"Drift detected in: {result.drift_columns}")
            ...     trigger_retrain()
        """
```

---

### 5.8 ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª (FR-REGISTRY)

#### 5.8.1 ãƒ¢ãƒ‡ãƒ«ç™»éŒ² (FR-REGISTRY-001)

**æ¦‚è¦**: ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²ã—ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

**å„ªå…ˆåº¦**: é«˜ï¼ˆå¿…é ˆï¼‰

**ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸**:

| ã‚¹ãƒ†ãƒ¼ã‚¸ | èª¬æ˜ | ç”¨é€” |
|---------|------|------|
| **Development** | é–‹ç™ºä¸­ | å®Ÿé¨“æ®µéš |
| **Staging** | ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚° | æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆ |
| **Production** | æœ¬ç•ª | æœ¬ç•ªé‹ç”¨ |
| **Archived** | ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– | éå»ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**:

```python
class ModelRegistry:
    def register_model(
        self,
        model: BaseModel,
        *,
        name: str,
        version: Optional[str] = None,
        stage: Literal["Development", "Staging", "Production", "Archived"] = "Development",
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[Dict[str, str]] = None,
    ) -> RegisteredModel:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
        
        Args:
            model: ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            name: ãƒ¢ãƒ‡ãƒ«å
            version: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆNoneã§è‡ªå‹•æ¡ç•ªï¼‰
            stage: ã‚¹ãƒ†ãƒ¼ã‚¸
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ€§èƒ½æŒ‡æ¨™ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æƒ…å ±ãªã©ï¼‰
            tags: ã‚¿ã‚°
            
        Returns:
            RegisteredModel: ç™»éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
                - model_id: UUID
                - name: str
                - version: str
                - stage: str
                - registered_at: datetime
                - path: Path
                
        Example:
            >>> registry = ModelRegistry()
            >>> registered = registry.register_model(
            ...     model=best_model,
            ...     name="AutoNHITS_sales_forecast",
            ...     stage="Staging",
            ...     metadata={
            ...         "mae": 1.23,
            ...         "rmse": 1.56,
            ...         "training_date": "2025-11-03",
            ...         "dataset_version": "a3f8c921"
            ...     },
            ...     tags={"model_type": "AutoNHITS", "dataset": "sales"}
            ... )
            >>> registered.model_id
            UUID('...')
        """
    
    def promote_model(
        self,
        model_id: UUID,
        *,
        from_stage: str,
        to_stage: str,
        require_approval: bool = True,
    ) -> RegisteredModel:
        """
        ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ˜‡æ ¼
        
        Args:
            model_id: ãƒ¢ãƒ‡ãƒ«ID
            from_stage: ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸
            to_stage: æ˜‡æ ¼å…ˆã‚¹ãƒ†ãƒ¼ã‚¸
            require_approval: æ‰¿èªå¿…é ˆãƒ•ãƒ©ã‚°
            
        Returns:
            RegisteredModel: æ˜‡æ ¼å¾Œã®ãƒ¢ãƒ‡ãƒ«
            
        Raises:
            InvalidStageTransitionError: ä¸æ­£ãªã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»
            ApprovalRequiredError: æ‰¿èªãŒå¿…è¦
            
        Example:
            >>> # Staging â†’ Productionã¸æ˜‡æ ¼
            >>> promoted = registry.promote_model(
            ...     model_id=registered.model_id,
            ...     from_stage="Staging",
            ...     to_stage="Production",
            ...     require_approval=True
            ... )
        """
    
    def get_model(
        self,
        name: str,
        *,
        version: Optional[str] = None,
        stage: Optional[str] = None,
    ) -> BaseModel:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        
        Args:
            name: ãƒ¢ãƒ‡ãƒ«å
            version: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆNoneã§æœ€æ–°ï¼‰
            stage: ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆæŒ‡å®šã—ãŸå ´åˆã€è©²å½“ã‚¹ãƒ†ãƒ¼ã‚¸ã®æœ€æ–°ï¼‰
            
        Returns:
            BaseModel: ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
            
        Example:
            >>> # Productionç’°å¢ƒã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            >>> prod_model = registry.get_model(
            ...     name="AutoNHITS_sales_forecast",
            ...     stage="Production"
            ... )
            >>> # ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—
            >>> v2_model = registry.get_model(
            ...     name="AutoNHITS_sales_forecast",
            ...     version="v2.0.1"
            ... )
        """
```

---

### 5.9 å¯è¦–åŒ–ãƒ»UI (FR-UI)

#### 5.9.1 Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ (FR-UI-001)

**æ¦‚è¦**: Streamlit/Dashã«ã‚ˆã‚‹é‹ç”¨UI

**å„ªå…ˆåº¦**: ä¸­

**ä¸»è¦ç”»é¢**:

| ç”»é¢ | æ©Ÿèƒ½ | è¡¨ç¤ºå†…å®¹ |
|-----|------|---------|
| **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰** | æ¦‚è¦è¡¨ç¤º | å®Ÿè¡Œä¸­Runã€ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã€æœ€æ–°äºˆæ¸¬çµæœ |
| **å®Ÿé¨“ä¸€è¦§** | å®Ÿé¨“ç®¡ç† | Experiment/Runä¸€è¦§ã€ãƒ•ã‚£ãƒ«ã‚¿ã€æ¤œç´¢ |
| **å®Ÿé¨“è©³ç´°** | è©³ç´°è¡¨ç¤º | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ã‚°ãƒ©ãƒ•ã€ãƒ­ã‚° |
| **ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ** | æ¯”è¼ƒåˆ†æ | è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒã€æ•£å¸ƒå›³ |
| **äºˆæ¸¬å®Ÿè¡Œ** | äºˆæ¸¬æ“ä½œ | ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã€äºˆæ¸¬å®Ÿè¡Œ |
| **å†å­¦ç¿’è¨­å®š** | ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ« | ãƒˆãƒªã‚¬ãƒ¼è¨­å®šã€å†å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **ãƒ¬ã‚¸ã‚¹ãƒˆãƒª** | ãƒ¢ãƒ‡ãƒ«ç®¡ç† | ç™»éŒ²ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã€ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§» |
| **ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–** | ç›£è¦– | CPU/GPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã€ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ |

**APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ** (FastAPI):

```python
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel

app = FastAPI()

class PredictionRequest(BaseModel):
    model_name: str
    model_version: Optional[str] = None
    data: Dict[str, List[Any]]
    horizon: int = 7
    return_quantiles: bool = False

@app.post("/api/v1/predict")
async def predict(request: PredictionRequest) -> Dict[str, Any]:
    """
    äºˆæ¸¬APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    Args:
        request: äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        
    Returns:
        Dict: äºˆæ¸¬çµæœ
            - predictions: List[Dict]
            - metadata: Dict
            
    Example:
        >>> import requests
        >>> response = requests.post(
        ...     "http://localhost:8000/api/v1/predict",
        ...     json={
        ...         "model_name": "AutoNHITS_sales_forecast",
        ...         "model_version": "v1.0.0",
        ...         "data": {
        ...             "unique_id": ["store_1"],
        ...             "ds": ["2025-11-03"],
        ...             "y": [123.4]
        ...         },
        ...         "horizon": 7
        ...     }
        ... )
        >>> response.json()
        {
            "predictions": [
                {"unique_id": "store_1", "ds": "2025-11-04", "y_pred": 125.7},
                ...
            ],
            "metadata": {"model_name": "AutoNHITS_sales_forecast", ...}
        }
    """

@app.post("/api/v1/experiments")
async def create_experiment(request: ExperimentCreateRequest) -> Dict[str, Any]:
    """å®Ÿé¨“ä½œæˆAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""

@app.get("/api/v1/experiments/{experiment_id}")
async def get_experiment(experiment_id: UUID) -> Dict[str, Any]:
    """å®Ÿé¨“å–å¾—APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""

@app.post("/api/v1/runs")
async def create_run(request: RunCreateRequest) -> Dict[str, Any]:
    """Runä½œæˆAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""

@app.get("/api/v1/models")
async def list_models(
    stage: Optional[str] = None,
    limit: int = 100,
    offset: int = 0
) -> Dict[str, Any]:
    """ãƒ¢ãƒ‡ãƒ«ä¸€è¦§APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
```

---

## 6. éæ©Ÿèƒ½è¦ä»¶è©³ç´°

### 6.1 æ€§èƒ½è¦ä»¶

| é …ç›® | è¦ä»¶ | æ¸¬å®šæ–¹æ³• |
|-----|------|---------|
| **å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚é–“** | <10åˆ† | Wall clock time |
| **100ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’æ™‚é–“** | <2æ™‚é–“ | Wall clock time |
| **äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (å˜ä¸€)** | <100ms | time.perf_counter |
| **äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ãƒãƒƒãƒ100ä»¶)** | <1ç§’ | time.perf_counter |
| **ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (1GB CSV)** | <30ç§’ | time.perf_counter |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | <16GB (ãƒ”ãƒ¼ã‚¯) | psutil |
| **GPU VRAMä½¿ç”¨é‡** | <12GB (ãƒ”ãƒ¼ã‚¯) | nvidia-smi |

---

### 6.2 å¯ç”¨æ€§è¦ä»¶

| é …ç›® | è¦ä»¶ |
|-----|------|
| **ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡** | 99% (æœˆé–“) |
| **MTBF (Mean Time Between Failures)** | >720æ™‚é–“ (30æ—¥) |
| **MTTR (Mean Time To Repair)** | <1æ™‚é–“ |
| **ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—** | æ—¥æ¬¡ |
| **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒæœŸé–“** | 30æ—¥ |

---

### 6.3 æ‹¡å¼µæ€§è¦ä»¶

| é …ç›® | è¦ä»¶ |
|-----|------|
| **ä¸¦åˆ—å®Ÿè¡Œæ•°** | æœ€å¤§32 (CPUã‚³ã‚¢æ•°ä¾å­˜) |
| **GPUæ•°** | æœ€å¤§4 (Multi-GPUå¯¾å¿œ) |
| **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º** | æœ€å¤§10GB (å˜ä¸€CSV) |
| **unique_idæ•°** | æœ€å¤§10,000 |
| **æ™‚ç³»åˆ—é•·** | æœ€å¤§100,000è¡Œ/unique_id |
| **ç‰¹å¾´é‡æ•°** | æœ€å¤§1,000 |

---

### 6.4 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶

| é …ç›® | è¦ä»¶ |
|-----|------|
| **èªè¨¼** | ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã®ãŸã‚ä¸è¦ï¼ˆå°†æ¥æ‹¡å¼µå¯èƒ½ï¼‰ |
| **ç§˜å¯†æƒ…å ±ç®¡ç†** | ç’°å¢ƒå¤‰æ•°ã®ã¿ã€ã‚³ãƒ¼ãƒ‰å†…ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ç¦æ­¢ |
| **PIIå‡¦ç†** | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœªå¯¾å¿œï¼ˆå¿…è¦æ™‚ã«åŒ¿ååŒ–å‡¦ç†è¿½åŠ ï¼‰ |
| **ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°** | ã™ã¹ã¦ã®APIå‘¼ã³å‡ºã—ã‚’ãƒ­ã‚°è¨˜éŒ² |
| **ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–** | ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æš—å·åŒ–æ¨å¥¨ |

---

### 6.5 ä¿å®ˆæ€§è¦ä»¶

| é …ç›® | è¦ä»¶ |
|-----|------|
| **ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸** | >90% (ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ) |
| **Pylintã‚¹ã‚³ã‚¢** | â‰¥8.5 |
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡** | 100% (å…¬é–‹API) |
| **ä¾å­˜é–¢ä¿‚ç®¡ç†** | pyproject.toml + requirements.txt |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†** | Git (ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°) |

---

## 7. APIä»•æ§˜è©³ç´°

### 7.1 REST API

**Base URL**: `http://localhost:8000/api/v1`

**èªè¨¼**: ç¾æ™‚ç‚¹ã§ã¯ä¸è¦ï¼ˆå°†æ¥æ‹¡å¼µå¯èƒ½ï¼‰

**å…±é€šãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼**:

```json
{
  "success": true,
  "data": {...},
  "error": null,
  "timestamp": "2025-11-03T12:34:56.789Z"
}
```

**ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼**:

```json
{
  "success": false,
  "data": null,
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Invalid input parameters",
    "details": {
      "field": "horizon",
      "reason": "must be positive integer"
    }
  },
  "timestamp": "2025-11-03T12:34:56.789Z"
}
```

---

### 7.2 ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§

#### 7.2.1 å®Ÿé¨“ç®¡ç†

```
POST   /experiments              å®Ÿé¨“ä½œæˆ
GET    /experiments              å®Ÿé¨“ä¸€è¦§å–å¾—
GET    /experiments/{id}         å®Ÿé¨“è©³ç´°å–å¾—
PUT    /experiments/{id}         å®Ÿé¨“æ›´æ–°
DELETE /experiments/{id}         å®Ÿé¨“å‰Šé™¤
```

#### 7.2.2 Runç®¡ç†

```
POST   /runs                     Runä½œæˆ
GET    /runs                     Runä¸€è¦§å–å¾—
GET    /runs/{id}                Runè©³ç´°å–å¾—
PUT    /runs/{id}/status         Runã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
DELETE /runs/{id}                Runå‰Šé™¤
GET    /runs/{id}/metrics        Runãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
GET    /runs/{id}/artifacts      Runã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¸€è¦§å–å¾—
```

#### 7.2.3 äºˆæ¸¬

```
POST   /predict                  äºˆæ¸¬å®Ÿè¡Œ
POST   /predict/batch            ãƒãƒƒãƒäºˆæ¸¬å®Ÿè¡Œ
GET    /predict/history          äºˆæ¸¬å±¥æ­´å–å¾—
```

#### 7.2.4 ãƒ¢ãƒ‡ãƒ«ç®¡ç†

```
POST   /models                   ãƒ¢ãƒ‡ãƒ«ç™»éŒ²
GET    /models                   ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—
GET    /models/{id}              ãƒ¢ãƒ‡ãƒ«è©³ç´°å–å¾—
PUT    /models/{id}/stage        ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¸å¤‰æ›´
DELETE /models/{id}              ãƒ¢ãƒ‡ãƒ«å‰Šé™¤
GET    /models/{id}/versions     ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸€è¦§å–å¾—
```

#### 7.2.5 ãƒ‡ãƒ¼ã‚¿ç®¡ç†

```
POST   /datasets                 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç™»éŒ²
GET    /datasets                 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§å–å¾—
GET    /datasets/{id}            ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—
POST   /datasets/{id}/profile    ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
```

---

### 7.3 è©³ç´°APIä»•æ§˜

#### 7.3.1 POST /predict

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**:

```json
{
  "model_name": "AutoNHITS_sales_forecast",
  "model_version": "v1.0.0",
  "data": {
    "unique_id": ["store_1", "store_2"],
    "ds": ["2025-11-03", "2025-11-03"],
    "y": [123.4, 234.5],
    "temperature": [15.2, 18.3],
    "promotion_flag": [0, 1]
  },
  "horizon": 7,
  "return_quantiles": true,
  "quantiles": [0.1, 0.5, 0.9]
}
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹**:

```json
{
  "success": true,
  "data": {
    "predictions": [
      {
        "unique_id": "store_1",
        "ds": "2025-11-04",
        "y_pred": 125.7,
        "y_pred_q10": 112.0,
        "y_pred_q50": 125.7,
        "y_pred_q90": 139.4
      },
      {
        "unique_id": "store_1",
        "ds": "2025-11-05",
        "y_pred": 127.3,
        "y_pred_q10": 113.5,
        "y_pred_q50": 127.3,
        "y_pred_q90": 141.1
      },
      ...
    ],
    "metadata": {
      "model_name": "AutoNHITS_sales_forecast",
      "model_version": "v1.0.0",
      "prediction_time": "2025-11-03T12:34:56.789Z",
      "input_rows": 2,
      "output_rows": 14,
      "latency_ms": 89.3
    }
  },
  "error": null,
  "timestamp": "2025-11-03T12:34:56.789Z"
}
```

---

## 8. ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¼ãƒè©³ç´°

### 8.1 PostgreSQLã‚¹ã‚­ãƒ¼ãƒ

#### 8.1.1 datasets (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)

```sql
CREATE TABLE datasets (
    id SERIAL PRIMARY KEY,
    dataset_version VARCHAR(16) UNIQUE NOT NULL,
    file_path TEXT NOT NULL,
    file_hash VARCHAR(64) NOT NULL,
    row_count INTEGER NOT NULL,
    column_count INTEGER NOT NULL,
    unique_id_count INTEGER NOT NULL,
    date_range_start TIMESTAMP,
    date_range_end TIMESTAMP,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_dataset_version ON datasets(dataset_version);
CREATE INDEX idx_created_at ON datasets(created_at DESC);
```

---

#### 8.1.2 experiments (å®Ÿé¨“)

```sql
CREATE TABLE experiments (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) UNIQUE NOT NULL,
    objective TEXT,
    description TEXT,
    tags JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR(100),
    status VARCHAR(50) DEFAULT 'active'
);

CREATE INDEX idx_exp_name ON experiments(name);
CREATE INDEX idx_exp_created_at ON experiments(created_at DESC);
CREATE INDEX idx_exp_status ON experiments(status);
```

---

#### 8.1.3 runs (å®Ÿè¡Œ)

```sql
CREATE TABLE runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    run_fingerprint VARCHAR(16) UNIQUE NOT NULL,
    run_name VARCHAR(255),
    status VARCHAR(50) DEFAULT 'pending',
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(50),
    hyperparameters JSONB NOT NULL,
    dataset_version VARCHAR(16) REFERENCES datasets(dataset_version),
    feature_set_id VARCHAR(100),
    training_window JSONB,
    code_revision VARCHAR(40),
    random_seed INTEGER,
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    duration_seconds FLOAT,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_run_exp_id ON runs(experiment_id);
CREATE INDEX idx_run_fingerprint ON runs(run_fingerprint);
CREATE INDEX idx_run_status ON runs(status);
CREATE INDEX idx_run_created_at ON runs(created_at DESC);
CREATE INDEX idx_run_model_name ON runs(model_name);
```

---

#### 8.1.4 metrics (ãƒ¡ãƒˆãƒªã‚¯ã‚¹)

```sql
CREATE TABLE metrics (
    id SERIAL PRIMARY KEY,
    run_id UUID NOT NULL REFERENCES runs(id) ON DELETE CASCADE,
    metric_name VARCHAR(100) NOT NULL,
    metric_value FLOAT NOT NULL,
    step INTEGER,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

CREATE INDEX idx_metric_run_id ON metrics(run_id);
CREATE INDEX idx_metric_name ON metrics(metric_name);
CREATE INDEX idx_metric_timestamp ON metrics(timestamp DESC);
CREATE INDEX idx_metric_run_name ON metrics(run_id, metric_name);
```

---

#### 8.1.5 artifacts (ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ)

```sql
CREATE TABLE artifacts (
    id SERIAL PRIMARY KEY,
    run_id UUID NOT NULL REFERENCES runs(id) ON DELETE CASCADE,
    artifact_type VARCHAR(50) NOT NULL,
    artifact_path TEXT NOT NULL,
    file_size_bytes BIGINT,
    file_hash VARCHAR(64),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_artifact_run_id ON artifacts(run_id);
CREATE INDEX idx_artifact_type ON artifacts(artifact_type);
CREATE INDEX idx_artifact_created_at ON artifacts(created_at DESC);
```

---

#### 8.1.6 models (ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª)

```sql
CREATE TABLE models (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    stage VARCHAR(50) DEFAULT 'Development',
    run_id UUID REFERENCES runs(id),
    model_path TEXT NOT NULL,
    model_size_bytes BIGINT,
    metadata JSONB,
    tags JSONB,
    registered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    promoted_at TIMESTAMP,
    promoted_by VARCHAR(100),
    UNIQUE(name, version)
);

CREATE INDEX idx_model_name ON models(name);
CREATE INDEX idx_model_stage ON models(stage);
CREATE INDEX idx_model_registered_at ON models(registered_at DESC);
```

---

#### 8.1.7 predictions (äºˆæ¸¬çµæœ)

```sql
CREATE TABLE predictions (
    id SERIAL PRIMARY KEY,
    model_id UUID REFERENCES models(id),
    unique_id VARCHAR(255) NOT NULL,
    ds TIMESTAMP NOT NULL,
    y_pred FLOAT NOT NULL,
    y_pred_lower FLOAT,
    y_pred_upper FLOAT,
    y_actual FLOAT,
    prediction_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

CREATE INDEX idx_pred_model_id ON predictions(model_id);
CREATE INDEX idx_pred_unique_id ON predictions(unique_id);
CREATE INDEX idx_pred_ds ON predictions(ds DESC);
CREATE INDEX idx_pred_time ON predictions(prediction_time DESC);
```

---

#### 8.1.8 feature_importances (ç‰¹å¾´é‡é‡è¦åº¦)

```sql
CREATE TABLE feature_importances (
    id SERIAL PRIMARY KEY,
    run_id UUID NOT NULL REFERENCES runs(id) ON DELETE CASCADE,
    feature_name VARCHAR(255) NOT NULL,
    importance_value FLOAT NOT NULL,
    importance_std FLOAT,
    rank INTEGER,
    method VARCHAR(50),
    computed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_fi_run_id ON feature_importances(run_id);
CREATE INDEX idx_fi_feature_name ON feature_importances(feature_name);
CREATE INDEX idx_fi_rank ON feature_importances(run_id, rank);
```

---

### 8.2 JSONBãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è©³ç´°

#### 8.2.1 hyperparameters (JSONB)

```json
{
  "input_size": 14,
  "h": 7,
  "loss": "MAE",
  "scaler": "standard",
  "max_steps": 1000,
  "learning_rate": 0.001,
  "batch_size": 32,
  "hidden_size": 128,
  "num_layers": 2,
  "dropout": 0.1,
  "optimizer": "Adam",
  "early_stopping_patience": 10
}
```

---

#### 8.2.2 metadata (JSONB)

```json
{
  "training_time_seconds": 123.45,
  "gpu_id": 0,
  "gpu_name": "NVIDIA RTX 5070 Ti",
  "peak_memory_mb": 8192,
  "num_parameters": 1234567,
  "pytorch_version": "2.0.1",
  "cuda_version": "11.8",
  "hostname": "ml-server-01",
  "python_version": "3.11.4"
}
```

---

#### 8.2.3 training_window (JSONB)

```json
{
  "start_date": "2023-01-01",
  "end_date": "2024-12-31",
  "split_method": "rolling_origin",
  "n_splits": 5,
  "horizon": 7,
  "gap": 0,
  "expanding_window": true
}
```

---

## 9. å“è³ªå±æ€§

### 9.1 Reusability (å†åˆ©ç”¨æ€§)

**ç›®æ¨™**: â­â­â­â­â­ (5/5)

**å®Ÿç¾æ–¹æ³•**:
- ãƒ¬ã‚¤ãƒ¤ãƒ¼åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆ9å±¤ï¼‰
- Adapter Patternã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«æŠ½è±¡åŒ–
- ä¾å­˜æ€§æ³¨å…¥ã«ã‚ˆã‚‹ç–çµåˆ
- å‹ãƒ’ãƒ³ãƒˆå®Œå…¨å¯¾å¿œ

**è©•ä¾¡åŸºæº–**:
- ä»–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®æµç”¨å¯èƒ½æ€§ >80%
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®‰å®šæ€§
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œå‚™

---

### 9.2 Testability (ãƒ†ã‚¹ãƒˆå®¹æ˜“æ€§)

**ç›®æ¨™**: â­â­â­â­â­ (5/5)

**å®Ÿç¾æ–¹æ³•**:
- ä¾å­˜æ€§æ³¨å…¥ã«ã‚ˆã‚‹ãƒ¢ãƒƒã‚¯å¯èƒ½æ€§
- ç´”ç²‹é–¢æ•°ã®å¤šç”¨
- TDD (Test-Driven Development)
- Fixture/Mocksã®æ•´å‚™

**è©•ä¾¡åŸºæº–**:
- ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >90%
- çµ±åˆãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >80%
- E2Eãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >70%

---

### 9.3 Maintainability (ä¿å®ˆæ€§)

**ç›®æ¨™**: â­â­â­â­â­ (5/5)

**å®Ÿç¾æ–¹æ³•**:
- SOLIDåŸå‰‡éµå®ˆ
- ä½çµåˆãƒ»é«˜å‡é›†
- æ˜ç¢ºãªå‘½åè¦å‰‡
- åŒ…æ‹¬çš„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

**è©•ä¾¡åŸºæº–**:
- Pylintã‚¹ã‚³ã‚¢ â‰¥8.5
- å¾ªç’°çš„è¤‡é›‘åº¦ <10
- é–¢æ•°ã®å¹³å‡è¡Œæ•° <50è¡Œ

---

### 9.4 Extensibility (æ‹¡å¼µæ€§)

**ç›®æ¨™**: â­â­â­â­â­ (5/5)

**å®Ÿç¾æ–¹æ³•**:
- ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- Factory Pattern
- Strategy Pattern
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

**è©•ä¾¡åŸºæº–**:
- æ–°ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ å·¥æ•° <2æ—¥
- æ–°ç‰¹å¾´é‡ã®è¿½åŠ å·¥æ•° <0.5æ—¥
- æ–°è©•ä¾¡æŒ‡æ¨™ã®è¿½åŠ å·¥æ•° <0.5æ—¥

---

### 9.5 Reliability (ä¿¡é ¼æ€§)

**ç›®æ¨™**: â­â­â­â­ (4/5)

**å®Ÿç¾æ–¹æ³•**:
- åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒªãƒˆãƒ©ã‚¤ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
- å†ªç­‰æ€§ä¿è¨¼
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

**è©•ä¾¡åŸºæº–**:
- ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡ >99%
- MTBF >720æ™‚é–“
- MTTR <1æ™‚é–“

---

### 9.6 Performance (æ€§èƒ½)

**ç›®æ¨™**: â­â­â­â­ (4/5)

**å®Ÿç¾æ–¹æ³•**:
- ä¸¦åˆ—å®Ÿè¡Œå¯¾å¿œ
- ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ 
- GPUæ´»ç”¨

**è©•ä¾¡åŸºæº–**:
- å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚é–“ <10åˆ†
- 100ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’ <2æ™‚é–“
- äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· <100ms

---

### 9.7 Security (ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£)

**ç›®æ¨™**: â­â­â­â­ (4/5)

**å®Ÿç¾æ–¹æ³•**:
- ç§˜å¯†æƒ…å ±ã®ç’°å¢ƒå¤‰æ•°ç®¡ç†
- PIIé™¤å¤–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- ãƒ‘ã‚¹æ¤œè¨¼
- ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°

**è©•ä¾¡åŸºæº–**:
- è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³ pass
- ç§˜å¯†æƒ…å ±ã®æ¼æ´© 0ä»¶
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆ 0ä»¶

---

### 9.8 Compatibility (äº’æ›æ€§)

**ç›®æ¨™**: â­â­â­â­â­ (5/5)

**å®Ÿç¾æ–¹æ³•**:
- Adapter Pattern
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
- æ®µéšçš„ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- å¾Œæ–¹äº’æ›æ€§ä¿è¨¼

**è©•ä¾¡åŸºæº–**:
- Python 3.11+ å¯¾å¿œ
- PyTorch 2.0+ å¯¾å¿œ
- è¤‡æ•°OSã‚µãƒãƒ¼ãƒˆï¼ˆLinux/macOS/Windowsï¼‰

---

## 10. åˆ¶ç´„äº‹é …

### 10.1 æ©Ÿèƒ½åˆ¶ç´„

| åˆ¶ç´„ | å†…å®¹ |
|-----|------|
| **ãƒ‡ãƒ¼ã‚¿å½¢å¼** | CSVå½¢å¼ã®ã¿ï¼ˆParquetã¯å°†æ¥å¯¾å¿œï¼‰ |
| **æ™‚é–“ç²’åº¦** | æ—¥æ¬¡ï½æœˆæ¬¡ã‚’æƒ³å®šï¼ˆç§’å˜ä½ã¯æœªå¯¾å¿œï¼‰ |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬** | ãƒãƒƒãƒä¸­å¿ƒï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯æœªå¯¾å¿œï¼‰ |
| **PIIå‡¦ç†** | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæœªå¯¾å¿œï¼ˆåˆ¥é€”åŒ¿ååŒ–å‡¦ç†ãŒå¿…è¦ï¼‰ |

---

### 10.2 æŠ€è¡“åˆ¶ç´„

| åˆ¶ç´„ | å†…å®¹ |
|-----|------|
| **Pythonç‰ˆ** | 3.11ä»¥ä¸Šå¿…é ˆ |
| **PostgreSQL** | 14ä»¥ä¸Šæ¨å¥¨ |
| **GPU** | CUDA 11.0ä»¥ä¸Šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ |
| **ãƒ¡ãƒ¢ãƒª** | 16GBä»¥ä¸Šæ¨å¥¨ |

---

### 10.3 é‹ç”¨åˆ¶ç´„

| åˆ¶ç´„ | å†…å®¹ |
|-----|------|
| **å®Ÿè¡Œç’°å¢ƒ** | ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ã¯å°†æ¥å¯¾å¿œï¼‰ |
| **èªè¨¼** | ç¾æ™‚ç‚¹ã§ã¯ä¸è¦ |
| **ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ** | å˜ä¸€ã‚µãƒ¼ãƒãƒ¼ï¼ˆåˆ†æ•£å®Ÿè¡Œã¯å°†æ¥å¯¾å¿œï¼‰ |

---

## 11. ãƒªã‚¹ã‚¯ã¨å¯¾ç­–

### 11.1 æŠ€è¡“ãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– |
|-------|------|---------|------|
| **OOM (Out of Memory)** | é«˜ | ä¸­ | ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã€ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° |
| **GPU OOM** | ä¸­ | ä¸­ | Mixed Precisionã€Gradient Accumulation |
| **å­¦ç¿’æ™‚é–“è¶…é** | ä¸­ | ä½ | Early Stoppingã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |
| **ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ** | é«˜ | é«˜ | æ¤œè¨¼å¼·åŒ–ã€ç•°å¸¸æ¤œå‡º |
| **ä¾å­˜é–¢ä¿‚ã®ç«¶åˆ** | ä¸­ | ä½ | ä»®æƒ³ç’°å¢ƒã€Dockerã‚³ãƒ³ãƒ†ãƒŠ |

---

### 11.2 é‹ç”¨ãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– |
|-------|------|---------|------|
| **ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³** | é«˜ | ä¸­ | è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã€ç›£è¦– |
| **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½åŠ£åŒ–** | é«˜ | ä¸­ | ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡ºã€è‡ªå‹•å†å­¦ç¿’ |
| **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—** | é«˜ | ä½ | å†—é•·åŒ–ã€ç›£è¦– |
| **ãƒ­ã‚°è‚¥å¤§åŒ–** | ä¸­ | é«˜ | ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€åœ§ç¸® |

---

### 11.3 ãƒ“ã‚¸ãƒã‚¹ãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– |
|-------|------|---------|------|
| **è¦ä»¶å¤‰æ›´** | ä¸­ | é«˜ | ã‚¢ã‚¸ãƒ£ã‚¤ãƒ«é–‹ç™ºã€æŸ”è»Ÿãªè¨­è¨ˆ |
| **ç´æœŸé…å»¶** | é«˜ | ä¸­ | MVPå„ªå…ˆã€æ®µéšçš„ãƒªãƒªãƒ¼ã‚¹ |
| **ã‚¹ã‚­ãƒ«ä¸è¶³** | ä¸­ | ä¸­ | ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™ |

---

## 12. ä»˜éŒ²

### 12.1 å‚è€ƒè³‡æ–™

| è³‡æ–™ | URL |
|-----|-----|
| **NeuralForecastå…¬å¼** | https://nixtlaverse.nixtla.io/neuralforecast/ |
| **MLflowå…¬å¼** | https://mlflow.org/ |
| **Optunaå…¬å¼** | https://optuna.org/ |
| **Rayå…¬å¼** | https://www.ray.io/ |
| **Hydraå…¬å¼** | https://hydra.cc/ |

---

### 12.2 ç”¨èªé›†

è©³ç´°ã¯ [2. ç”¨èªå®šç¾©](#2-ç”¨èªå®šç¾©) ã‚’å‚ç…§

---

### 12.3 å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | å¤‰æ›´å†…å®¹ | æ‹…å½“è€… |
|------|-----------|---------|--------|
| 2025-11-03 | v1.0.0 | åˆç‰ˆä½œæˆ | Claude Team |

---

### 12.4 æ‰¿èª

| å½¹å‰² | æ°å | æ‰¿èªæ—¥ | ç½²å |
|-----|------|--------|------|
| Product Owner | - | - | - |
| Tech Lead | - | - | - |
| QA Lead | - | - | - |

---

**End of Document**

---

**ç·ãƒšãƒ¼ã‚¸æ•°**: ç´„200ãƒšãƒ¼ã‚¸ç›¸å½“
**ç·æ–‡å­—æ•°**: ç´„50,000æ–‡å­—
**è©³ç´°åº¦**: é«˜ç²¾ç´°ï¼ˆã‚¯ãƒ©ã‚¹ã€ãƒ¡ã‚½ãƒƒãƒ‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€å¤‰æ•°ãƒ¬ãƒ™ãƒ«ï¼‰
