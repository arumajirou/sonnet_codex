# ğŸ“¦ 10_DEPLOYMENT_GUIDE.md - ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰

**Time Series Forecasting System - Deployment Guide**

---

## ğŸ“‹ ç›®æ¬¡

1. [æ¦‚è¦](#1-æ¦‚è¦)
2. [ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶](#2-ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶)
3. [å‰ææ¡ä»¶](#3-å‰ææ¡ä»¶)
4. [ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤](#4-ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤)
5. [Dockerç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤](#5-dockerç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤)
6. [æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤](#6-æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™ä¸­)
7. [ç’°å¢ƒå¤‰æ•°è¨­å®š](#7-ç’°å¢ƒå¤‰æ•°è¨­å®š)
8. [ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š](#8-ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š)
9. [ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š](#9-ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š)
10. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š](#10-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š)
11. [ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°è¨­å®š](#11-ç›£è¦–ãƒ­ã‚®ãƒ³ã‚°è¨­å®š)
12. [ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§æ‰‹é †](#12-ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©æ—§æ‰‹é †)
13. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#13-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
14. [ä»˜éŒ²](#14-ä»˜éŒ²)

---

## 1. æ¦‚è¦

### 1.1 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç›®çš„

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å„ç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ã‚’æä¾›ã—ã¾ã™ã€‚

### 1.2 å¯¾è±¡èª­è€…

- **é–‹ç™ºè€…**: ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- **DevOpsã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢**: Docker/Kubernetesãƒ‡ãƒ—ãƒ­ã‚¤
- **é‹ç”¨æ‹…å½“è€…**: æœ¬ç•ªç’°å¢ƒã®ç®¡ç†ãƒ»ä¿å®ˆ

### 1.3 ãƒ‡ãƒ—ãƒ­ã‚¤ç’°å¢ƒ

| ç’°å¢ƒ | ç”¨é€” | ã‚¤ãƒ³ãƒ•ãƒ© | å¯ç”¨æ€§ |
|-----|------|---------|--------|
| **Local** | é–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚° | ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ | N/A |
| **Docker** | çµ±åˆãƒ†ã‚¹ãƒˆãƒ»ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚° | Docker Compose | å˜ä¸€ãƒ›ã‚¹ãƒˆ |
| **Production** | æœ¬ç•ªé‹ç”¨ | Kubernetes/VM | é«˜å¯ç”¨æ€§ |

---

## 2. ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶

### 2.1 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

#### æœ€å°æ§‹æˆ

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | æœ€å°è¦ä»¶ | æ¨å¥¨è¦ä»¶ |
|--------------|---------|---------|
| **CPU** | 4ã‚³ã‚¢ | 8ã‚³ã‚¢ä»¥ä¸Š |
| **ãƒ¡ãƒ¢ãƒª** | 8GB | 16GBä»¥ä¸Š |
| **ãƒ‡ã‚£ã‚¹ã‚¯** | 50GB | 100GBä»¥ä¸Šï¼ˆSSDï¼‰ |
| **GPU** | ãªã— | NVIDIA GPUï¼ˆCUDAå¯¾å¿œï¼‰ |
| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** | 10Mbps | 100Mbpsä»¥ä¸Š |

#### æ¨å¥¨æ§‹æˆï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | æ¨å¥¨è¦ä»¶ |
|--------------|---------|
| **CPU** | 16ã‚³ã‚¢ä»¥ä¸Š |
| **ãƒ¡ãƒ¢ãƒª** | 32GBä»¥ä¸Š |
| **ãƒ‡ã‚£ã‚¹ã‚¯** | 500GBä»¥ä¸Šï¼ˆNVMe SSDï¼‰ |
| **GPU** | NVIDIA A100/V100 |
| **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** | 1Gbpsä»¥ä¸Š |

---

### 2.2 ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶

#### å¿…é ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢

| ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ç”¨é€” |
|------------|-----------|------|
| **Python** | 3.11.x | ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ |
| **PostgreSQL** | 14.xä»¥ä¸Š | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ |
| **pip** | æœ€æ–°ç‰ˆ | ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç† |

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢

| ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ç”¨é€” |
|------------|-----------|------|
| **Docker** | 24.xä»¥ä¸Š | ã‚³ãƒ³ãƒ†ãƒŠåŒ– |
| **Docker Compose** | 2.xä»¥ä¸Š | ãƒãƒ«ãƒã‚³ãƒ³ãƒ†ãƒŠç®¡ç† |
| **MLflow** | 2.xä»¥ä¸Š | å®Ÿé¨“ç®¡ç† |
| **Ray** | 2.xä»¥ä¸Š | åˆ†æ•£å®Ÿè¡Œ |
| **CUDA** | 11.8ä»¥ä¸Š | GPUè¨ˆç®— |

---

### 2.3 OSäº’æ›æ€§

| OS | ã‚µãƒãƒ¼ãƒˆãƒ¬ãƒ™ãƒ« | å‚™è€ƒ |
|----|--------------|------|
| **Ubuntu** 20.04+ | å®Œå…¨ã‚µãƒãƒ¼ãƒˆ | æ¨å¥¨ |
| **macOS** 12+ | å®Œå…¨ã‚µãƒãƒ¼ãƒˆ | Apple Siliconå¯¾å¿œ |
| **Windows** 10+ | ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆ | WSL2æ¨å¥¨ |
| **CentOS/RHEL** 8+ | ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆ | - |

---

## 3. å‰ææ¡ä»¶

### 3.1 å¿…è¦ãªçŸ¥è­˜

- **åŸºæœ¬çš„ãªã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³æ“ä½œ**
- **Pythonä»®æƒ³ç’°å¢ƒã®ç†è§£**
- **Docker/Docker Composeã®åŸºç¤çŸ¥è­˜**ï¼ˆDockerç’°å¢ƒã®å ´åˆï¼‰
- **PostgreSQLã®åŸºæœ¬æ“ä½œ**

### 3.2 äº‹å‰æº–å‚™

#### 3.2.1 ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™

```bash
# å¿…è¦ãªæ¨©é™ã‚’ç¢ºèª
id  # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¢ºèª
groups  # æ‰€å±ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¢ºèª

# Dockerã‚°ãƒ«ãƒ¼ãƒ—ã¸ã®è¿½åŠ ï¼ˆDockerä½¿ç”¨ã®å ´åˆï¼‰
sudo usermod -aG docker $USER
newgrp docker  # ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å³åº§ã«åæ˜ 
```

#### 3.2.2 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š

```bash
# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
sudo ufw allow 5432/tcp  # PostgreSQL
sudo ufw allow 8000/tcp  # FastAPI
sudo ufw allow 5000/tcp  # MLflow
sudo ufw allow 8265/tcp  # Ray Dashboard

# SELinuxè¨­å®šï¼ˆCentOS/RHELã®å ´åˆï¼‰
sudo setenforce 0  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
# ã¾ãŸã¯
sudo vi /etc/selinux/config
# SELINUX=permissive ã«å¤‰æ›´
```

#### 3.2.3 å¿…è¦ãªãƒãƒ¼ãƒˆ

| ãƒãƒ¼ãƒˆ | ã‚µãƒ¼ãƒ“ã‚¹ | ç”¨é€” |
|-------|---------|------|
| 5432 | PostgreSQL | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ |
| 8000 | FastAPI | Web API |
| 5000 | MLflow | å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° |
| 8265 | Ray | åˆ†æ•£å®Ÿè¡Œãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ |
| 9090 | Prometheus | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† |
| 3000 | Grafana | ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ |

---

## 4. ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

### 4.1 ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ•ãƒ­ãƒ¼

```mermaid
graph TB
    A[é–‹å§‹] --> B[Python 3.11ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]
    B --> C[ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³]
    C --> D[ä»®æƒ³ç’°å¢ƒä½œæˆ]
    D --> E[ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]
    E --> F[PostgreSQLã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—]
    F --> G[ç’°å¢ƒå¤‰æ•°è¨­å®š]
    G --> H[ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–]
    H --> I[å‹•ä½œç¢ºèª]
    I --> J[å®Œäº†]
```

---

### 4.2 Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 4.2.1 Python 3.11ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**Ubuntu/Debian**:

```bash
# ãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ 
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update

# Python 3.11ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt install python3.11 python3.11-venv python3.11-dev

# ç¢ºèª
python3.11 --version
```

**macOS**:

```bash
# HomebrewçµŒç”±ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install python@3.11

# ç¢ºèª
python3.11 --version
```

**Windows**:

```powershell
# Pythonå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# https://www.python.org/downloads/

# ã¾ãŸã¯ Chocolateyä½¿ç”¨
choco install python311

# ç¢ºèª
python --version
```

---

#### 4.2.2 ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
# HTTPSã§ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-org/time-series-forecasting-system.git
cd time-series-forecasting-system

# ã¾ãŸã¯SSHã§ã‚¯ãƒ­ãƒ¼ãƒ³
git clone git@github.com:your-org/time-series-forecasting-system.git
cd time-series-forecasting-system

# ãƒ–ãƒ©ãƒ³ãƒç¢ºèª
git branch -a
git checkout main  # ã¾ãŸã¯ develop
```

---

#### 4.2.3 ä»®æƒ³ç’°å¢ƒã®ä½œæˆ

**æ–¹æ³•1: venvï¼ˆæ¨™æº–ï¼‰**:

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python3.11 -m venv .venv

# æœ‰åŠ¹åŒ–
source .venv/bin/activate  # Linux/macOS
# ã¾ãŸã¯
.venv\Scripts\activate  # Windows

# ä»®æƒ³ç’°å¢ƒãŒæœ‰åŠ¹ã«ãªã£ãŸã“ã¨ã‚’ç¢ºèª
which python  # ä»®æƒ³ç’°å¢ƒã®Pythonãƒ‘ã‚¹ãŒè¡¨ç¤ºã•ã‚Œã‚‹
python --version  # Python 3.11.x
```

**æ–¹æ³•2: Poetryï¼ˆæ¨å¥¨ï¼‰**:

```bash
# Poetryã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -sSL https://install.python-poetry.org | python3 -

# Poetryã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
export PATH="$HOME/.local/bin:$PATH"

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
poetry install

# ä»®æƒ³ç’°å¢ƒæœ‰åŠ¹åŒ–
poetry shell

# ã¾ãŸã¯ã€ä»®æƒ³ç’°å¢ƒå†…ã§ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
poetry run python --version
```

---

#### 4.2.4 ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# pipã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
pip install --upgrade pip setuptools wheel

# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# é–‹ç™ºç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install -r requirements-dev.txt

# é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e .

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
pip list
pip check  # ä¾å­˜é–¢ä¿‚ã®æ•´åˆæ€§ç¢ºèª
```

**requirements.txtï¼ˆä¸»è¦ä¾å­˜é–¢ä¿‚ï¼‰**:

```txt
# Core
neuralforecast>=1.6.0,<2.0.0
pandas>=2.0.0,<3.0.0
numpy>=1.24.0,<2.0.0
torch>=2.0.0,<3.0.0

# Database
sqlalchemy>=2.0.0,<3.0.0
psycopg2-binary>=2.9.0
alembic>=1.12.0

# API
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.0.0

# Tracking (Optional)
mlflow>=2.8.0
wandb>=0.16.0

# Distributed Computing (Optional)
ray[default]>=2.8.0

# Hyperparameter Optimization
optuna>=3.4.0

# Utilities
python-dotenv>=1.0.0
typer>=0.9.0
rich>=13.6.0
```

---

### 4.3 PostgreSQLã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 4.3.1 PostgreSQLã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**Ubuntu/Debian**:

```bash
# PostgreSQL 14ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt install postgresql-14 postgresql-contrib-14

# ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
sudo systemctl start postgresql
sudo systemctl enable postgresql

# çŠ¶æ…‹ç¢ºèª
sudo systemctl status postgresql
```

**macOS**:

```bash
# Homebrewã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install postgresql@14

# ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
brew services start postgresql@14

# ç¢ºèª
psql --version
```

**Dockerï¼ˆæ¨å¥¨ï¼‰**:

```bash
# PostgreSQLã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•
docker run -d \
  --name ts-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=yourpassword \
  -e POSTGRES_DB=ts_forecast_system \
  -p 5432:5432 \
  -v postgres-data:/var/lib/postgresql/data \
  postgres:14

# ç¢ºèª
docker ps | grep ts-postgres
```

---

#### 4.3.2 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ

```bash
# PostgreSQLã«æ¥ç¶š
sudo -u postgres psql

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
CREATE DATABASE ts_forecast_system;

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
CREATE USER ts_user WITH ENCRYPTED PASSWORD 'yourpassword';

# æ¨©é™ä»˜ä¸
GRANT ALL PRIVILEGES ON DATABASE ts_forecast_system TO ts_user;

# ç¢ºèª
\l  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¸€è¦§
\du  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§
\q  # çµ‚äº†
```

---

#### 4.3.3 æ¥ç¶šç¢ºèª

```bash
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ
psql -h localhost -U ts_user -d ts_forecast_system -W

# SQLå®Ÿè¡Œãƒ†ã‚¹ãƒˆ
SELECT version();
SELECT current_database();

# çµ‚äº†
\q
```

---

### 4.4 ç’°å¢ƒå¤‰æ•°è¨­å®š

#### 4.4.1 .envãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

```bash
# ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚³ãƒ”ãƒ¼
cp .env.example .env

# ã‚¨ãƒ‡ã‚£ã‚¿ã§ç·¨é›†
vi .env  # ã¾ãŸã¯ nano, code ãªã©
```

**æœ€å°æ§‹æˆã®.env**:

```bash
# ========================================
# Database Configuration
# ========================================
DATABASE_URL=postgresql://ts_user:yourpassword@localhost:5432/ts_forecast_system
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20
DATABASE_POOL_TIMEOUT=30
DATABASE_ECHO=false

# ========================================
# Path Configuration
# ========================================
DATA_DIR=./data
MODEL_DIR=./models
OUTPUT_DIR=./outputs
LOG_DIR=./logs

# ========================================
# Execution Configuration
# ========================================
MAX_PARALLEL_RUNS=4
DEFAULT_BACKEND=cuda  # cuda, cpu, mps
ENABLE_GPU=true
GPU_MEMORY_LIMIT=0.8

# ========================================
# Logging Configuration
# ========================================
LOG_LEVEL=INFO
LOG_FORMAT=json
ENABLE_FILE_LOGGING=true

# ========================================
# Optional Services
# ========================================
ENABLE_MLFLOW=false
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=time_series_forecasting

ENABLE_WANDB=false
WANDB_API_KEY=your_wandb_key
WANDB_PROJECT=time-series-forecasting

ENABLE_RAY=false
RAY_ADDRESS=auto
RAY_NAMESPACE=ts_forecast
```

---

#### 4.4.2 ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿

```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
source .env

# ã¾ãŸã¯
export $(cat .env | grep -v '^#' | xargs)

# ç¢ºèª
echo $DATABASE_URL
echo $DATA_DIR
```

---

### 4.5 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ

```bash
# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
mkdir -p data/{raw,processed,external,interim}
mkdir -p models/{staging,production}
mkdir -p outputs/{runs,predictions,plots,reports}
mkdir -p logs/{app,training,inference}

# æ¨©é™è¨­å®š
chmod -R 755 data models outputs logs

# ç¢ºèª
tree -L 2 -d
```

**æœŸå¾…ã•ã‚Œã‚‹æ§‹é€ **:

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ external/
â”‚   â””â”€â”€ interim/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ production/
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ predictions/
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ reports/
â””â”€â”€ logs/
    â”œâ”€â”€ app/
    â”œâ”€â”€ training/
    â””â”€â”€ inference/
```

---

### 4.6 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–

#### 4.6.1 Alembicãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```bash
# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ç¢ºèª
alembic history

# æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
alembic upgrade head

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºèª
alembic current

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒç¢ºèª
psql -h localhost -U ts_user -d ts_forecast_system -c "\dt"
```

---

#### 4.6.2 åˆæœŸãƒ‡ãƒ¼ã‚¿æŠ•å…¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

```bash
# ã‚·ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
python scripts/setup/seed_data.py

# ç¢ºèª
psql -h localhost -U ts_user -d ts_forecast_system << EOF
SELECT COUNT(*) FROM experiments;
SELECT COUNT(*) FROM models;
EOF
```

---

### 4.7 å‹•ä½œç¢ºèª

#### 4.7.1 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```bash
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
python -c "
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv

load_dotenv()
engine = create_engine(os.getenv('DATABASE_URL'))
with engine.connect() as conn:
    result = conn.execute('SELECT 1')
    print('Database connection: OK')
"
```

---

#### 4.7.2 ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
pytest tests/unit/ -v

# çµ±åˆãƒ†ã‚¹ãƒˆ
pytest tests/integration/ -v

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ã
pytest tests/ -v --cov=src/nf_auto_runner --cov-report=html

# ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç¢ºèª
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

---

#### 4.7.3 ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ

```bash
# CLIãƒ˜ãƒ«ãƒ—è¡¨ç¤º
python -m nf_auto_runner.app.main --help

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ
python -m nf_auto_runner.app.main \
  --data-path ./data/sample/sample_data.csv \
  --models NHITS,PatchTST \
  --horizons 24,48 \
  --backend cpu \
  --output-dir ./outputs/test_run

# å®Ÿè¡Œçµæœç¢ºèª
ls -lh ./outputs/test_run/
```

---

## 5. Dockerç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤

### 5.1 Dockerã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ•ãƒ­ãƒ¼

```mermaid
graph TB
    A[é–‹å§‹] --> B[Docker/Docker Composeã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]
    B --> C[.envãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ]
    C --> D[docker-compose.ymlãƒ¬ãƒ“ãƒ¥ãƒ¼]
    D --> E[ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰]
    E --> F[ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•]
    F --> G[ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯]
    G --> H[å‹•ä½œç¢ºèª]
    H --> I[å®Œäº†]
```

---

### 5.2 Dockerç’°å¢ƒæº–å‚™

#### 5.2.1 Dockerã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**Ubuntu/Debian**:

```bash
# å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³å‰Šé™¤
sudo apt remove docker docker-engine docker.io containerd runc

# ãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ 
sudo apt update
sudo apt install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \
  sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
  https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Dockerã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io \
  docker-buildx-plugin docker-compose-plugin

# Dockerã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 
sudo usermod -aG docker $USER
newgrp docker

# ç¢ºèª
docker --version
docker compose version
```

**macOS**:

```bash
# Docker Desktopã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install --cask docker

# Dockerèµ·å‹•
open -a Docker

# ç¢ºèª
docker --version
docker compose version
```

---

### 5.3 Docker Composeè¨­å®š

#### 5.3.1 docker-compose.yml

```yaml
version: '3.8'

services:
  # ========================================
  # PostgreSQL Database
  # ========================================
  postgres:
    image: postgres:14-alpine
    container_name: ts-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?error}
      POSTGRES_DB: ${POSTGRES_DB:-ts_forecast_system}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/db/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ts-network
    restart: unless-stopped

  # ========================================
  # Application
  # ========================================
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        PYTHON_VERSION: "3.11"
    container_name: ts-app
    env_file:
      - .env
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-ts_forecast_system}
      MLFLOW_TRACKING_URI: ${ENABLE_MLFLOW:-false}
      RAY_ADDRESS: ${ENABLE_RAY:-false}
    volumes:
      - ./data:/app/data:rw
      - ./models:/app/models:rw
      - ./outputs:/app/outputs:rw
      - ./logs:/app/logs:rw
      - ./src:/app/src:ro
      - ./conf:/app/conf:ro
    ports:
      - "${APP_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      mlflow:
        condition: service_started
        required: false
      ray-head:
        condition: service_started
        required: false
    networks:
      - ts-network
    restart: unless-stopped
    command: uvicorn nf_auto_runner.app.api:app --host 0.0.0.0 --port 8000

  # ========================================
  # MLflow Server (Optional)
  # ========================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.8.1
    container_name: ts-mlflow
    environment:
      BACKEND_STORE_URI: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-ts_forecast_system}
      ARTIFACT_ROOT: /mlflow/artifacts
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL:-}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
    volumes:
      - mlflow-artifacts:/mlflow/artifacts
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - ts-network
    restart: unless-stopped
    command: >
      mlflow server
      --backend-store-uri ${BACKEND_STORE_URI}
      --default-artifact-root ${ARTIFACT_ROOT}
      --host 0.0.0.0
      --port 5000
    profiles:
      - mlflow

  # ========================================
  # Ray Head (Optional)
  # ========================================
  ray-head:
    image: rayproject/ray:2.8.1-py311
    container_name: ts-ray-head
    environment:
      RAY_memory_monitor_refresh_ms: 0
    volumes:
      - ./data:/data:ro
      - ./models:/models:rw
      - ray-data:/tmp/ray
    ports:
      - "${RAY_DASHBOARD_PORT:-8265}:8265"
      - "6379:6379"
      - "10001:10001"
    networks:
      - ts-network
    restart: unless-stopped
    command: >
      ray start
      --head
      --dashboard-host=0.0.0.0
      --dashboard-port=8265
      --port=6379
      --block
    shm_size: '2gb'
    profiles:
      - ray

  # ========================================
  # Ray Worker (Optional)
  # ========================================
  ray-worker:
    image: rayproject/ray:2.8.1-py311
    environment:
      RAY_memory_monitor_refresh_ms: 0
    volumes:
      - ./data:/data:ro
      - ./models:/models:rw
      - ray-data:/tmp/ray
    depends_on:
      - ray-head
    networks:
      - ts-network
    restart: unless-stopped
    command: >
      ray start
      --address=ray-head:6379
      --block
    deploy:
      replicas: ${RAY_WORKER_REPLICAS:-2}
    shm_size: '2gb'
    profiles:
      - ray

  # ========================================
  # Prometheus (Optional)
  # ========================================
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: ts-prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    networks:
      - ts-network
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    profiles:
      - monitoring

  # ========================================
  # Grafana (Optional)
  # ========================================
  grafana:
    image: grafana/grafana:10.1.5
    container_name: ts-grafana
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:?error}
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
      - grafana-data:/var/lib/grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    depends_on:
      - prometheus
    networks:
      - ts-network
    restart: unless-stopped
    profiles:
      - monitoring

# ========================================
# Networks
# ========================================
networks:
  ts-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16

# ========================================
# Volumes
# ========================================
volumes:
  postgres-data:
    driver: local
  mlflow-artifacts:
    driver: local
  ray-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
```

---

#### 5.3.2 Dockerfile

```dockerfile
# ========================================
# Multi-stage Build
# ========================================

# ----------------------------------------
# Stage 1: Builder
# ----------------------------------------
FROM python:3.11-slim as builder

# å¼•æ•°
ARG PYTHON_VERSION=3.11

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
WORKDIR /build

# ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Pythonä¾å­˜é–¢ä¿‚
COPY requirements.txt .
RUN pip install --user --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --user --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰
COPY src /build/src
COPY setup.py pyproject.toml README.md /build/
RUN pip install --user --no-cache-dir -e .

# ----------------------------------------
# Stage 2: Runtime
# ----------------------------------------
FROM python:3.11-slim

# ãƒ©ãƒ™ãƒ«
LABEL maintainer="your-team@example.com"
LABEL version="1.0.0"
LABEL description="Time Series Forecasting System"

# ç’°å¢ƒå¤‰æ•°
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PATH=/home/appuser/.local/bin:$PATH

# érootãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
RUN groupadd -g 1000 appuser && \
    useradd -r -u 1000 -g appuser -m -d /home/appuser -s /bin/bash appuser

# ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ä¾å­˜é–¢ä¿‚
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Builderã‚¹ãƒ†ãƒ¼ã‚¸ã‹ã‚‰ã‚³ãƒ”ãƒ¼
COPY --from=builder --chown=appuser:appuser /root/.local /home/appuser/.local

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
WORKDIR /app

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
COPY --chown=appuser:appuser src /app/src
COPY --chown=appuser:appuser conf /app/conf
COPY --chown=appuser:appuser scripts /app/scripts

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
RUN mkdir -p /app/data /app/models /app/outputs /app/logs && \
    chown -R appuser:appuser /app

# ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ‡ã‚Šæ›¿ãˆ
USER appuser

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
EXPOSE 8000
CMD ["uvicorn", "nf_auto_runner.app.api:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

### 5.4 Dockerç’°å¢ƒèµ·å‹•

#### 5.4.1 åŸºæœ¬æ§‹æˆã§èµ·å‹•

```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
cat .env

# ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
docker compose build

# ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•ï¼ˆãƒ‡ã‚¿ãƒƒãƒãƒ¢ãƒ¼ãƒ‰ï¼‰
docker compose up -d

# ãƒ­ã‚°ç¢ºèª
docker compose logs -f

# èµ·å‹•ç¢ºèª
docker compose ps
```

---

#### 5.4.2 å…¨ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•

```bash
# å…¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æœ‰åŠ¹åŒ–
docker compose --profile mlflow --profile ray --profile monitoring up -d

# ã¾ãŸã¯
COMPOSE_PROFILES=mlflow,ray,monitoring docker compose up -d

# ç¢ºèª
docker compose ps
```

---

#### 5.4.3 å€‹åˆ¥ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•

```bash
# PostgreSQLã®ã¿
docker compose up -d postgres

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ + PostgreSQL
docker compose up -d app

# MLflowè¿½åŠ 
docker compose --profile mlflow up -d

# Rayè¿½åŠ 
docker compose --profile ray up -d
```

---

### 5.5 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

#### 5.5.1 ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª

```bash
# å…¨ã‚³ãƒ³ãƒ†ãƒŠã®çŠ¶æ…‹
docker compose ps

# ç‰¹å®šã‚³ãƒ³ãƒ†ãƒŠã®è©³ç´°
docker inspect ts-app

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯çŠ¶æ…‹
docker compose ps --format "table {{.Name}}\t{{.Status}}\t{{.Health}}"
```

---

#### 5.5.2 æ¥ç¶šç¢ºèª

```bash
# PostgreSQLæ¥ç¶š
docker compose exec postgres psql -U postgres -d ts_forecast_system -c "SELECT 1;"

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³API
curl http://localhost:8000/health

# MLflow UI
curl http://localhost:5000/health

# Ray Dashboard
curl http://localhost:8265/api/cluster_status
```

---

#### 5.5.3 ãƒ­ã‚°ç¢ºèª

```bash
# å…¨ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°
docker compose logs -f

# ç‰¹å®šã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°
docker compose logs -f app
docker compose logs -f postgres

# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ã
docker compose logs -f --timestamps app

# æœ€æ–°100è¡Œ
docker compose logs --tail=100 app
```

---

### 5.6 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### 5.6.1 ã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•

```bash
# å…¨ã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•
docker compose restart

# ç‰¹å®šã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•
docker compose restart app

# å¼·åˆ¶å†ä½œæˆ
docker compose up -d --force-recreate app
```

---

#### 5.6.2 ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

```bash
# ã‚³ãƒ³ãƒ†ãƒŠåœæ­¢
docker compose down

# ã‚³ãƒ³ãƒ†ãƒŠ + ãƒœãƒªãƒ¥ãƒ¼ãƒ å‰Šé™¤
docker compose down -v

# ã‚³ãƒ³ãƒ†ãƒŠ + ã‚¤ãƒ¡ãƒ¼ã‚¸å‰Šé™¤
docker compose down --rmi all

# å®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
docker compose down -v --rmi all --remove-orphans
```

---

## 6. æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆæº–å‚™ä¸­ï¼‰

### 6.1 æœ¬ç•ªç’°å¢ƒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TB
    subgraph "Load Balancer"
        LB[NGINX Load Balancer<br/>HAProxy]
    end
    
    subgraph "Application Tier"
        APP1[App Instance 1<br/>Kubernetes Pod]
        APP2[App Instance 2<br/>Kubernetes Pod]
        APP3[App Instance 3<br/>Kubernetes Pod]
    end
    
    subgraph "Database Tier"
        DB_MASTER[(PostgreSQL Master)]
        DB_REPLICA1[(PostgreSQL Replica 1)]
        DB_REPLICA2[(PostgreSQL Replica 2)]
    end
    
    subgraph "Compute Tier"
        RAY_HEAD[Ray Head Node]
        RAY_WORKER1[Ray Worker 1]
        RAY_WORKER2[Ray Worker N]
    end
    
    subgraph "Storage Tier"
        S3[S3/MinIO<br/>Object Storage]
    end
    
    subgraph "Monitoring Tier"
        PROM[Prometheus]
        GRAF[Grafana]
        ALERT[Alertmanager]
    end
    
    LB --> APP1
    LB --> APP2
    LB --> APP3
    
    APP1 --> DB_MASTER
    APP2 --> DB_MASTER
    APP3 --> DB_MASTER
    
    DB_MASTER --> DB_REPLICA1
    DB_MASTER --> DB_REPLICA2
    
    APP1 --> RAY_HEAD
    APP2 --> RAY_HEAD
    APP3 --> RAY_HEAD
    
    RAY_HEAD --> RAY_WORKER1
    RAY_HEAD --> RAY_WORKER2
    
    APP1 --> S3
    APP2 --> S3
    APP3 --> S3
    
    PROM --> APP1
    PROM --> APP2
    PROM --> APP3
    PROM --> ALERT
    GRAF --> PROM
```

---

### 6.2 Kubernetesãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆè¨ˆç”»ä¸­ï¼‰

#### 6.2.1 Helmãƒãƒ£ãƒ¼ãƒˆæ§‹æˆ

```yaml
# values.yamlï¼ˆæŠœç²‹ï¼‰

replicaCount: 3

image:
  repository: your-registry/ts-forecasting
  tag: "1.0.0"
  pullPolicy: IfNotPresent

resources:
  limits:
    cpu: 4000m
    memory: 8Gi
    nvidia.com/gpu: 1
  requests:
    cpu: 2000m
    memory: 4Gi
    nvidia.com/gpu: 1

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

postgresql:
  enabled: true
  auth:
    username: ts_user
    database: ts_forecast_system
  primary:
    persistence:
      size: 100Gi
  readReplicas:
    replicaCount: 2

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: ts-forecast.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: ts-forecast-tls
      hosts:
        - ts-forecast.example.com
```

---

#### 6.2.2 ãƒ‡ãƒ—ãƒ­ã‚¤ã‚³ãƒãƒ³ãƒ‰

```bash
# Helmãƒªãƒã‚¸ãƒˆãƒªè¿½åŠ 
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# åå‰ç©ºé–“ä½œæˆ
kubectl create namespace ts-forecast

# Helmãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤
helm install ts-forecast ./helm/ts-forecast \
  --namespace ts-forecast \
  --values values.yaml \
  --wait

# ãƒ‡ãƒ—ãƒ­ã‚¤ç¢ºèª
kubectl get pods -n ts-forecast
kubectl get services -n ts-forecast
kubectl get ingress -n ts-forecast

# ãƒ­ã‚°ç¢ºèª
kubectl logs -n ts-forecast -l app=ts-forecast --tail=100 -f
```

---

### 6.3 ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œ

#### 6.3.1 AWS

```bash
# EKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
eksctl create cluster \
  --name ts-forecast-cluster \
  --region us-west-2 \
  --nodegroup-name standard-workers \
  --node-type m5.2xlarge \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 10 \
  --managed

# RDS PostgreSQLä½œæˆ
aws rds create-db-instance \
  --db-instance-identifier ts-forecast-db \
  --db-instance-class db.r5.xlarge \
  --engine postgres \
  --engine-version 14.9 \
  --master-username admin \
  --master-user-password <password> \
  --allocated-storage 100 \
  --storage-type gp3 \
  --multi-az

# S3ãƒã‚±ãƒƒãƒˆä½œæˆ
aws s3 mb s3://ts-forecast-artifacts
aws s3api put-bucket-versioning \
  --bucket ts-forecast-artifacts \
  --versioning-configuration Status=Enabled
```

---

#### 6.3.2 GCP

```bash
# GKEã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
gcloud container clusters create ts-forecast-cluster \
  --zone us-central1-a \
  --num-nodes 3 \
  --machine-type n1-standard-4 \
  --enable-autoscaling \
  --min-nodes 2 \
  --max-nodes 10

# Cloud SQL PostgreSQLä½œæˆ
gcloud sql instances create ts-forecast-db \
  --database-version=POSTGRES_14 \
  --tier=db-n1-standard-4 \
  --region=us-central1 \
  --enable-bin-log \
  --backup

# Cloud Storageä½œæˆ
gsutil mb -l us-central1 gs://ts-forecast-artifacts
gsutil versioning set on gs://ts-forecast-artifacts
```

---

#### 6.3.3 Azure

```bash
# AKSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆ
az aks create \
  --resource-group ts-forecast-rg \
  --name ts-forecast-cluster \
  --node-count 3 \
  --node-vm-size Standard_D4s_v3 \
  --enable-cluster-autoscaler \
  --min-count 2 \
  --max-count 10 \
  --generate-ssh-keys

# Azure Database for PostgreSQLä½œæˆ
az postgres flexible-server create \
  --resource-group ts-forecast-rg \
  --name ts-forecast-db \
  --location eastus \
  --admin-user admin \
  --admin-password <password> \
  --sku-name Standard_D4s_v3 \
  --version 14 \
  --storage-size 128

# Blob Storageä½œæˆ
az storage account create \
  --name tsforecaststorage \
  --resource-group ts-forecast-rg \
  --location eastus \
  --sku Standard_LRS \
  --enable-versioning
```

---

## 7. ç’°å¢ƒå¤‰æ•°è¨­å®š

### 7.1 ç’°å¢ƒå¤‰æ•°ä¸€è¦§

#### 7.1.1 å¿…é ˆç’°å¢ƒå¤‰æ•°

| å¤‰æ•°å | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ | ä¾‹ |
|-------|------|------------|-----|
| `DATABASE_URL` | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šURL | ãªã— | `postgresql://user:pass@localhost:5432/db` |
| `DATA_DIR` | ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `./data` | `/app/data` |
| `MODEL_DIR` | ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `./models` | `/app/models` |
| `OUTPUT_DIR` | å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `./outputs` | `/app/outputs` |
| `LOG_DIR` | ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª | `./logs` | `/app/logs` |

---

#### 7.1.2 ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°

| å¤‰æ•°å | èª¬æ˜ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ | ä¾‹ |
|-------|------|------------|-----|
| `MAX_PARALLEL_RUNS` | æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•° | `4` | `10` |
| `DEFAULT_BACKEND` | è¨ˆç®—ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | `cpu` | `cuda` |
| `ENABLE_GPU` | GPUä½¿ç”¨æœ‰åŠ¹åŒ– | `false` | `true` |
| `GPU_MEMORY_LIMIT` | GPU ãƒ¡ãƒ¢ãƒªåˆ¶é™ | `0.8` | `0.9` |
| `LOG_LEVEL` | ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« | `INFO` | `DEBUG` |
| `LOG_FORMAT` | ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ | `json` | `text` |
| `ENABLE_MLFLOW` | MLflowæœ‰åŠ¹åŒ– | `false` | `true` |
| `MLFLOW_TRACKING_URI` | MLflow URI | ãªã— | `http://localhost:5000` |
| `ENABLE_WANDB` | W&Bæœ‰åŠ¹åŒ– | `false` | `true` |
| `WANDB_API_KEY` | W&B APIã‚­ãƒ¼ | ãªã— | `your_key` |
| `ENABLE_RAY` | Rayæœ‰åŠ¹åŒ– | `false` | `true` |
| `RAY_ADDRESS` | Ray ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ | `auto` | `ray://localhost:10001` |

---

### 7.2 ç’°å¢ƒåˆ¥è¨­å®š

#### 7.2.1 é–‹ç™ºç’°å¢ƒ

```bash
# .env.development

# Database
DATABASE_URL=postgresql://dev_user:dev_pass@localhost:5432/ts_forecast_dev
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# Paths
DATA_DIR=./data
MODEL_DIR=./models
OUTPUT_DIR=./outputs
LOG_DIR=./logs

# Execution
MAX_PARALLEL_RUNS=2
DEFAULT_BACKEND=cpu
ENABLE_GPU=false

# Logging
LOG_LEVEL=DEBUG
LOG_FORMAT=text
ENABLE_FILE_LOGGING=true

# Optional Services
ENABLE_MLFLOW=false
ENABLE_WANDB=false
ENABLE_RAY=false
```

---

#### 7.2.2 ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ

```bash
# .env.staging

# Database
DATABASE_URL=postgresql://staging_user:staging_pass@staging-db:5432/ts_forecast_staging
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20

# Paths
DATA_DIR=/data
MODEL_DIR=/models
OUTPUT_DIR=/outputs
LOG_DIR=/logs

# Execution
MAX_PARALLEL_RUNS=8
DEFAULT_BACKEND=cuda
ENABLE_GPU=true
GPU_MEMORY_LIMIT=0.8

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
ENABLE_FILE_LOGGING=true

# Optional Services
ENABLE_MLFLOW=true
MLFLOW_TRACKING_URI=http://mlflow:5000
ENABLE_WANDB=false
ENABLE_RAY=true
RAY_ADDRESS=ray://ray-head:10001
```

---

#### 7.2.3 æœ¬ç•ªç’°å¢ƒ

```bash
# .env.production

# Database (ç’°å¢ƒå¤‰æ•°ã‚„Secretã‹ã‚‰å–å¾—)
DATABASE_URL=${DB_CONNECTION_STRING}
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=40
DATABASE_POOL_TIMEOUT=30

# Paths
DATA_DIR=/mnt/data
MODEL_DIR=/mnt/models
OUTPUT_DIR=/mnt/outputs
LOG_DIR=/mnt/logs

# Execution
MAX_PARALLEL_RUNS=10
DEFAULT_BACKEND=cuda
ENABLE_GPU=true
GPU_MEMORY_LIMIT=0.9

# Logging
LOG_LEVEL=WARNING
LOG_FORMAT=json
ENABLE_FILE_LOGGING=true

# Optional Services
ENABLE_MLFLOW=true
MLFLOW_TRACKING_URI=${MLFLOW_URI}
MLFLOW_S3_ENDPOINT_URL=${S3_ENDPOINT}
AWS_ACCESS_KEY_ID=${AWS_KEY_ID}
AWS_SECRET_ACCESS_KEY=${AWS_SECRET_KEY}

ENABLE_WANDB=true
WANDB_API_KEY=${WANDB_KEY}
WANDB_PROJECT=ts-forecast-prod

ENABLE_RAY=true
RAY_ADDRESS=ray://${RAY_HEAD_HOST}:10001
RAY_NAMESPACE=production
```

---

### 7.3 ç’°å¢ƒå¤‰æ•°ç®¡ç†ãƒ„ãƒ¼ãƒ«

#### 7.3.1 direnv

```bash
# direnvã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install direnv  # macOS
# ã¾ãŸã¯
sudo apt install direnv  # Ubuntu

# ã‚·ã‚§ãƒ«è¨­å®šè¿½åŠ 
echo 'eval "$(direnv hook bash)"' >> ~/.bashrc
source ~/.bashrc

# .envrcãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
cat > .envrc << 'EOF'
# é–‹ç™ºç’°å¢ƒç”¨
export DATABASE_URL=postgresql://dev_user:dev_pass@localhost:5432/ts_forecast_dev
export DATA_DIR=./data
export LOG_LEVEL=DEBUG
EOF

# è¨±å¯
direnv allow .
```

---

#### 7.3.2 AWS Systems Manager Parameter Store

```bash
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç™»éŒ²
aws ssm put-parameter \
  --name /ts-forecast/prod/database-url \
  --value "postgresql://user:pass@host:5432/db" \
  --type SecureString \
  --tier Advanced

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
aws ssm get-parameter \
  --name /ts-forecast/prod/database-url \
  --with-decryption \
  --query 'Parameter.Value' \
  --output text
```

---

#### 7.3.3 Kubernetes Secrets

```bash
# Secretä½œæˆ
kubectl create secret generic ts-forecast-secrets \
  --from-literal=database-url='postgresql://user:pass@host:5432/db' \
  --from-literal=mlflow-uri='http://mlflow:5000' \
  --from-literal=wandb-api-key='your_key' \
  --namespace ts-forecast

# Secretç¢ºèª
kubectl get secrets -n ts-forecast
kubectl describe secret ts-forecast-secrets -n ts-forecast

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®ä½¿ç”¨ï¼ˆdeployment.yamlï¼‰
# envFrom:
#   - secretRef:
#       name: ts-forecast-secrets
```

---

## 8. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š

### 8.1 PostgreSQLè¨­å®šæœ€é©åŒ–

#### 8.1.1 postgresql.conf

```ini
# ========================================
# Connection Settings
# ========================================
max_connections = 200
superuser_reserved_connections = 3

# ========================================
# Memory Settings
# ========================================
shared_buffers = 4GB
effective_cache_size = 12GB
maintenance_work_mem = 1GB
work_mem = 64MB

# ========================================
# WAL Settings
# ========================================
wal_buffers = 16MB
min_wal_size = 2GB
max_wal_size = 8GB
checkpoint_completion_target = 0.9

# ========================================
# Query Tuning
# ========================================
random_page_cost = 1.1
effective_io_concurrency = 200
default_statistics_target = 100

# ========================================
# Logging
# ========================================
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB
log_min_duration_statement = 1000
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '

# ========================================
# Autovacuum
# ========================================
autovacuum = on
autovacuum_max_workers = 3
autovacuum_naptime = 10s
```

---

#### 8.1.2 pg_hba.conf

```ini
# TYPE  DATABASE        USER            ADDRESS                 METHOD

# Local connections
local   all             all                                     peer

# IPv4 local connections
host    all             all             127.0.0.1/32            scram-sha-256
host    all             all             172.25.0.0/16           scram-sha-256

# IPv6 local connections
host    all             all             ::1/128                 scram-sha-256

# Allow replication connections
host    replication     replicator      172.25.0.0/16           scram-sha-256
```

---

### 8.2 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ

#### 8.2.1 åˆæœŸåŒ–SQLã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```sql
-- scripts/db/init.sql

-- ========================================
-- Extensions
-- ========================================
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- ========================================
-- Schemas
-- ========================================
CREATE SCHEMA IF NOT EXISTS ts_forecast;
SET search_path TO ts_forecast, public;

-- ========================================
-- Tables
-- ========================================

-- Experiments
CREATE TABLE IF NOT EXISTS experiments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL UNIQUE,
    description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Runs
CREATE TABLE IF NOT EXISTS runs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    experiment_id UUID NOT NULL REFERENCES experiments(id) ON DELETE CASCADE,
    run_name VARCHAR(255) NOT NULL,
    status VARCHAR(50) NOT NULL CHECK (status IN ('running', 'completed', 'failed', 'cancelled')),
    start_time TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    end_time TIMESTAMP WITH TIME ZONE,
    parameters JSONB,
    metrics JSONB,
    artifacts JSONB,
    error_message TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(experiment_id, run_name)
);

-- Models
CREATE TABLE IF NOT EXISTS models (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    run_id UUID NOT NULL REFERENCES runs(id) ON DELETE CASCADE,
    model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_path TEXT NOT NULL,
    model_size_bytes BIGINT,
    hyperparameters JSONB,
    training_metrics JSONB,
    validation_metrics JSONB,
    test_metrics JSONB,
    is_production BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Predictions
CREATE TABLE IF NOT EXISTS predictions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    model_id UUID NOT NULL REFERENCES models(id) ON DELETE CASCADE,
    prediction_time TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    input_data JSONB NOT NULL,
    predictions JSONB NOT NULL,
    prediction_metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- ========================================
-- Indexes
-- ========================================

-- Experiments
CREATE INDEX idx_experiments_name ON experiments(name);
CREATE INDEX idx_experiments_created_at ON experiments(created_at);

-- Runs
CREATE INDEX idx_runs_experiment_id ON runs(experiment_id);
CREATE INDEX idx_runs_status ON runs(status);
CREATE INDEX idx_runs_start_time ON runs(start_time);
CREATE INDEX idx_runs_parameters ON runs USING GIN (parameters);
CREATE INDEX idx_runs_metrics ON runs USING GIN (metrics);

-- Models
CREATE INDEX idx_models_run_id ON models(run_id);
CREATE INDEX idx_models_name_version ON models(model_name, model_version);
CREATE INDEX idx_models_is_production ON models(is_production) WHERE is_production = TRUE;
CREATE INDEX idx_models_created_at ON models(created_at);

-- Predictions
CREATE INDEX idx_predictions_model_id ON predictions(model_id);
CREATE INDEX idx_predictions_time ON predictions(prediction_time);
CREATE INDEX idx_predictions_input ON predictions USING GIN (input_data);

-- ========================================
-- Functions
-- ========================================

-- Updated_atè‡ªå‹•æ›´æ–°ãƒˆãƒªã‚¬ãƒ¼é–¢æ•°
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

-- Triggers
CREATE TRIGGER update_experiments_updated_at
    BEFORE UPDATE ON experiments
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_runs_updated_at
    BEFORE UPDATE ON runs
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_models_updated_at
    BEFORE UPDATE ON models
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- ========================================
-- Views
-- ========================================

-- æœ€æ–°ã®å®Ÿé¨“æ¦‚è¦
CREATE OR REPLACE VIEW v_latest_experiments AS
SELECT
    e.id,
    e.name,
    e.description,
    COUNT(r.id) as total_runs,
    SUM(CASE WHEN r.status = 'completed' THEN 1 ELSE 0 END) as completed_runs,
    SUM(CASE WHEN r.status = 'failed' THEN 1 ELSE 0 END) as failed_runs,
    SUM(CASE WHEN r.status = 'running' THEN 1 ELSE 0 END) as running_runs,
    MAX(r.start_time) as last_run_time,
    e.created_at,
    e.updated_at
FROM experiments e
LEFT JOIN runs r ON e.id = r.experiment_id
GROUP BY e.id, e.name, e.description, e.created_at, e.updated_at
ORDER BY e.updated_at DESC;

-- æœ¬ç•ªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
CREATE OR REPLACE VIEW v_production_models AS
SELECT
    m.id,
    m.model_name,
    m.model_version,
    m.model_path,
    m.training_metrics,
    m.validation_metrics,
    m.test_metrics,
    r.run_name,
    e.name as experiment_name,
    m.created_at,
    m.updated_at
FROM models m
JOIN runs r ON m.run_id = r.id
JOIN experiments e ON r.experiment_id = e.id
WHERE m.is_production = TRUE
ORDER BY m.created_at DESC;

-- ========================================
-- Grants
-- ========================================

-- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æ¨©é™ä»˜ä¸
GRANT USAGE ON SCHEMA ts_forecast TO ts_user;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA ts_forecast TO ts_user;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA ts_forecast TO ts_user;
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA ts_forecast TO ts_user;

-- ========================================
-- Comments
-- ========================================

COMMENT ON SCHEMA ts_forecast IS 'Time Series Forecasting System schema';
COMMENT ON TABLE experiments IS 'å®Ÿé¨“ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON TABLE runs IS 'å®Ÿè¡Œç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON TABLE models IS 'ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON TABLE predictions IS 'äºˆæ¸¬çµæœãƒ†ãƒ¼ãƒ–ãƒ«';
```

---

### 8.3 ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç®¡ç†

#### 8.3.1 Alembicè¨­å®š

```python
# alembic/env.py

from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Alembic Config object
config = context.config

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Import your models' MetaData object
from nf_auto_runner.db import Base
target_metadata = Base.metadata

# Override sqlalchemy.url with environment variable
config.set_main_option('sqlalchemy.url', os.getenv('DATABASE_URL'))


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

---

#### 8.3.2 ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

```bash
# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ç¢ºèª
alembic history --verbose

# ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
alembic current

# æ–°ã—ã„ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
alembic revision --autogenerate -m "Add new table for feature X"

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
alembic upgrade head

# ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
alembic upgrade abc123

# ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆ1ã¤å‰ï¼‰
alembic downgrade -1

# ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
alembic downgrade abc123

# å®Œå…¨ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
alembic downgrade base
```

---

### 8.4 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

#### 8.4.1 è«–ç†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```bash
# pg_dumpã§ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pg_dump -h localhost -U postgres -d ts_forecast_system \
  -F c -b -v -f backup_$(date +%Y%m%d_%H%M%S).dump

# ã‚¹ã‚­ãƒ¼ãƒã®ã¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pg_dump -h localhost -U postgres -d ts_forecast_system \
  -s -F p -f schema_$(date +%Y%m%d).sql

# ç‰¹å®šãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pg_dump -h localhost -U postgres -d ts_forecast_system \
  -t ts_forecast.models -t ts_forecast.predictions \
  -F c -f models_backup_$(date +%Y%m%d).dump
```

---

#### 8.4.2 ç‰©ç†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆpg_basebackupï¼‰

```bash
# ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pg_basebackup -h localhost -U postgres \
  -D /backup/base_$(date +%Y%m%d) \
  -Ft -z -P

# ç¶™ç¶šçš„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ“ãƒ³ã‚°è¨­å®š
# postgresql.conf ã«è¿½åŠ :
# wal_level = replica
# archive_mode = on
# archive_command = 'cp %p /backup/wal/%f'
```

---

#### 8.4.3 è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# scripts/backup/backup_db.sh

set -e

# è¨­å®š
BACKUP_DIR="/backup/postgres"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/ts_forecast_${TIMESTAMP}.dump"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p "${BACKUP_DIR}"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
echo "Starting backup at $(date)"
pg_dump -h localhost -U postgres -d ts_forecast_system \
  -F c -b -v -f "${BACKUP_FILE}"

# åœ§ç¸®
gzip "${BACKUP_FILE}"
echo "Backup completed: ${BACKUP_FILE}.gz"

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
find "${BACKUP_DIR}" -name "ts_forecast_*.dump.gz" \
  -mtime +${RETENTION_DAYS} -delete
echo "Old backups cleaned up"

# S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
if [ -n "${AWS_S3_BUCKET}" ]; then
    aws s3 cp "${BACKUP_FILE}.gz" \
      "s3://${AWS_S3_BUCKET}/backups/postgres/"
    echo "Backup uploaded to S3"
fi

echo "Backup process finished at $(date)"
```

---

#### 8.4.4 cronã‚¸ãƒ§ãƒ–è¨­å®š

```bash
# crontab -e

# æ¯æ—¥åˆå‰2æ™‚ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
0 2 * * * /path/to/scripts/backup/backup_db.sh >> /var/log/backup.log 2>&1

# æ¯é€±æ—¥æ›œæ—¥åˆå‰3æ™‚ã«VACUUM
0 3 * * 0 psql -h localhost -U postgres -d ts_forecast_system -c "VACUUM ANALYZE;" >> /var/log/vacuum.log 2>&1
```

---

## 9. ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š

### 9.1 MLflowã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 9.1.1 MLflowã‚µãƒ¼ãƒãƒ¼èµ·å‹•

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«èµ·å‹•ï¼ˆSQLiteãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼‰
mlflow server \
  --backend-store-uri sqlite:///mlflow.db \
  --default-artifact-root ./mlruns \
  --host 0.0.0.0 \
  --port 5000

# PostgreSQLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
mlflow server \
  --backend-store-uri postgresql://user:pass@localhost:5432/mlflow \
  --default-artifact-root s3://mlflow-artifacts \
  --host 0.0.0.0 \
  --port 5000
```

---

#### 9.1.2 MLflowè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# conf/mlflow.yml

tracking:
  uri: ${MLFLOW_TRACKING_URI}
  experiment_name: ${MLFLOW_EXPERIMENT_NAME}
  run_name_prefix: "run"
  tags:
    project: "time-series-forecasting"
    version: "1.0.0"

artifacts:
  location: ${MLFLOW_ARTIFACT_ROOT}
  storage_type: "s3"  # s3, azure, gcs, local
  
logging:
  log_models: true
  log_params: true
  log_metrics: true
  log_artifacts: true
  log_system_metrics: true
  
autolog:
  disable: false
  log_input_examples: true
  log_model_signatures: true
  exclusive: false
```

---

### 9.2 Rayã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 9.2.1 Rayã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼èµ·å‹•

```bash
# Rayãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰èµ·å‹•
ray start --head \
  --dashboard-host=0.0.0.0 \
  --dashboard-port=8265 \
  --port=6379 \
  --num-cpus=8 \
  --num-gpus=1

# Rayãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰èµ·å‹•
ray start \
  --address='ray-head-ip:6379' \
  --num-cpus=8 \
  --num-gpus=1

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çŠ¶æ…‹ç¢ºèª
ray status
```

---

#### 9.2.2 Rayè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# conf/ray.yml

cluster:
  address: ${RAY_ADDRESS}
  namespace: ${RAY_NAMESPACE}
  runtime_env:
    pip:
      - neuralforecast
      - torch
      - pandas
    env_vars:
      OMP_NUM_THREADS: "1"

resources:
  num_cpus: 1
  num_gpus: 0
  memory: 4294967296  # 4GB
  
scheduling:
  placement_strategy: "SPREAD"
  max_pending_placement_groups: 100
  
autoscaling:
  enabled: true
  min_workers: 2
  max_workers: 10
  idle_timeout_minutes: 5
```

---

### 9.3 Weights & Biases (W&B)ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### 9.3.1 W&Bãƒ­ã‚°ã‚¤ãƒ³

```bash
# W&B APIã‚­ãƒ¼è¨­å®š
export WANDB_API_KEY=your_api_key

# ã¾ãŸã¯ãƒ­ã‚°ã‚¤ãƒ³ã‚³ãƒãƒ³ãƒ‰
wandb login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
wandb init \
  --project time-series-forecasting \
  --entity your-team \
  --name experiment-1
```

---

#### 9.3.2 W&Bè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# conf/wandb.yml

project: ${WANDB_PROJECT}
entity: ${WANDB_ENTITY}
name: ${WANDB_RUN_NAME}

config:
  log_frequency: 1
  save_code: true
  log_model: true
  log_dataset: false

tags:
  - time-series
  - forecasting
  - production

notes: "Time series forecasting experiment"

mode: "online"  # online, offline, disabled
```

---

## 10. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### 10.1 ç§˜å¯†æƒ…å ±ç®¡ç†

#### 10.1.1 ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹ç®¡ç†

```bash
# .envï¼ˆçµ¶å¯¾ã«Gitã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„ï¼‰
DATABASE_PASSWORD=strong_password_here
MLFLOW_TRACKING_TOKEN=secret_token
WANDB_API_KEY=secret_api_key
AWS_SECRET_ACCESS_KEY=aws_secret
```

**.gitignore**:

```
# ç’°å¢ƒå¤‰æ•°
.env
.env.*
!.env.example

# ç§˜å¯†éµ
*.pem
*.key
secrets/
```

---

#### 10.1.2 AWS Secrets Managerã®ä½¿ç”¨

```python
# scripts/utils/get_secret.py

import boto3
from botocore.exceptions import ClientError
import json


def get_secret(secret_name: str, region_name: str = "us-west-2") -> dict:
    """
    AWS Secrets Managerã‹ã‚‰ç§˜å¯†æƒ…å ±ã‚’å–å¾—
    """
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )

    try:
        response = client.get_secret_value(SecretId=secret_name)
    except ClientError as e:
        raise e

    if 'SecretString' in response:
        secret = response['SecretString']
        return json.loads(secret)
    else:
        raise ValueError("Secret not found")


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    secrets = get_secret("ts-forecast/prod/db-credentials")
    DATABASE_URL = f"postgresql://{secrets['username']}:{secrets['password']}@" \
                   f"{secrets['host']}:{secrets['port']}/{secrets['database']}"
    print(DATABASE_URL)
```

---

### 10.2 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

#### 10.2.1 ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š

```bash
# UFWï¼ˆUbuntuï¼‰
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow ssh
sudo ufw allow 8000/tcp  # API
sudo ufw allow 5432/tcp from 172.25.0.0/16  # PostgreSQL (Docker network only)
sudo ufw enable

# iptables
sudo iptables -A INPUT -p tcp --dport 8000 -j ACCEPT
sudo iptables -A INPUT -p tcp --dport 5432 -s 172.25.0.0/16 -j ACCEPT
sudo iptables -A INPUT -j DROP
```

---

#### 10.2.2 SSL/TLSè¨¼æ˜æ›¸è¨­å®š

```bash
# Let's Encryptè¨¼æ˜æ›¸å–å¾—
sudo apt install certbot
sudo certbot certonly --standalone -d ts-forecast.example.com

# è¨¼æ˜æ›¸ã®å ´æ‰€
# /etc/letsencrypt/live/ts-forecast.example.com/fullchain.pem
# /etc/letsencrypt/live/ts-forecast.example.com/privkey.pem

# è‡ªå‹•æ›´æ–°è¨­å®š
sudo crontab -e
# 0 3 * * * certbot renew --quiet --post-hook "systemctl reload nginx"
```

**NGINX SSLè¨­å®š**:

```nginx
server {
    listen 443 ssl http2;
    server_name ts-forecast.example.com;

    ssl_certificate /etc/letsencrypt/live/ts-forecast.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/ts-forecast.example.com/privkey.pem;
    
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

---

### 10.3 èªè¨¼ãƒ»èªå¯

#### 10.3.1 JWTèªè¨¼è¨­å®š

```python
# src/nf_auto_runner/app/auth.py

from datetime import datetime, timedelta
from typing import Optional
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import os

# è¨­å®š
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key-here")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æ¤œè¨¼"""
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒã‚·ãƒ¥åŒ–"""
    return pwd_context.hash(password)


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """JWTãƒˆãƒ¼ã‚¯ãƒ³ä½œæˆ"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt


async def get_current_user(token: str = Depends(oauth2_scheme)):
    """ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—"""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ï¼ˆDB fromï¼‰
    # ...
    
    return {"username": username}
```

---

## 11. ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°è¨­å®š

### 11.1 Prometheusè¨­å®š

#### 11.1.1 prometheus.yml

```yaml
# monitoring/prometheus/prometheus.yml

global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'ts-forecast'
    environment: 'production'

scrape_configs:
  # Application
  - job_name: 'ts-forecast-app'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    
  # PostgreSQL
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  # Ray
  - job_name: 'ray'
    static_configs:
      - targets: ['ray-head:8265']
    metrics_path: '/metrics'
    
  # Node Exporter
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

rule_files:
  - '/etc/prometheus/rules/*.yml'
```

---

#### 11.1.2 ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«

```yaml
# monitoring/prometheus/rules/alerts.yml

groups:
  - name: application_alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} req/s"
      
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1e9 > 8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}GB"
      
      - alert: DatabaseConnectionPoolExhausted
        expr: db_connection_pool_active / db_connection_pool_size > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool almost exhausted"
```

---

### 11.2 Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### 11.2.1 ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š

```yaml
# monitoring/grafana/datasources/prometheus.yml

apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      timeInterval: "15s"
      queryTimeout: "60s"
```

---

#### 11.2.2 ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®šç¾©

```json
{
  "dashboard": {
    "title": "Time Series Forecasting - System Overview",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{path}}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "5xx errors"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Active Runs",
        "targets": [
          {
            "expr": "ts_forecast_active_runs",
            "legendFormat": "Active"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Memory Usage",
        "targets": [
          {
            "expr": "process_resident_memory_bytes / 1e9",
            "legendFormat": "RSS (GB)"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```

---

### 11.3 ãƒ­ã‚°ç®¡ç†

#### 11.3.1 ãƒ­ã‚°è¨­å®š

```python
# src/nf_auto_runner/utils/logging_config.py

import logging
import logging.config
from pythonjsonlogger import jsonlogger
import os


def setup_logging():
    """ãƒ­ã‚°è¨­å®šã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    
    log_level = os.getenv("LOG_LEVEL", "INFO")
    log_format = os.getenv("LOG_FORMAT", "json")
    log_dir = os.getenv("LOG_DIR", "./logs")
    
    os.makedirs(f"{log_dir}/app", exist_ok=True)
    
    config = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "json": {
                "()": jsonlogger.JsonFormatter,
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s"
            },
            "text": {
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            }
        },
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
                "formatter": log_format,
                "stream": "ext://sys.stdout"
            },
            "file": {
                "class": "logging.handlers.RotatingFileHandler",
                "formatter": log_format,
                "filename": f"{log_dir}/app/app.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 10
            },
            "error_file": {
                "class": "logging.handlers.RotatingFileHandler",
                "formatter": log_format,
                "filename": f"{log_dir}/app/error.log",
                "maxBytes": 10485760,
                "backupCount": 10,
                "level": "ERROR"
            }
        },
        "root": {
            "level": log_level,
            "handlers": ["console", "file", "error_file"]
        },
        "loggers": {
            "uvicorn": {"level": "INFO"},
            "sqlalchemy.engine": {"level": "WARNING"},
            "alembic": {"level": "INFO"}
        }
    }
    
    logging.config.dictConfig(config)
    logger = logging.getLogger(__name__)
    logger.info("Logging configured", extra={"log_level": log_level, "log_format": log_format})
```

---

#### 11.3.2 ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

```bash
# /etc/logrotate.d/ts-forecast

/app/logs/**/*.log {
    daily
    rotate 30
    compress
    delaycompress
    notifempty
    create 0640 appuser appuser
    sharedscripts
    postrotate
        # ã‚·ã‚°ãƒŠãƒ«é€ä¿¡ã§ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å†ã‚ªãƒ¼ãƒ—ãƒ³
        /usr/bin/killall -SIGUSR1 python3 || true
    endscript
}
```

---

## 12. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§æ‰‹é †

### 12.1 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

#### 12.1.1 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| å¯¾è±¡ | é »åº¦ | ä¿æŒæœŸé–“ | æ–¹æ³• |
|-----|------|---------|------|
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | æ¯æ—¥ | 30æ—¥ | pg_dump + S3 |
| **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«** | æ¯é€± | 90æ—¥ | rsync + S3 |
| **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«** | å¤‰æ›´æ™‚ | æ°¸ç¶š | Git |
| **ãƒ­ã‚°** | æ¯é€± | 90æ—¥ | tar.gz + S3 |

---

#### 12.1.2 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# scripts/backup/full_backup.sh

set -euo pipefail

# è¨­å®š
BACKUP_ROOT="/backup"
S3_BUCKET="s3://ts-forecast-backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# ãƒ­ã‚°è¨­å®š
LOG_FILE="/var/log/backup_${TIMESTAMP}.log"
exec 1> >(tee -a "${LOG_FILE}")
exec 2>&1

echo "========================================="
echo "Starting backup at $(date)"
echo "========================================="

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Backing up database..."
DB_BACKUP="${BACKUP_ROOT}/db/ts_forecast_${TIMESTAMP}.dump"
mkdir -p "${BACKUP_ROOT}/db"
pg_dump -h localhost -U postgres -d ts_forecast_system \
    -F c -b -v -f "${DB_BACKUP}"
gzip "${DB_BACKUP}"
echo "Database backup completed: ${DB_BACKUP}.gz"

# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Backing up models..."
MODEL_BACKUP="${BACKUP_ROOT}/models/models_${TIMESTAMP}.tar.gz"
mkdir -p "${BACKUP_ROOT}/models"
tar -czf "${MODEL_BACKUP}" -C /app models/
echo "Models backup completed: ${MODEL_BACKUP}"

# ãƒ­ã‚°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Backing up logs..."
LOG_BACKUP="${BACKUP_ROOT}/logs/logs_${TIMESTAMP}.tar.gz"
mkdir -p "${BACKUP_ROOT}/logs"
tar -czf "${LOG_BACKUP}" -C /app logs/
echo "Logs backup completed: ${LOG_BACKUP}"

# S3ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
if [ -n "${S3_BUCKET}" ]; then
    echo "Uploading to S3..."
    aws s3 sync "${BACKUP_ROOT}" "${S3_BUCKET}/${TIMESTAMP}/" \
        --storage-class STANDARD_IA
    echo "S3 upload completed"
fi

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šå‰ï¼‰
echo "Cleaning up old backups..."
find "${BACKUP_ROOT}" -type f -mtime +30 -delete
echo "Cleanup completed"

echo "========================================="
echo "Backup completed successfully at $(date)"
echo "========================================="
```

---

### 12.2 å¾©æ—§æ‰‹é †

#### 12.2.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§

```bash
#!/bin/bash
# scripts/restore/restore_db.sh

set -euo pipefail

# å¼•æ•°ãƒã‚§ãƒƒã‚¯
if [ $# -ne 1 ]; then
    echo "Usage: $0 <backup_file.dump.gz>"
    exit 1
fi

BACKUP_FILE="$1"

echo "Starting database restore from ${BACKUP_FILE}"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
if [ ! -f "${BACKUP_FILE}" ]; then
    echo "Error: Backup file not found"
    exit 1
fi

# è§£å‡
TEMP_DIR=$(mktemp -d)
gunzip -c "${BACKUP_FILE}" > "${TEMP_DIR}/backup.dump"

# ç¾åœ¨ã®DBã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
echo "Creating safety backup of current database..."
pg_dump -h localhost -U postgres -d ts_forecast_system \
    -F c -b -f "${TEMP_DIR}/safety_backup.dump"

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‰Šé™¤ãƒ»å†ä½œæˆ
echo "Dropping and recreating database..."
psql -h localhost -U postgres << EOF
DROP DATABASE IF EXISTS ts_forecast_system;
CREATE DATABASE ts_forecast_system;
GRANT ALL PRIVILEGES ON DATABASE ts_forecast_system TO ts_user;
EOF

# ãƒªã‚¹ãƒˆã‚¢
echo "Restoring database..."
pg_restore -h localhost -U postgres -d ts_forecast_system \
    -v "${TEMP_DIR}/backup.dump"

# ç¢ºèª
echo "Verifying restore..."
psql -h localhost -U postgres -d ts_forecast_system -c "
    SELECT COUNT(*) FROM experiments;
    SELECT COUNT(*) FROM runs;
    SELECT COUNT(*) FROM models;
"

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
rm -rf "${TEMP_DIR}"

echo "Database restore completed successfully"
```

---

#### 12.2.2 ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å¾©æ—§

```bash
#!/bin/bash
# scripts/restore/restore_models.sh

set -euo pipefail

if [ $# -ne 1 ]; then
    echo "Usage: $0 <models_backup.tar.gz>"
    exit 1
fi

BACKUP_FILE="$1"
RESTORE_DIR="/app/models"

echo "Starting models restore from ${BACKUP_FILE}"

# ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
if [ -d "${RESTORE_DIR}" ]; then
    echo "Backing up current models..."
    mv "${RESTORE_DIR}" "${RESTORE_DIR}.old_$(date +%Y%m%d_%H%M%S)"
fi

# ãƒªã‚¹ãƒˆã‚¢
echo "Restoring models..."
mkdir -p "${RESTORE_DIR}"
tar -xzf "${BACKUP_FILE}" -C "${RESTORE_DIR}" --strip-components=1

# æ¨©é™è¨­å®š
chown -R appuser:appuser "${RESTORE_DIR}"
chmod -R 755 "${RESTORE_DIR}"

# ç¢ºèª
echo "Verifying restore..."
ls -lh "${RESTORE_DIR}"

echo "Models restore completed successfully"
```

---

#### 12.2.3 ç½å®³å¾©æ—§ï¼ˆDRï¼‰æ‰‹é †

**å®Œå…¨ãªå¾©æ—§æ‰‹é †**:

```bash
#!/bin/bash
# scripts/restore/disaster_recovery.sh

set -euo pipefail

echo "========================================="
echo "Starting Disaster Recovery Process"
echo "========================================="

# S3ã‹ã‚‰æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
echo "Downloading latest backups from S3..."
BACKUP_DIR="/tmp/dr_restore"
mkdir -p "${BACKUP_DIR}"

aws s3 sync "s3://ts-forecast-backups/" "${BACKUP_DIR}/" \
    --exclude "*" \
    --include "*/ts_forecast_*.dump.gz" \
    --include "*/models_*.tar.gz"

# æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
LATEST_DB=$(find "${BACKUP_DIR}" -name "ts_forecast_*.dump.gz" | sort -r | head -n1)
LATEST_MODELS=$(find "${BACKUP_DIR}" -name "models_*.tar.gz" | sort -r | head -n1)

echo "Latest DB backup: ${LATEST_DB}"
echo "Latest Models backup: ${LATEST_MODELS}"

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§
echo "Restoring database..."
./scripts/restore/restore_db.sh "${LATEST_DB}"

# ãƒ¢ãƒ‡ãƒ«å¾©æ—§
echo "Restoring models..."
./scripts/restore/restore_models.sh "${LATEST_MODELS}"

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
echo "Restarting services..."
docker compose restart app

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "Performing health checks..."
sleep 10
curl -f http://localhost:8000/health || exit 1

echo "========================================="
echo "Disaster Recovery Completed Successfully"
echo "========================================="
```

---

## 13. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 13.1 ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

#### 13.1.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼

**å•é¡Œ**:
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server
```

**è§£æ±ºç­–**:

```bash
# 1. PostgreSQLã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
sudo systemctl status postgresql
docker ps | grep postgres

# 2. æ¥ç¶šãƒ†ã‚¹ãƒˆ
psql -h localhost -U postgres -d ts_forecast_system

# 3. pg_hba.confã‚’ç¢ºèª
sudo cat /etc/postgresql/14/main/pg_hba.conf

# 4. ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ç¢ºèª
sudo ufw status
sudo netstat -tulpn | grep 5432

# 5. ç’°å¢ƒå¤‰æ•°ç¢ºèª
echo $DATABASE_URL
```

---

#### 13.1.2 ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**å•é¡Œ**:
```
RuntimeError: CUDA out of memory
```

**è§£æ±ºç­–**:

```bash
# 1. GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ç¢ºèª
nvidia-smi

# 2. ç’°å¢ƒå¤‰æ•°ã§åˆ¶é™
export GPU_MEMORY_LIMIT=0.7

# 3. ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›
# conf/config.yaml
batch_size: 32  # 64ã‹ã‚‰å‰Šæ¸›

# 4. ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å®Ÿè¡Œæ•°å‰Šæ¸›
export MAX_PARALLEL_RUNS=2

# 5. ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªç¢ºèª
free -h
```

---

#### 13.1.3 ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼

**å•é¡Œ**:
```
ModuleNotFoundError: No module named 'neuralforecast'
```

**è§£æ±ºç­–**:

```bash
# 1. ä»®æƒ³ç’°å¢ƒã®ç¢ºèª
which python
python --version

# 2. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade pip
pip install -r requirements.txt --force-reinstall

# 3. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèª
pip list | grep neuralforecast
pip check

# 4. ç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³
pip cache purge
rm -rf ~/.cache/pip

# 5. ä»®æƒ³ç’°å¢ƒå†ä½œæˆ
deactivate
rm -rf .venv
python3.11 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

#### 13.1.4 Docker ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•å¤±æ•—

**å•é¡Œ**:
```
Error response from daemon: driver failed programming external connectivity
```

**è§£æ±ºç­–**:

```bash
# 1. ãƒãƒ¼ãƒˆä½¿ç”¨çŠ¶æ³ç¢ºèª
sudo lsof -i :5432
sudo lsof -i :8000

# 2. æ—¢å­˜ã‚³ãƒ³ãƒ†ãƒŠåœæ­¢
docker compose down

# 3. ãƒãƒ¼ãƒˆè§£æ”¾
sudo kill -9 <PID>

# 4. Dockerãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†ä½œæˆ
docker network prune
docker network create ts-network

# 5. ã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•
docker compose up -d --force-recreate
```

---

### 13.2 ãƒ­ã‚°ç¢ºèªæ–¹æ³•

#### 13.2.1 ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ
tail -f logs/app/app.log

# Dockerç’°å¢ƒ
docker compose logs -f app

# ç‰¹å®šæœŸé–“ã®ãƒ­ã‚°
docker compose logs --since 1h app

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿
grep ERROR logs/app/app.log
docker compose logs app 2>&1 | grep ERROR
```

---

#### 13.2.2 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ã‚°

```bash
# PostgreSQLãƒ­ã‚°ç¢ºèª
sudo tail -f /var/log/postgresql/postgresql-14-main.log

# Dockerç’°å¢ƒ
docker compose logs -f postgres

# ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªç¢ºèª
sudo grep "duration:" /var/log/postgresql/postgresql-14-main.log | \
    grep -E "duration: [0-9]{4,}" | \
    sort -t: -k2 -n
```

---

#### 13.2.3 ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°

```bash
# systemdãƒ­ã‚°
sudo journalctl -u ts-forecast -f

# Dockerãƒ­ã‚°
docker logs ts-app -f --tail=100

# ã‚«ãƒ¼ãƒãƒ«ãƒ­ã‚°
dmesg | tail -100
```

---

### 13.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ºæ–­

#### 13.3.1 ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
# scripts/utils/profile_app.py

import cProfile
import pstats
import io
from pstats import SortKey


def profile_function(func, *args, **kwargs):
    """é–¢æ•°ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    pr = cProfile.Profile()
    pr.enable()
    
    result = func(*args, **kwargs)
    
    pr.disable()
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats(SortKey.CUMULATIVE)
    ps.print_stats()
    
    print(s.getvalue())
    return result


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    from nf_auto_runner.app.main import run_experiment
    profile_function(run_experiment, config_path="conf/config.yaml")
```

---

#### 13.3.2 ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
# scripts/utils/memory_profile.py

from memory_profiler import profile
import tracemalloc


@profile
def memory_intensive_function():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š"""
    # å‡¦ç†
    pass


def trace_memory():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒˆãƒ¬ãƒ¼ã‚¹"""
    tracemalloc.start()
    
    # å‡¦ç†
    memory_intensive_function()
    
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
    
    print("[ Top 10 Memory Consumers ]")
    for stat in top_stats[:10]:
        print(stat)
    
    tracemalloc.stop()


if __name__ == "__main__":
    trace_memory()
```

---

## 14. ä»˜éŒ²

### 14.1 ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### 14.1.1 ãƒ‡ãƒ—ãƒ­ã‚¤å‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
## ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ

- [ ] Python 3.11ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- [ ] ä»®æƒ³ç’°å¢ƒä½œæˆæ¸ˆã¿
- [ ] ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- [ ] PostgreSQLèµ·å‹•ç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆå®Œäº†
- [ ] ç’°å¢ƒå¤‰æ•°è¨­å®šå®Œäº†
- [ ] ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨æ¸ˆã¿
- [ ] ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒæˆåŠŸ
- [ ] ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª

## Dockerç’°å¢ƒ

- [ ] Docker/Docker Composeã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- [ ] .envãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ¸ˆã¿
- [ ] docker-compose.ymlãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†
- [ ] ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰æˆåŠŸ
- [ ] ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•æˆåŠŸ
- [ ] ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯é€šé
- [ ] APIå‹•ä½œç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª

## æœ¬ç•ªç’°å¢ƒ

- [ ] ã‚¤ãƒ³ãƒ•ãƒ©ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°å®Œäº†
- [ ] SSL/TLSè¨¼æ˜æ›¸è¨­å®šæ¸ˆã¿
- [ ] ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šå®Œäº†
- [ ] ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šå®Œäº†
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®šå®Œäº†
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°æ¸ˆã¿
- [ ] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †ç¢ºèªæ¸ˆã¿
- [ ] è² è·ãƒ†ã‚¹ãƒˆå®Ÿæ–½æ¸ˆã¿
```

---

#### 14.1.2 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
## èªè¨¼ãƒ»èªå¯

- [ ] ç§˜å¯†æƒ…å ±ã¯ç’°å¢ƒå¤‰æ•°ã§ç®¡ç†
- [ ] .envãƒ•ã‚¡ã‚¤ãƒ«ã¯.gitignoreè¿½åŠ æ¸ˆã¿
- [ ] JWTç§˜å¯†éµã¯å¼·åŠ›ãªå€¤ã«è¨­å®š
- [ ] APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«èªè¨¼å®Ÿè£…
- [ ] æ¨©é™ãƒã‚§ãƒƒã‚¯å®Ÿè£…

## ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

- [ ] ä¸è¦ãªãƒãƒ¼ãƒˆã¯é–‰ã˜ã‚‹
- [ ] SSL/TLSè¨¼æ˜æ›¸è¨­å®š
- [ ] ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«æœ‰åŠ¹åŒ–
- [ ] VPN/Bastionãƒ›ã‚¹ãƒˆçµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹

## ãƒ‡ãƒ¼ã‚¿

- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã¯å¼·åŠ›
- [ ] PII ãƒ‡ãƒ¼ã‚¿ã®åŒ¿ååŒ–å®Ÿè£…
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯æš—å·åŒ–
- [ ] ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°è¨˜éŒ²

## ã‚³ãƒ¼ãƒ‰

- [ ] ä¾å­˜é–¢ä¿‚ã®è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³å®Ÿæ–½
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒé©ç”¨æ¸ˆã¿
- [ ] Secretsã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãªã—
- [ ] SQL ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–æ¸ˆã¿
```

---

### 14.2 ã‚³ãƒãƒ³ãƒ‰ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

#### 14.2.1 ã‚ˆãä½¿ã†ã‚³ãƒãƒ³ãƒ‰

```bash
# ========================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ========================================

# å­¦ç¿’å®Ÿè¡Œ
python -m nf_auto_runner.app.main \
  --data-path data/sample.csv \
  --models NHITS,PatchTST \
  --horizons 24

# APIèµ·å‹•
uvicorn nf_auto_runner.app.api:app \
  --host 0.0.0.0 \
  --port 8000 \
  --reload

# ========================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
# ========================================

# ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
alembic upgrade head
alembic downgrade -1
alembic revision --autogenerate -m "message"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
pg_dump -h localhost -U postgres -d ts_forecast_system \
  -F c -b -f backup.dump

# ãƒªã‚¹ãƒˆã‚¢
pg_restore -h localhost -U postgres -d ts_forecast_system \
  -v backup.dump

# ========================================
# Docker
# ========================================

# èµ·å‹•
docker compose up -d
docker compose --profile mlflow --profile ray up -d

# åœæ­¢
docker compose down
docker compose down -v  # ãƒœãƒªãƒ¥ãƒ¼ãƒ å‰Šé™¤

# ãƒ­ã‚°
docker compose logs -f app
docker compose logs --tail=100 postgres

# å†èµ·å‹•
docker compose restart app

# ========================================
# ãƒ†ã‚¹ãƒˆ
# ========================================

# å…¨ãƒ†ã‚¹ãƒˆ
pytest tests/ -v

# ã‚«ãƒãƒ¬ãƒƒã‚¸
pytest tests/ --cov=src --cov-report=html

# ç‰¹å®šãƒ†ã‚¹ãƒˆ
pytest tests/unit/test_config.py::test_config_load -v

# ========================================
# ã‚³ãƒ¼ãƒ‰å“è³ª
# ========================================

# Lint
pylint src/nf_auto_runner/
flake8 src/ tests/
mypy src/nf_auto_runner/ --strict

# Format
black src/ tests/
isort src/ tests/

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³
bandit -r src/nf_auto_runner/
```

---

### 14.3 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
graph TD
    A[å•é¡Œç™ºç”Ÿ] --> B{ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã¯?}
    
    B -->|æ¥ç¶šã‚¨ãƒ©ãƒ¼| C[ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èµ·å‹•ç¢ºèª]
    C --> D[ç’°å¢ƒå¤‰æ•°ç¢ºèª]
    D --> E[ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ç¢ºèª]
    
    B -->|ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼| F[GPU/CPUãƒ¡ãƒ¢ãƒªç¢ºèª]
    F --> G[ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´]
    G --> H[ä¸¦åˆ—æ•°å‰Šæ¸›]
    
    B -->|ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼| I[ä»®æƒ³ç’°å¢ƒç¢ºèª]
    I --> J[ä¾å­˜é–¢ä¿‚å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«]
    J --> K[Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª]
    
    B -->|Dockerã‚¨ãƒ©ãƒ¼| L[ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ…‹ç¢ºèª]
    L --> M[ãƒ­ã‚°ç¢ºèª]
    M --> N[ãƒãƒ¼ãƒˆç«¶åˆç¢ºèª]
    
    E --> O{è§£æ±ºã—ãŸ?}
    H --> O
    K --> O
    N --> O
    
    O -->|Yes| P[å®Œäº†]
    O -->|No| Q[ã‚µãƒãƒ¼ãƒˆã«é€£çµ¡]
```

---

### 14.4 å‚è€ƒè³‡æ–™

#### 14.4.1 å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **NeuralForecast**: https://nixtla.github.io/neuralforecast/
- **FastAPI**: https://fastapi.tiangolo.com/
- **PostgreSQL**: https://www.postgresql.org/docs/
- **Docker**: https://docs.docker.com/
- **MLflow**: https://mlflow.org/docs/latest/index.html
- **Ray**: https://docs.ray.io/
- **Prometheus**: https://prometheus.io/docs/
- **Grafana**: https://grafana.com/docs/

---

#### 14.4.2 é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

| ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ | èª¬æ˜ |
|------------|------|
| `00_INTEGRATED_DESIGN_OVERVIEW.md` | çµ±åˆè¨­è¨ˆæ¦‚è¦ |
| `01_REQUIREMENTS_SPECIFICATION_DETAILED.md` | è©³ç´°è¦ä»¶å®šç¾© |
| `02_NON_FUNCTIONAL_REQUIREMENTS.md` | éæ©Ÿèƒ½è¦ä»¶ |
| `03_ARCHITECTURE_DESIGN_DETAILED.md` | ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ |
| `07_IMPLEMENTATION_GUIDE.md` | å®Ÿè£…ã‚¬ã‚¤ãƒ‰ |
| `08_CODING_STANDARDS.md` | ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„ |
| `09_TESTING_STRATEGY.md` | ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ |
| `11_OPERATIONS_RUNBOOK.md` | é‹ç”¨æ‰‹é †æ›¸ |
| `12_MONITORING_GUIDE.md` | ç›£è¦–ã‚¬ã‚¤ãƒ‰ |

---

## âœ¨ ã¾ã¨ã‚

æœ¬ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰ã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å„ç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ã‚’åŒ…æ‹¬çš„ã«æä¾›ã—ã¦ã„ã¾ã™ã€‚

### ãƒ‡ãƒ—ãƒ­ã‚¤ã®ãƒã‚¤ãƒ³ãƒˆ

- âœ… **æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤**: ãƒ­ãƒ¼ã‚«ãƒ« â†’ Docker â†’ æœ¬ç•ª
- âœ… **ç’°å¢ƒåˆ†é›¢**: é–‹ç™ºã€ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ã€æœ¬ç•ªç’°å¢ƒã®æ˜ç¢ºãªåˆ†é›¢
- âœ… **è‡ªå‹•åŒ–**: Docker Composeã€Kubernetes ã«ã‚ˆã‚‹è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
- âœ… **ç›£è¦–**: Prometheus/Grafana ã«ã‚ˆã‚‹åŒ…æ‹¬çš„ãªç›£è¦–
- âœ… **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨å¾©æ—§æ‰‹é †ã®æ•´å‚™
- âœ… **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ç§˜å¯†æƒ…å ±ç®¡ç†ã€èªè¨¼ã€æš—å·åŒ–ã®å®Ÿè£…

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… **æœ¬ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰ã‚’èª­äº†**
2. ğŸ”§ ç’°å¢ƒã«å¿œã˜ãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
   - ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º: ã‚»ã‚¯ã‚·ãƒ§ãƒ³4
   - Dockerç’°å¢ƒ: ã‚»ã‚¯ã‚·ãƒ§ãƒ³5
   - æœ¬ç•ªç’°å¢ƒ: ã‚»ã‚¯ã‚·ãƒ§ãƒ³6ï¼ˆæº–å‚™ä¸­ï¼‰
3. ğŸ” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã®å®Ÿæ–½
4. ğŸ“Š ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
5. ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥ã®å®Ÿè£…
6. ğŸ“ é‹ç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç¢ºèª

---

**Happy Deploying! ğŸš€**

---
**End of Document**
