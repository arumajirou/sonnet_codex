# ç›£è¦–ã‚¬ã‚¤ãƒ‰
**Monitoring Guide for Time Series Forecasting System**

---

## ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±

| é …ç›® | å†…å®¹ |
|-----|------|
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«** | æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  ç›£è¦–ã‚¬ã‚¤ãƒ‰ |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** | v1.0.0 |
| **ä½œæˆæ—¥** | 2025-11-03 |
| **æœ€çµ‚æ›´æ–°æ—¥** | 2025-11-03 |
| **å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ ** | NeuralForecast Auto Runner + Time Series Forecasting System |
| **å¯¾è±¡èª­è€…** | SREã€é‹ç”¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€é–‹ç™ºè€…ã€ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€… |

---

## ç›®æ¬¡

1. [ç›£è¦–ã®æ¦‚è¦](#1-ç›£è¦–ã®æ¦‚è¦)
2. [ç›£è¦–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#2-ç›£è¦–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
3. [ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†](#3-ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†)
4. [ãƒ­ã‚®ãƒ³ã‚°æˆ¦ç•¥](#4-ãƒ­ã‚®ãƒ³ã‚°æˆ¦ç•¥)
5. [ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š](#5-ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š)
6. [ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ](#6-ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ)
7. [ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°](#7-ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°)
8. [ç›£è¦–ãƒ„ãƒ¼ãƒ«è¨­å®š](#8-ç›£è¦–ãƒ„ãƒ¼ãƒ«è¨­å®š)
9. [é‹ç”¨ç›£è¦–æ‰‹é †](#9-é‹ç”¨ç›£è¦–æ‰‹é †)
10. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#10-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
11. [ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](#11-ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)
12. [ä»˜éŒ²](#12-ä»˜éŒ²)

---

## 1. ç›£è¦–ã®æ¦‚è¦

### 1.1 ç›£è¦–ã®ç›®çš„

æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ç›£è¦–ã¯ã€ä»¥ä¸‹ã®3ã¤ã®ä¸»è¦ç›®çš„ã‚’æŒã¡ã¾ã™:

#### 1. **å¯è¦³æ¸¬æ€§ã®ç¢ºä¿** (Observability)
- ã‚·ã‚¹ãƒ†ãƒ ã®å†…éƒ¨çŠ¶æ…‹ã‚’å¤–éƒ¨ã‹ã‚‰ç†è§£å¯èƒ½ã«ã™ã‚‹
- å•é¡Œç™ºç”Ÿæ™‚ã®è¿…é€ŸãªåŸå› ç‰¹å®š
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç™ºè¦‹

#### 2. **ä¿¡é ¼æ€§ã®ç¶­æŒ** (Reliability)
- ã‚·ã‚¹ãƒ†ãƒ ã®ç¨¼åƒç‡99%ä»¥ä¸Šã‚’ç¶­æŒ
- MTTR (Mean Time To Repair) < 1æ™‚é–“
- MTTD (Mean Time To Detect) < 10åˆ†

#### 3. **äºˆé˜²çš„ãªå¯¾å¿œ** (Proactive Operations)
- éšœå®³ã®äºˆå…†æ¤œçŸ¥
- ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã®äº‹å‰è­¦å‘Š
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®æ—©æœŸç™ºè¦‹

---

### 1.2 ç›£è¦–ã®3ã¤ã®æŸ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           å¯è¦³æ¸¬æ€§ã®3ã¤ã®æŸ±                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Metrics  â”‚  â”‚   Logs   â”‚  â”‚  Traces  â”‚     â”‚
â”‚  â”‚ (ãƒ¡ãƒˆãƒªã‚¯ã‚¹)â”‚  â”‚ (ãƒ­ã‚°)    â”‚  â”‚(ãƒˆãƒ¬ãƒ¼ã‚¹) â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚              â”‚              â”‚          â”‚
â”‚       â”‚              â”‚              â”‚          â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                     â”‚                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚ çµ±åˆã•ã‚ŒãŸ    â”‚                   â”‚
â”‚              â”‚ å¯è¦³æ¸¬æ€§     â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1.3 ç›£è¦–ã®éšå±¤

| éšå±¤ | å†…å®¹ | ãƒ„ãƒ¼ãƒ« | å„ªå…ˆåº¦ |
|-----|------|-------|--------|
| **ã‚¤ãƒ³ãƒ•ãƒ©å±¤** | CPU, ãƒ¡ãƒ¢ãƒª, ãƒ‡ã‚£ã‚¹ã‚¯, ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ | Prometheus + Node Exporter | P0 |
| **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å±¤** | Docker, PostgreSQL, Ray | Prometheus + å„ç¨®Exporter | P0 |
| **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤** | å®Ÿè¡Œä¸­Runæ•°ã€ã‚¨ãƒ©ãƒ¼ç‡ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | Prometheus + ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | P0 |
| **ãƒ“ã‚¸ãƒã‚¹å±¤** | äºˆæ¸¬ç²¾åº¦ã€ãƒ¢ãƒ‡ãƒ«æ•°ã€å­¦ç¿’é »åº¦ | Grafana + ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ | P1 |

---

## 2. ç›£è¦–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### 2.1 ç›£è¦–ã‚¹ã‚¿ãƒƒã‚¯å…¨ä½“åƒ

```mermaid
graph TB
    subgraph "Application"
        App[NF Auto Runner]
        App --> |metrics| PromClient[Prometheus Client]
        App --> |logs| StdOut[stdout/stderr]
        App --> |traces| OTel[OpenTelemetry<br/>Collector]
    end
    
    subgraph "Collection"
        PromClient --> |pull| Prometheus[Prometheus]
        StdOut --> |forward| Loki[Loki]
        OTel --> |push| Jaeger[Jaeger]
        
        NodeExp[Node Exporter] --> |pull| Prometheus
        PgExp[Postgres Exporter] --> |pull| Prometheus
        RayMetrics[Ray Metrics] --> |pull| Prometheus
    end
    
    subgraph "Storage"
        Prometheus --> |TSDB| PromDB[(Prometheus DB)]
        Loki --> |chunks| LokiDB[(Loki Store)]
        Jaeger --> |spans| JaegerDB[(Jaeger Store)]
    end
    
    subgraph "Visualization & Alerting"
        Prometheus --> Grafana[Grafana]
        Loki --> Grafana
        Jaeger --> Grafana
        
        Prometheus --> AlertMgr[Alert Manager]
        AlertMgr --> Email[Email]
        AlertMgr --> Slack[Slack]
        AlertMgr --> PagerDuty[PagerDuty]
    end
    
    subgraph "Users"
        Ops[Operations Team]
        Dev[Development Team]
        Ops --> Grafana
        Dev --> Grafana
    end
```

---

### 2.2 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä¸€è¦§

#### 2.2.1 ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½¹å‰² | åé›†é–“éš” | ä¿æŒæœŸé–“ |
|------------|------|---------|---------|
| **Prometheus** | ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»ä¿å­˜ | 1åˆ† | 30æ—¥ |
| **Node Exporter** | ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | 1åˆ† | 30æ—¥ |
| **Postgres Exporter** | DB ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | 1åˆ† | 30æ—¥ |
| **Prometheus Client** | ã‚¢ãƒ—ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  | - |

---

#### 2.2.2 ãƒ­ã‚°åé›†

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½¹å‰² | ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ | ä¿æŒæœŸé–“ |
|------------|------|------------|---------|
| **structlog** | æ§‹é€ åŒ–ãƒ­ã‚®ãƒ³ã‚° | JSON | - |
| **Loki** | ãƒ­ã‚°é›†ç´„ãƒ»ä¿å­˜ | LogQL | 90æ—¥ |
| **Promtail** | ãƒ­ã‚°è»¢é€ | - | - |

---

#### 2.2.3 ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½¹å‰² | ãƒ—ãƒ­ãƒˆã‚³ãƒ« | ä¿æŒæœŸé–“ |
|------------|------|-----------|---------|
| **OpenTelemetry** | ãƒˆãƒ¬ãƒ¼ã‚¹åé›† | OTLP | - |
| **Jaeger** | ãƒˆãƒ¬ãƒ¼ã‚¹å¯è¦–åŒ– | - | 7æ—¥ |

---

### 2.3 ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```
Application Layer:
â”œâ”€ Metrics â†’ Prometheus Client â†’ Prometheus (pull)
â”œâ”€ Logs â†’ structlog â†’ stdout â†’ Loki (push)
â””â”€ Traces â†’ OpenTelemetry â†’ Jaeger (push)

Infrastructure Layer:
â”œâ”€ System Metrics â†’ Node Exporter â†’ Prometheus (pull)
â”œâ”€ Database Metrics â†’ Postgres Exporter â†’ Prometheus (pull)
â””â”€ Container Metrics â†’ cAdvisor â†’ Prometheus (pull)

Visualization Layer:
â”œâ”€ Prometheus â†’ Grafana (query)
â”œâ”€ Loki â†’ Grafana (query)
â””â”€ Jaeger â†’ Grafana (query)

Alerting Layer:
â””â”€ Prometheus â†’ Alert Manager â†’ Notification Channels
```

---

## 3. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

### 3.1 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆ†é¡

#### 3.1.1 The Four Golden Signals

| Signal | èª¬æ˜ | ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¾‹ | SLO |
|--------|------|------------|-----|
| **Latency** | å¿œç­”æ™‚é–“ | å­¦ç¿’æ™‚é–“ã€äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | p95<10ç§’ |
| **Traffic** | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡ | å®Ÿè¡Œä¸­Runæ•°ã€API QPS | - |
| **Errors** | ã‚¨ãƒ©ãƒ¼ç‡ | å¤±æ•—Runç‡ | <10% |
| **Saturation** | ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ | CPU/ãƒ¡ãƒ¢ãƒª/ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ | <80% |

---

### 3.2 ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 3.2.1 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# prometheus_metrics.py

from prometheus_client import Gauge, Counter, Histogram, Info
import psutil
import time

# CPU ãƒ¡ãƒˆãƒªã‚¯ã‚¹
cpu_usage = Gauge(
    'system_cpu_usage_percent',
    'CPU usage percentage',
    ['core']
)

cpu_count = Gauge(
    'system_cpu_count',
    'Number of CPU cores'
)

# ãƒ¡ãƒ¢ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹
memory_usage = Gauge(
    'system_memory_usage_bytes',
    'Memory usage in bytes',
    ['type']  # total, available, used, free
)

memory_usage_percent = Gauge(
    'system_memory_usage_percent',
    'Memory usage percentage'
)

# ãƒ‡ã‚£ã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹
disk_usage = Gauge(
    'system_disk_usage_bytes',
    'Disk usage in bytes',
    ['path', 'type']  # used, free, total
)

disk_usage_percent = Gauge(
    'system_disk_usage_percent',
    'Disk usage percentage',
    ['path']
)

disk_io = Counter(
    'system_disk_io_bytes_total',
    'Total disk I/O in bytes',
    ['device', 'direction']  # read, write
)

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹
network_bytes = Counter(
    'system_network_bytes_total',
    'Total network traffic in bytes',
    ['interface', 'direction']  # sent, received
)

network_errors = Counter(
    'system_network_errors_total',
    'Total network errors',
    ['interface', 'direction']  # sent, received
)

# GPU ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (GPUãŒã‚ã‚‹å ´åˆ)
gpu_usage = Gauge(
    'system_gpu_usage_percent',
    'GPU usage percentage',
    ['gpu_id']
)

gpu_memory = Gauge(
    'system_gpu_memory_bytes',
    'GPU memory in bytes',
    ['gpu_id', 'type']  # used, free, total
)

gpu_temperature = Gauge(
    'system_gpu_temperature_celsius',
    'GPU temperature in Celsius',
    ['gpu_id']
)


class SystemMetricsCollector:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
    
    def __init__(self, interval: int = 60):
        """
        Args:
            interval: åé›†é–“éš”ï¼ˆç§’ï¼‰
        """
        self.interval = interval
    
    def collect_cpu_metrics(self):
        """CPU ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        # CPUä½¿ç”¨ç‡ï¼ˆã‚³ã‚¢ã”ã¨ï¼‰
        for i, percent in enumerate(psutil.cpu_percent(percpu=True)):
            cpu_usage.labels(core=f'cpu{i}').set(percent)
        
        # CPUç·æ•°
        cpu_count.set(psutil.cpu_count())
    
    def collect_memory_metrics(self):
        """ãƒ¡ãƒ¢ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        mem = psutil.virtual_memory()
        
        memory_usage.labels(type='total').set(mem.total)
        memory_usage.labels(type='available').set(mem.available)
        memory_usage.labels(type='used').set(mem.used)
        memory_usage.labels(type='free').set(mem.free)
        
        memory_usage_percent.set(mem.percent)
    
    def collect_disk_metrics(self):
        """ãƒ‡ã‚£ã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        for partition in psutil.disk_partitions():
            try:
                usage = psutil.disk_usage(partition.mountpoint)
                
                disk_usage.labels(
                    path=partition.mountpoint,
                    type='total'
                ).set(usage.total)
                
                disk_usage.labels(
                    path=partition.mountpoint,
                    type='used'
                ).set(usage.used)
                
                disk_usage.labels(
                    path=partition.mountpoint,
                    type='free'
                ).set(usage.free)
                
                disk_usage_percent.labels(
                    path=partition.mountpoint
                ).set(usage.percent)
            
            except PermissionError:
                # ä¸€éƒ¨ã®ãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆã¯ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯
                continue
        
        # ãƒ‡ã‚£ã‚¹ã‚¯I/O
        disk_io_counters = psutil.disk_io_counters(perdisk=True)
        for device, counters in disk_io_counters.items():
            disk_io.labels(device=device, direction='read').inc(
                counters.read_bytes
            )
            disk_io.labels(device=device, direction='write').inc(
                counters.write_bytes
            )
    
    def collect_network_metrics(self):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        net_io = psutil.net_io_counters(pernic=True)
        
        for interface, counters in net_io.items():
            network_bytes.labels(
                interface=interface,
                direction='sent'
            ).inc(counters.bytes_sent)
            
            network_bytes.labels(
                interface=interface,
                direction='received'
            ).inc(counters.bytes_recv)
            
            network_errors.labels(
                interface=interface,
                direction='sent'
            ).inc(counters.errout)
            
            network_errors.labels(
                interface=interface,
                direction='received'
            ).inc(counters.errin)
    
    def collect_gpu_metrics(self):
        """GPU ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            
            for gpu in gpus:
                gpu_usage.labels(gpu_id=gpu.id).set(gpu.load * 100)
                
                gpu_memory.labels(
                    gpu_id=gpu.id,
                    type='used'
                ).set(gpu.memoryUsed * 1024 * 1024)  # MB to bytes
                
                gpu_memory.labels(
                    gpu_id=gpu.id,
                    type='free'
                ).set(gpu.memoryFree * 1024 * 1024)
                
                gpu_memory.labels(
                    gpu_id=gpu.id,
                    type='total'
                ).set(gpu.memoryTotal * 1024 * 1024)
                
                gpu_temperature.labels(gpu_id=gpu.id).set(gpu.temperature)
        
        except ImportError:
            # GPU ãŒãªã„ã€ã¾ãŸã¯GPUtilãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„
            pass
    
    def collect_all(self):
        """å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
        self.collect_cpu_metrics()
        self.collect_memory_metrics()
        self.collect_disk_metrics()
        self.collect_network_metrics()
        self.collect_gpu_metrics()
    
    def start(self):
        """å®šæœŸåé›†ã‚’é–‹å§‹"""
        while True:
            self.collect_all()
            time.sleep(self.interval)
```

**åé›†é–“éš”**: 1åˆ†
**ä¿æŒæœŸé–“**: 30æ—¥

---

### 3.3 ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 3.3.1 å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# application_metrics.py

from prometheus_client import Counter, Gauge, Histogram, Summary
from typing import List
import time

# Runå®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹
runs_total = Counter(
    'nf_runs_total',
    'Total number of runs',
    ['status', 'model_name', 'backend']
)

runs_active = Gauge(
    'nf_runs_active',
    'Number of currently active runs'
)

runs_queued = Gauge(
    'nf_runs_queued',
    'Number of runs in queue'
)

# å­¦ç¿’æ™‚é–“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
training_duration_seconds = Histogram(
    'nf_training_duration_seconds',
    'Model training duration in seconds',
    ['model_name', 'backend'],
    buckets=[60, 120, 300, 600, 1200, 1800, 3600]  # 1åˆ†ã€œ1æ™‚é–“
)

# äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¡ãƒˆãƒªã‚¯ã‚¹
prediction_latency_seconds = Histogram(
    'nf_prediction_latency_seconds',
    'Prediction latency in seconds',
    ['model_name'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]  # 10msã€œ5ç§’
)

# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒ¡ãƒˆãƒªã‚¯ã‚¹
dataset_size = Histogram(
    'nf_dataset_size_rows',
    'Dataset size in number of rows',
    buckets=[100, 1000, 10000, 100000, 1000000, 10000000]
)

unique_ids_count = Histogram(
    'nf_unique_ids_count',
    'Number of unique IDs',
    buckets=[10, 100, 1000, 10000]
)

# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
errors_total = Counter(
    'nf_errors_total',
    'Total number of errors',
    ['error_type', 'component']
)

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
model_accuracy = Gauge(
    'nf_model_accuracy',
    'Model accuracy metric (e.g., sMAPE)',
    ['model_name', 'metric_type']
)

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
memory_usage_bytes = Gauge(
    'nf_memory_usage_bytes',
    'Memory usage by component',
    ['component']
)

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
db_connections = Gauge(
    'nf_db_connections_active',
    'Number of active database connections'
)

db_query_duration_seconds = Histogram(
    'nf_db_query_duration_seconds',
    'Database query duration in seconds',
    ['query_type'],
    buckets=[0.001, 0.01, 0.1, 0.5, 1.0, 5.0]  # 1msã€œ5ç§’
)


class ApplicationMetricsCollector:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
    
    @staticmethod
    def record_run_start(model_name: str, backend: str):
        """Runé–‹å§‹ã‚’è¨˜éŒ²"""
        runs_active.inc()
        runs_total.labels(
            status='started',
            model_name=model_name,
            backend=backend
        ).inc()
    
    @staticmethod
    def record_run_success(
        model_name: str,
        backend: str,
        duration: float
    ):
        """RunæˆåŠŸã‚’è¨˜éŒ²"""
        runs_active.dec()
        runs_total.labels(
            status='success',
            model_name=model_name,
            backend=backend
        ).inc()
        
        training_duration_seconds.labels(
            model_name=model_name,
            backend=backend
        ).observe(duration)
    
    @staticmethod
    def record_run_failure(
        model_name: str,
        backend: str,
        error_type: str
    ):
        """Runå¤±æ•—ã‚’è¨˜éŒ²"""
        runs_active.dec()
        runs_total.labels(
            status='failed',
            model_name=model_name,
            backend=backend
        ).inc()
        
        errors_total.labels(
            error_type=error_type,
            component='training'
        ).inc()
    
    @staticmethod
    def record_prediction(model_name: str, duration: float):
        """äºˆæ¸¬å®Ÿè¡Œã‚’è¨˜éŒ²"""
        prediction_latency_seconds.labels(
            model_name=model_name
        ).observe(duration)
    
    @staticmethod
    def record_dataset_info(n_rows: int, n_unique_ids: int):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¨˜éŒ²"""
        dataset_size.observe(n_rows)
        unique_ids_count.observe(n_unique_ids)
    
    @staticmethod
    def update_model_accuracy(
        model_name: str,
        metric_type: str,
        value: float
    ):
        """ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ã‚’æ›´æ–°"""
        model_accuracy.labels(
            model_name=model_name,
            metric_type=metric_type
        ).set(value)
    
    @staticmethod
    def record_db_query(query_type: str, duration: float):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã‚’è¨˜éŒ²"""
        db_query_duration_seconds.labels(
            query_type=query_type
        ).observe(duration)
```

**åé›†é–“éš”**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ï¼‰
**ä¿æŒæœŸé–“**: 30æ—¥

---

### 3.4 ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 3.4.1 KPI ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
# business_metrics.py

from prometheus_client import Gauge, Counter
from datetime import datetime, timedelta

# æ—¥æ¬¡KPI
daily_runs_count = Counter(
    'nf_daily_runs_count',
    'Number of runs per day',
    ['date']
)

daily_models_trained = Counter(
    'nf_daily_models_trained',
    'Number of models trained per day',
    ['date']
)

# é€±æ¬¡KPI
weekly_average_accuracy = Gauge(
    'nf_weekly_average_accuracy',
    'Weekly average model accuracy',
    ['week', 'metric_type']
)

# ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡
system_uptime_seconds = Counter(
    'nf_system_uptime_seconds_total',
    'Total system uptime in seconds'
)

system_availability_percent = Gauge(
    'nf_system_availability_percent',
    'System availability percentage'
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£
user_sessions = Gauge(
    'nf_user_sessions_active',
    'Number of active user sessions'
)

# ã‚³ã‚¹ãƒˆåŠ¹ç‡
cost_per_prediction = Gauge(
    'nf_cost_per_prediction_usd',
    'Estimated cost per prediction in USD',
    ['model_type']
)


class BusinessMetricsCollector:
    """ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†"""
    
    def __init__(self, db_session):
        self.db_session = db_session
    
    def collect_daily_kpis(self):
        """æ—¥æ¬¡KPIã‚’åé›†"""
        today = datetime.now().date()
        
        # ä»Šæ—¥ã®Runæ•°
        run_count = self.db_session.query(Run).filter(
            Run.created_at >= today
        ).count()
        
        daily_runs_count.labels(date=str(today)).inc(run_count)
        
        # ä»Šæ—¥å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«æ•°
        model_count = self.db_session.query(Model).filter(
            Model.created_at >= today
        ).count()
        
        daily_models_trained.labels(date=str(today)).inc(model_count)
    
    def collect_weekly_kpis(self):
        """é€±æ¬¡KPIã‚’åé›†"""
        # ä»Šé€±ã®é–‹å§‹æ—¥
        today = datetime.now().date()
        week_start = today - timedelta(days=today.weekday())
        
        # ä»Šé€±ã®å¹³å‡ç²¾åº¦
        avg_accuracy = self.db_session.query(
            func.avg(Model.smape)
        ).filter(
            Model.created_at >= week_start
        ).scalar()
        
        if avg_accuracy:
            weekly_average_accuracy.labels(
                week=str(week_start),
                metric_type='smape'
            ).set(avg_accuracy)
    
    def update_availability(self, downtime_seconds: float):
        """ç¨¼åƒç‡ã‚’æ›´æ–°"""
        # æœˆé–“ã®ç·æ™‚é–“ï¼ˆç§’ï¼‰
        total_seconds = 30 * 24 * 60 * 60  # 30æ—¥
        
        # ç¨¼åƒç‡ã‚’è¨ˆç®—
        availability = (
            (total_seconds - downtime_seconds) / total_seconds
        ) * 100
        
        system_availability_percent.set(availability)
```

**åé›†é–“éš”**: 1æ™‚é–“
**ä¿æŒæœŸé–“**: 180æ—¥

---

### 3.5 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

#### 3.5.1 Prometheus Exporterè¨­å®š

```python
# prometheus_exporter.py

from prometheus_client import start_http_server
import time
import logging

logger = logging.getLogger(__name__)


class PrometheusExporter:
    """Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self, port: int = 9090):
        """
        Args:
            port: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ãƒãƒ¼ãƒˆ
        """
        self.port = port
        self.collectors = []
    
    def register_collector(self, collector):
        """ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç™»éŒ²"""
        self.collectors.append(collector)
    
    def start(self):
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’èµ·å‹•"""
        try:
            # HTTPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
            start_http_server(self.port)
            logger.info(
                f"Prometheus exporter started on port {self.port}"
            )
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚’é–‹å§‹
            for collector in self.collectors:
                if hasattr(collector, 'start'):
                    collector.start()
            
            # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
            while True:
                time.sleep(60)
        
        except Exception as e:
            logger.error(f"Failed to start Prometheus exporter: {e}")
            raise


# ä½¿ç”¨ä¾‹
if __name__ == '__main__':
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ
    exporter = PrometheusExporter(port=9090)
    
    # ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç™»éŒ²
    system_collector = SystemMetricsCollector(interval=60)
    exporter.register_collector(system_collector)
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’èµ·å‹•
    exporter.start()
```

---

## 4. ãƒ­ã‚®ãƒ³ã‚°æˆ¦ç•¥

### 4.1 ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«

#### 4.1.1 ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«å®šç¾©

| ãƒ¬ãƒ™ãƒ« | ç”¨é€” | ä¾‹ | å‡ºåŠ›å…ˆ |
|-------|------|---|--------|
| **CRITICAL** | ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä¸å¯ | stdout, file, alert |
| **ERROR** | ã‚¨ãƒ©ãƒ¼ã ãŒç¶™ç¶šå¯èƒ½ | ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å¤±æ•— | stdout, file |
| **WARNING** | è­¦å‘Š | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡é«˜ã„ | stdout, file |
| **INFO** | é€šå¸¸å‹•ä½œã®è¨˜éŒ² | ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ | stdout, file |
| **DEBUG** | ãƒ‡ãƒãƒƒã‚°æƒ…å ± | å¤‰æ•°å€¤ã€é–¢æ•°å‘¼ã³å‡ºã— | file (é–‹ç™ºæ™‚ã®ã¿) |

---

### 4.2 æ§‹é€ åŒ–ãƒ­ã‚®ãƒ³ã‚°

#### 4.2.1 ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

```json
{
  "timestamp": "2025-11-03T12:34:56.789Z",
  "level": "INFO",
  "logger": "nf_auto_runner.service.execution",
  "message": "Model training started",
  "context": {
    "run_id": "abc123",
    "experiment_id": "exp456",
    "model_name": "AutoNHITS",
    "backend": "optuna",
    "dataset_version": "v1.0",
    "user_id": "user123"
  },
  "metadata": {
    "hostname": "server01",
    "process_id": 12345,
    "thread_id": 67890,
    "correlation_id": "req-xyz789"
  },
  "performance": {
    "cpu_percent": 45.2,
    "memory_mb": 2048,
    "duration_ms": 1250
  }
}
```

---

#### 4.2.2 æ§‹é€ åŒ–ãƒ­ã‚¬ãƒ¼å®Ÿè£…

```python
# structured_logger.py

import structlog
import logging
import sys
from pathlib import Path
from typing import Any, Dict, Optional
import json


class StructuredLogger:
    """æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›ã‚¯ãƒ©ã‚¹"""
    
    _instance = None
    
    def __new__(cls):
        """Singleton ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        if not hasattr(self, 'initialized'):
            self._configure_logging()
            self.initialized = True
    
    def _configure_logging(self):
        """ãƒ­ã‚®ãƒ³ã‚°è¨­å®š"""
        # structlogè¨­å®š
        structlog.configure(
            processors=[
                structlog.contextvars.merge_contextvars,
                structlog.processors.add_log_level,
                structlog.processors.TimeStamper(
                    fmt="iso",
                    utc=True
                ),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            wrapper_class=structlog.make_filtering_bound_logger(
                logging.INFO
            ),
            context_class=dict,
            logger_factory=structlog.PrintLoggerFactory(),
            cache_logger_on_first_use=True,
        )
        
        # æ¨™æº–ãƒ­ã‚¬ãƒ¼è¨­å®š
        logging.basicConfig(
            format="%(message)s",
            stream=sys.stdout,
            level=logging.INFO,
        )
    
    @classmethod
    def get_logger(
        cls,
        name: str,
        **initial_context: Any
    ) -> structlog.BoundLogger:
        """
        ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—
        
        Args:
            name: ãƒ­ã‚¬ãƒ¼å
            **initial_context: åˆæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            structlog.BoundLogger
        """
        instance = cls()
        logger = structlog.get_logger(name)
        
        # åˆæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒã‚¤ãƒ³ãƒ‰
        if initial_context:
            logger = logger.bind(**initial_context)
        
        return logger
    
    @staticmethod
    def log_event(
        logger: structlog.BoundLogger,
        event: str,
        level: str = 'info',
        **context: Any
    ):
        """
        ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ­ã‚°
        
        Args:
            logger: ãƒ­ã‚¬ãƒ¼
            event: ã‚¤ãƒ™ãƒ³ãƒˆå
            level: ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
            **context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        """
        log_method = getattr(logger, level.lower())
        log_method(event, **context)


# ä½¿ç”¨ä¾‹
logger = StructuredLogger.get_logger(
    'nf_auto_runner.training',
    component='training_service',
    environment='production'
)

# ãƒ­ã‚°å‡ºåŠ›
logger.info(
    'model_training_started',
    run_id='abc123',
    model_name='AutoLSTM',
    backend='optuna'
)

logger.error(
    'model_training_failed',
    run_id='abc123',
    model_name='AutoLSTM',
    error_type='ResourceExhausted',
    error_message='Out of memory'
)
```

---

### 4.3 ãƒ­ã‚°å‡ºåŠ›å…ˆ

#### 4.3.1 å‡ºåŠ›å…ˆè¨­å®š

| å‡ºåŠ›å…ˆ | ç”¨é€” | ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ | å„ªå…ˆåº¦ |
|-------|------|--------------|--------|
| **ãƒ•ã‚¡ã‚¤ãƒ«** | æ°¸ç¶šåŒ– | æ—¥æ¬¡ or 100MB | P0 |
| **æ¨™æº–å‡ºåŠ›** | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¢ºèª | ãªã— | P0 |
| **Loki** | é›†ç´„ãƒ»æ¤œç´¢ | ã‚µãƒ¼ãƒãƒ¼å´ | P2 |
| **Elasticsearch** | åˆ†æ | ã‚µãƒ¼ãƒãƒ¼å´ | P3 |

---

#### 4.3.2 ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼è¨­å®š

```python
# file_logger.py

import logging
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
from pathlib import Path


class FileLogger:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚¬ãƒ¼"""
    
    @staticmethod
    def setup_rotating_file_handler(
        log_file: Path,
        max_bytes: int = 100 * 1024 * 1024,  # 100MB
        backup_count: int = 10
    ) -> RotatingFileHandler:
        """
        ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        
        Args:
            log_file: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            max_bytes: æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            backup_count: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°
        
        Returns:
            RotatingFileHandler
        """
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä½œæˆ
        handler = RotatingFileHandler(
            filename=log_file,
            maxBytes=max_bytes,
            backupCount=backup_count,
            encoding='utf-8'
        )
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        
        return handler
    
    @staticmethod
    def setup_timed_rotating_handler(
        log_file: Path,
        when: str = 'midnight',
        interval: int = 1,
        backup_count: int = 30
    ) -> TimedRotatingFileHandler:
        """
        æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        
        Args:
            log_file: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            when: ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒŸãƒ³ã‚°
            interval: ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–“éš”
            backup_count: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°
        
        Returns:
            TimedRotatingFileHandler
        """
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ä½œæˆ
        handler = TimedRotatingFileHandler(
            filename=log_file,
            when=when,
            interval=interval,
            backupCount=backup_count,
            encoding='utf-8'
        )
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        
        return handler


# ä½¿ç”¨ä¾‹
from pathlib import Path

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
app_logger = logging.getLogger('nf_auto_runner')
app_logger.setLevel(logging.INFO)

# ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
app_handler = FileLogger.setup_rotating_file_handler(
    log_file=Path('logs/app/nf_runner.log'),
    max_bytes=100 * 1024 * 1024,  # 100MB
    backup_count=10
)
app_logger.addHandler(app_handler)

# æ™‚é–“ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
daily_handler = FileLogger.setup_timed_rotating_handler(
    log_file=Path('logs/daily/nf_runner.log'),
    when='midnight',
    interval=1,
    backup_count=30
)
app_logger.addHandler(daily_handler)
```

---

### 4.4 ãƒ­ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†

#### 4.4.1 ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°

```python
# log_context.py

import contextvars
from typing import Any, Dict, Optional
import uuid


# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°
correlation_id_var = contextvars.ContextVar(
    'correlation_id',
    default=None
)

run_id_var = contextvars.ContextVar(
    'run_id',
    default=None
)

user_id_var = contextvars.ContextVar(
    'user_id',
    default=None
)


class LogContext:
    """ãƒ­ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†"""
    
    @staticmethod
    def set_correlation_id(correlation_id: Optional[str] = None) -> str:
        """
        ç›¸é–¢IDã‚’è¨­å®š
        
        Args:
            correlation_id: ç›¸é–¢IDï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
        Returns:
            è¨­å®šã•ã‚ŒãŸç›¸é–¢ID
        """
        if correlation_id is None:
            correlation_id = str(uuid.uuid4())
        
        correlation_id_var.set(correlation_id)
        return correlation_id
    
    @staticmethod
    def get_correlation_id() -> Optional[str]:
        """ç›¸é–¢IDã‚’å–å¾—"""
        return correlation_id_var.get()
    
    @staticmethod
    def set_run_id(run_id: str):
        """Run IDã‚’è¨­å®š"""
        run_id_var.set(run_id)
    
    @staticmethod
    def get_run_id() -> Optional[str]:
        """Run IDã‚’å–å¾—"""
        return run_id_var.get()
    
    @staticmethod
    def set_user_id(user_id: str):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’è¨­å®š"""
        user_id_var.set(user_id)
    
    @staticmethod
    def get_user_id() -> Optional[str]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’å–å¾—"""
        return user_id_var.get()
    
    @staticmethod
    def get_context() -> Dict[str, Any]:
        """å…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        context = {}
        
        if (correlation_id := correlation_id_var.get()) is not None:
            context['correlation_id'] = correlation_id
        
        if (run_id := run_id_var.get()) is not None:
            context['run_id'] = run_id
        
        if (user_id := user_id_var.get()) is not None:
            context['user_id'] = user_id
        
        return context
    
    @staticmethod
    def clear():
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢"""
        correlation_id_var.set(None)
        run_id_var.set(None)
        user_id_var.set(None)


# ä½¿ç”¨ä¾‹
from nf_auto_runner.logging import StructuredLogger, LogContext

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
LogContext.set_correlation_id()
LogContext.set_run_id('run-123')
LogContext.set_user_id('user-456')

# ãƒ­ã‚¬ãƒ¼å–å¾—ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè‡ªå‹•ä»˜ä¸ï¼‰
logger = StructuredLogger.get_logger(
    'nf_auto_runner.training',
    **LogContext.get_context()
)

# ãƒ­ã‚°å‡ºåŠ›
logger.info('model_training_started', model_name='AutoLSTM')
# å‡ºåŠ›: {"timestamp": "...", "correlation_id": "...", "run_id": "run-123", ...}
```

---

## 5. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

### 5.1 ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«

#### 5.1.1 Critical ã‚¢ãƒ©ãƒ¼ãƒˆ

| ã‚¢ãƒ©ãƒ¼ãƒˆå | æ¡ä»¶ | é‡è¦åº¦ | é€šçŸ¥å…ˆ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|----------|------|-------|-------|----------|
| **SystemDown** | ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢ | Critical | Email, Slack, PagerDuty | å³åº§ã«å¯¾å¿œ |
| **DatabaseDown** | DBæ¥ç¶šä¸å¯ for 1åˆ† | Critical | Email, Slack, PagerDuty | å³åº§ã«å¯¾å¿œ |
| **DiskFull** | ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡>95% | Critical | Email, Slack | 1æ™‚é–“ä»¥å†…ã«å¯¾å¿œ |

---

#### 5.1.2 High ã‚¢ãƒ©ãƒ¼ãƒˆ

| ã‚¢ãƒ©ãƒ¼ãƒˆå | æ¡ä»¶ | é‡è¦åº¦ | é€šçŸ¥å…ˆ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|----------|------|-------|-------|----------|
| **HighMemoryUsage** | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡>90% for 5åˆ† | High | Email, Slack | 2æ™‚é–“ä»¥å†…ã«å¯¾å¿œ |
| **HighErrorRate** | ã‚¨ãƒ©ãƒ¼ç‡>10% for 5åˆ† | High | Email, Slack | 2æ™‚é–“ä»¥å†…ã«å¯¾å¿œ |
| **SlowResponse** | p95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·>10ç§’ for 10åˆ† | High | Slack | 4æ™‚é–“ä»¥å†…ã«å¯¾å¿œ |

---

#### 5.1.3 Medium ã‚¢ãƒ©ãƒ¼ãƒˆ

| ã‚¢ãƒ©ãƒ¼ãƒˆå | æ¡ä»¶ | é‡è¦åº¦ | é€šçŸ¥å…ˆ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|----------|------|-------|-------|----------|
| **HighCPUUsage** | CPUä½¿ç”¨ç‡>80% for 10åˆ† | Medium | Slack | ç¿Œå–¶æ¥­æ—¥ã«ç¢ºèª |
| **LongRunningTask** | å®Ÿè¡Œæ™‚é–“>2xå¹³å‡ | Medium | Slack | ç¿Œå–¶æ¥­æ—¥ã«ç¢ºèª |
| **QueueBacklog** | ã‚­ãƒ¥ãƒ¼>50 for 10åˆ† | Medium | Slack | ç¿Œå–¶æ¥­æ—¥ã«ç¢ºèª |

---

### 5.2 Prometheusã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«

#### 5.2.1 ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«è¨­å®š

```yaml
# prometheus/alerts/nf_runner_alerts.yml

groups:
  - name: nf_runner_critical
    interval: 1m
    rules:
      # ã‚·ã‚¹ãƒ†ãƒ ãƒ€ã‚¦ãƒ³
      - alert: SystemDown
        expr: up{job="nf-runner"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "NF Runner is down"
          description: "NF Runner process has been down for more than 1 minute"
          dashboard: "https://grafana.example.com/d/nf-runner"
      
      # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL has been down for more than 1 minute"
      
      # ãƒ‡ã‚£ã‚¹ã‚¯æº€æ¯
      - alert: DiskFull
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            / node_filesystem_size_bytes{mountpoint="/"}
          ) * 100 < 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Disk space is critically low"
          description: "Disk usage is above 95% (current: {{ $value }}%)"

  - name: nf_runner_high
    interval: 1m
    rules:
      # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
      - alert: HighMemoryUsage
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes * 100 > 90
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "Memory usage is high"
          description: "Memory usage is above 90% for 5 minutes (current: {{ $value }}%)"
      
      # é«˜ã‚¨ãƒ©ãƒ¼ç‡
      - alert: HighErrorRate
        expr: |
          (
            rate(nf_runs_total{status="failed"}[5m])
            / rate(nf_runs_total[5m])
          ) * 100 > 10
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 10% for 5 minutes (current: {{ $value }}%)"
      
      # é…ã„å¿œç­”
      - alert: SlowResponse
        expr: |
          histogram_quantile(0.95,
            rate(nf_training_duration_seconds_bucket[10m])
          ) > 600
        for: 10m
        labels:
          severity: high
        annotations:
          summary: "Training duration is slow"
          description: "95th percentile training duration is above 10 minutes (current: {{ $value }}s)"

  - name: nf_runner_medium
    interval: 5m
    rules:
      # é«˜CPUä½¿ç”¨ç‡
      - alert: HighCPUUsage
        expr: |
          100 - (avg by (instance) (
            rate(node_cpu_seconds_total{mode="idle"}[5m])
          ) * 100) > 80
        for: 10m
        labels:
          severity: medium
        annotations:
          summary: "CPU usage is high"
          description: "CPU usage is above 80% for 10 minutes (current: {{ $value }}%)"
      
      # é•·æ™‚é–“å®Ÿè¡Œã‚¿ã‚¹ã‚¯
      - alert: LongRunningTask
        expr: |
          nf_training_duration_seconds > (
            avg_over_time(nf_training_duration_seconds[24h]) * 2
          )
        labels:
          severity: medium
        annotations:
          summary: "Long running task detected"
          description: "Task duration is more than 2x the average (current: {{ $value }}s)"
      
      # ã‚­ãƒ¥ãƒ¼æ»ç•™
      - alert: QueueBacklog
        expr: nf_runs_queued > 50
        for: 10m
        labels:
          severity: medium
        annotations:
          summary: "Queue backlog detected"
          description: "More than 50 runs are queued for 10 minutes (current: {{ $value }})"
```

---

### 5.3 Alert Managerè¨­å®š

#### 5.3.1 Alert Managerè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# alertmanager/alertmanager.yml

global:
  # ãƒ¡ãƒ¼ãƒ«è¨­å®š
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: '${SMTP_PASSWORD}'
  
  # Slack Webhook URL
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨­å®š
route:
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå—ä¿¡è€…
  receiver: 'team-email'
  
  # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–è¨­å®š
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  
  # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«
  routes:
    # Critical ã‚¢ãƒ©ãƒ¼ãƒˆ
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true
      repeat_interval: 1h
    
    # High ã‚¢ãƒ©ãƒ¼ãƒˆ
    - match:
        severity: high
      receiver: 'high-alerts'
      repeat_interval: 2h
    
    # Medium ã‚¢ãƒ©ãƒ¼ãƒˆ
    - match:
        severity: medium
      receiver: 'medium-alerts'
      repeat_interval: 6h

# å—ä¿¡è€…è¨­å®š
receivers:
  # Critical ã‚¢ãƒ©ãƒ¼ãƒˆå—ä¿¡è€…
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
    slack_configs:
      - channel: '#alerts-critical'
        title: '[CRITICAL] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Description:* {{ .Annotations.description }}
          *Dashboard:* {{ .Annotations.dashboard }}
          {{ end }}
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
  
  # High ã‚¢ãƒ©ãƒ¼ãƒˆå—ä¿¡è€…
  - name: 'high-alerts'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: '[HIGH] {{ .GroupLabels.alertname }}'
    slack_configs:
      - channel: '#alerts-high'
        title: '[HIGH] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
  
  # Medium ã‚¢ãƒ©ãƒ¼ãƒˆå—ä¿¡è€…
  - name: 'medium-alerts'
    slack_configs:
      - channel: '#alerts-medium'
        title: '[MEDIUM] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
  
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå—ä¿¡è€…
  - name: 'team-email'
    email_configs:
      - to: 'team@example.com'

# æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«
inhibit_rules:
  # SystemDownã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç«ä¸­ã¯ä»–ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æŠ‘åˆ¶
  - source_match:
      alertname: 'SystemDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']
  
  # DatabaseDownã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç«ä¸­ã¯DBé–¢é€£ã‚¢ãƒ©ãƒ¼ãƒˆã‚’æŠ‘åˆ¶
  - source_match:
      alertname: 'DatabaseDown'
    target_match_re:
      alertname: 'Database.*'
    equal: ['instance']
```

---

### 5.4 ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

#### 5.4.1 Emailé€šçŸ¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```html
<!-- alertmanager/templates/email.html -->

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{{ .GroupLabels.alertname }}</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }
        .alert-critical {
            background-color: #ff4444;
            color: white;
            padding: 15px;
            border-radius: 5px;
        }
        .alert-high {
            background-color: #ff8800;
            color: white;
            padding: 15px;
            border-radius: 5px;
        }
        .alert-medium {
            background-color: #ffbb33;
            color: white;
            padding: 15px;
            border-radius: 5px;
        }
        .details {
            margin-top: 20px;
            padding: 15px;
            background-color: #f5f5f5;
            border-radius: 5px;
        }
        .actions {
            margin-top: 20px;
            padding: 15px;
            background-color: #e3f2fd;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <div class="alert-{{ .GroupLabels.severity }}">
        <h2>[{{ .GroupLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}</h2>
    </div>
    
    <div class="details">
        <h3>Details</h3>
        {{ range .Alerts }}
        <p>
            <strong>Time:</strong> {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}<br>
            <strong>Instance:</strong> {{ .Labels.instance }}<br>
            <strong>Description:</strong> {{ .Annotations.description }}
        </p>
        {{ end }}
    </div>
    
    <div class="actions">
        <h3>Recommended Actions</h3>
        <ol>
            <li>Check the dashboard: <a href="{{ .Annotations.dashboard }}">{{ .Annotations.dashboard }}</a></li>
            <li>Review recent logs</li>
            <li>Check system resources</li>
            <li>Follow the runbook</li>
        </ol>
    </div>
    
    <p>
        <small>
            This alert was generated by Prometheus Alert Manager.<br>
            Dashboard: <a href="{{ .ExternalURL }}">{{ .ExternalURL }}</a>
        </small>
    </p>
</body>
</html>
```

---

#### 5.4.2 Slacké€šçŸ¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```yaml
# alertmanager/templates/slack.tmpl

{{ define "slack.default.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
{{ end }}

{{ define "slack.default.text" }}
{{ range .Alerts }}
*Alert:* {{ .Labels.alertname }}
*Severity:* {{ .Labels.severity }}
*Instance:* {{ .Labels.instance }}
*Description:* {{ .Annotations.description }}
*Dashboard:* {{ .Annotations.dashboard }}
*Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
---
{{ end }}
{{ end }}
```

---

## 6. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ

### 6.1 Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹æˆ

#### 6.1.1 ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä¸€è¦§

| ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å | ç›®çš„ | ä¸»è¦ãƒ‘ãƒãƒ« | æ›´æ–°é »åº¦ |
|-------------|------|----------|---------|
| **System Overview** | ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ¦‚è¦ | ç¨¼åƒç‡ã€ãƒªã‚½ãƒ¼ã‚¹ã€Runæ•° | 1åˆ† |
| **Application Metrics** | ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦– | å®Ÿè¡Œæ™‚é–“ã€ã‚¨ãƒ©ãƒ¼ç‡ã€QPS | 1åˆ† |
| **Model Performance** | ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | ç²¾åº¦ã€å­¦ç¿’æ™‚é–“ã€äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | 5åˆ† |
| **Database Monitoring** | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›£è¦– | æ¥ç¶šæ•°ã€ã‚¯ã‚¨ãƒªæ™‚é–“ã€å®¹é‡ | 1åˆ† |
| **Business KPIs** | ãƒ“ã‚¸ãƒã‚¹KPI | æ—¥æ¬¡Runæ•°ã€é€±æ¬¡ç²¾åº¦ã€ã‚³ã‚¹ãƒˆ | 1æ™‚é–“ |

---

### 6.2 System Overview ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### 6.2.1 ãƒ‘ãƒãƒ«æ§‹æˆ

```json
{
  "dashboard": {
    "title": "NF Runner - System Overview",
    "tags": ["nf-runner", "system"],
    "timezone": "browser",
    "refresh": "1m",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "System Uptime",
        "type": "stat",
        "gridPos": {"x": 0, "y": 0, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "time() - process_start_time_seconds{job=\"nf-runner\"}",
            "legendFormat": "Uptime",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 3600, "color": "yellow"},
                {"value": 86400, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "CPU Usage",
        "type": "graph",
        "gridPos": {"x": 6, "y": 0, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU %",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "percent",
            "max": 100,
            "min": 0
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {"params": [80], "type": "gt"},
              "operator": {"type": "and"},
              "query": {"params": ["A", "5m", "now"]},
              "reducer": {"params": [], "type": "avg"},
              "type": "query"
            }
          ],
          "executionErrorState": "alerting",
          "frequency": "1m",
          "handler": 1,
          "name": "CPU Usage Alert",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 3,
        "title": "Memory Usage",
        "type": "graph",
        "gridPos": {"x": 12, "y": 0, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "Memory %",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "percent",
            "max": 100,
            "min": 0
          }
        ]
      },
      {
        "id": 4,
        "title": "Disk Usage",
        "type": "graph",
        "gridPos": {"x": 18, "y": 0, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "(node_filesystem_size_bytes{mountpoint=\"/\"} - node_filesystem_avail_bytes{mountpoint=\"/\"}) / node_filesystem_size_bytes{mountpoint=\"/\"} * 100",
            "legendFormat": "Disk %",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "percent",
            "max": 100,
            "min": 0
          }
        ]
      },
      {
        "id": 5,
        "title": "Active Runs",
        "type": "stat",
        "gridPos": {"x": 0, "y": 4, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "nf_runs_active",
            "legendFormat": "Active Runs",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 50, "color": "yellow"},
                {"value": 100, "color": "red"}
              ]
            }
          }
        }
      },
      {
        "id": 6,
        "title": "Run Success Rate",
        "type": "gauge",
        "gridPos": {"x": 6, "y": 4, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "(sum(rate(nf_runs_total{status=\"success\"}[5m])) / sum(rate(nf_runs_total[5m]))) * 100",
            "legendFormat": "Success Rate %",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "max": 100,
            "min": 0,
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 90, "color": "yellow"},
                {"value": 95, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "id": 7,
        "title": "Training Duration (p50, p95, p99)",
        "type": "graph",
        "gridPos": {"x": 12, "y": 4, "w": 12, "h": 4},
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(nf_training_duration_seconds_bucket[5m]))",
            "legendFormat": "p50",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.95, rate(nf_training_duration_seconds_bucket[5m]))",
            "legendFormat": "p95",
            "refId": "B"
          },
          {
            "expr": "histogram_quantile(0.99, rate(nf_training_duration_seconds_bucket[5m]))",
            "legendFormat": "p99",
            "refId": "C"
          }
        ],
        "yaxes": [
          {
            "format": "s",
            "logBase": 1,
            "min": 0
          }
        ]
      },
      {
        "id": 8,
        "title": "Error Rate by Type",
        "type": "graph",
        "gridPos": {"x": 0, "y": 8, "w": 12, "h": 4},
        "targets": [
          {
            "expr": "sum(rate(nf_errors_total[5m])) by (error_type)",
            "legendFormat": "{{ error_type }}",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "ops",
            "logBase": 1,
            "min": 0
          }
        ]
      },
      {
        "id": 9,
        "title": "Database Connections",
        "type": "graph",
        "gridPos": {"x": 12, "y": 8, "w": 12, "h": 4},
        "targets": [
          {
            "expr": "nf_db_connections_active",
            "legendFormat": "Active Connections",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "short",
            "logBase": 1,
            "min": 0
          }
        ]
      }
    ]
  }
}
```

---

### 6.3 Application Metrics ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å›ºæœ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

**ä¸»è¦ãƒ‘ãƒãƒ«**:
- Runå®Ÿè¡Œçµ±è¨ˆï¼ˆæˆåŠŸ/å¤±æ•—/å®Ÿè¡Œä¸­ï¼‰
- ãƒ¢ãƒ‡ãƒ«åˆ¥å®Ÿè¡Œæ™‚é–“
- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ã‚¨ãƒ©ãƒ¼ç‡ãƒˆãƒ¬ãƒ³ãƒ‰
- ã‚­ãƒ¥ãƒ¼é•·æ¨ç§»

---

### 6.4 Model Performance ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç›£è¦–ã—ã¾ã™ã€‚

**ä¸»è¦ãƒ‘ãƒãƒ«**:
- ãƒ¢ãƒ‡ãƒ«åˆ¥ç²¾åº¦ï¼ˆsMAPE, RMSE, MAEï¼‰
- å­¦ç¿’æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰
- äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
- ãƒ¢ãƒ‡ãƒ«æ›´æ–°é »åº¦
- æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ

---

## 7. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

### 7.1 åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®æ¦‚è¦

#### 7.1.1 ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç›®çš„

| ç›®çš„ | èª¬æ˜ | ãƒ„ãƒ¼ãƒ« |
|-----|------|-------|
| **å‡¦ç†ãƒ•ãƒ­ãƒ¼å¯è¦–åŒ–** | ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æµã‚Œã‚’è¿½è·¡ | Jaeger |
| **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š** | é…ã„å‡¦ç†ã‚’ç™ºè¦‹ | Jaeger |
| **ä¾å­˜é–¢ä¿‚æŠŠæ¡** | ã‚µãƒ¼ãƒ“ã‚¹é–“ã®ä¾å­˜é–¢ä¿‚ | Jaeger |
| **ã‚¨ãƒ©ãƒ¼è¿½è·¡** | ã‚¨ãƒ©ãƒ¼ã®ç™ºç”Ÿç®‡æ‰€ç‰¹å®š | Jaeger |

---

### 7.2 OpenTelemetryçµ±åˆ

#### 7.2.1 ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°è¨­å®š

```python
# tracing.py

from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import SERVICE_NAME, Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor
from typing import Optional
import os


class TracingManager:
    """ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç®¡ç†"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """Singleton ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        if not self._initialized:
            self._setup_tracing()
            self._initialized = True
    
    def _setup_tracing(self):
        """ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # ãƒªã‚½ãƒ¼ã‚¹è¨­å®š
        resource = Resource(attributes={
            SERVICE_NAME: "nf-auto-runner"
        })
        
        # TracerProviderè¨­å®š
        provider = TracerProvider(resource=resource)
        
        # Jaeger Exporterè¨­å®š
        jaeger_host = os.getenv("JAEGER_HOST", "localhost")
        jaeger_port = int(os.getenv("JAEGER_PORT", "6831"))
        
        jaeger_exporter = JaegerExporter(
            agent_host_name=jaeger_host,
            agent_port=jaeger_port,
        )
        
        # SpanProcessorè¿½åŠ 
        provider.add_span_processor(
            BatchSpanProcessor(jaeger_exporter)
        )
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«TracerProviderã‚’è¨­å®š
        trace.set_tracer_provider(provider)
        
        # è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ«ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        RequestsInstrumentor().instrument()
        SQLAlchemyInstrumentor().instrument()
    
    @staticmethod
    def get_tracer(name: str) -> trace.Tracer:
        """
        Tracerã‚’å–å¾—
        
        Args:
            name: Tracerå
        
        Returns:
            trace.Tracer
        """
        return trace.get_tracer(name)


# ä½¿ç”¨ä¾‹
from opentelemetry import trace

# ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
tracing_manager = TracingManager()

# Tracerå–å¾—
tracer = tracing_manager.get_tracer('nf_auto_runner.training')

# ã‚¹ãƒ‘ãƒ³ä½œæˆ
with tracer.start_as_current_span('train_model') as span:
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å‡¦ç†
    span.set_attribute('model_name', 'AutoLSTM')
    span.set_attribute('backend', 'optuna')
    span.add_event('Training started')
    
    # ... å­¦ç¿’å‡¦ç† ...
    
    span.add_event('Training completed')
```

---

### 7.3 ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒ‘ãƒ³

#### 7.3.1 é–¢æ•°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼

```python
# trace_decorator.py

from opentelemetry import trace
from functools import wraps
from typing import Callable, Any
import time


def trace_function(
    name: Optional[str] = None,
    attributes: Optional[Dict[str, Any]] = None
):
    """
    é–¢æ•°ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    Args:
        name: ã‚¹ãƒ‘ãƒ³åï¼ˆNoneã®å ´åˆã¯é–¢æ•°åï¼‰
        attributes: ã‚¹ãƒ‘ãƒ³å±æ€§
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            span_name = name or func.__name__
            
            with tracer.start_as_current_span(span_name) as span:
                # å±æ€§ã‚’è¿½åŠ 
                if attributes:
                    for key, value in attributes.items():
                        span.set_attribute(key, value)
                
                # é–¢æ•°å¼•æ•°ã‚’è¨˜éŒ²
                span.set_attribute('function.args', str(args))
                span.set_attribute('function.kwargs', str(kwargs))
                
                # å®Ÿè¡Œé–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆ
                span.add_event(f'{span_name} started')
                
                start_time = time.time()
                
                try:
                    # é–¢æ•°å®Ÿè¡Œ
                    result = func(*args, **kwargs)
                    
                    # æˆåŠŸã‚¤ãƒ™ãƒ³ãƒˆ
                    span.add_event(f'{span_name} completed')
                    span.set_attribute('success', True)
                    
                    return result
                
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼è¨˜éŒ²
                    span.record_exception(e)
                    span.set_attribute('success', False)
                    span.set_attribute('error.type', type(e).__name__)
                    span.set_attribute('error.message', str(e))
                    
                    raise
                
                finally:
                    # å®Ÿè¡Œæ™‚é–“è¨˜éŒ²
                    duration = time.time() - start_time
                    span.set_attribute('duration_seconds', duration)
        
        return wrapper
    return decorator


# ä½¿ç”¨ä¾‹
@trace_function(
    name='train_neural_forecast_model',
    attributes={'component': 'training'}
)
def train_model(model_name: str, data: pd.DataFrame):
    """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
    # ... å­¦ç¿’å‡¦ç† ...
    pass
```

---

## 8. ç›£è¦–ãƒ„ãƒ¼ãƒ«è¨­å®š

### 8.1 Prometheusè¨­å®š

#### 8.1.1 Prometheusè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# prometheus/prometheus.yml

global:
  scrape_interval: 1m
  evaluation_interval: 1m
  external_labels:
    cluster: 'production'
    region: 'us-west-1'

# ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿
rule_files:
  - '/etc/prometheus/alerts/*.yml'

# Alert Managerè¨­å®š
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - 'alertmanager:9093'

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—è¨­å®š
scrape_configs:
  # Prometheusè‡ªèº«
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  # NF Runner ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
  - job_name: 'nf-runner'
    static_configs:
      - targets: ['nf-runner:9090']
    metric_relabel_configs:
      # é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é™¤å¤–
      - source_labels: [__name__]
        regex: 'nf_.*_bucket'
        action: drop
  
  # Node Exporter (ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
  
  # Postgres Exporter (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
  
  # cAdvisor (ã‚³ãƒ³ãƒ†ãƒŠãƒ¡ãƒˆãƒªã‚¯ã‚¹)
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
  
  # Blackbox Exporter (æ­»æ´»ç›£è¦–)
  - job_name: 'blackbox'
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - http://nf-runner:8000/health
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

# ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
storage:
  tsdb:
    path: /prometheus/data
    retention.time: 30d
    retention.size: 50GB
```

---

### 8.2 Grafanaè¨­å®š

#### 8.2.1 Grafanaè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```ini
# grafana/grafana.ini

[server]
protocol = http
http_port = 3000
domain = grafana.example.com
root_url = %(protocol)s://%(domain)s:%(http_port)s/
serve_from_sub_path = false

[database]
type = postgres
host = postgres:5432
name = grafana
user = grafana
password = ${GRAFANA_DB_PASSWORD}
ssl_mode = require

[auth]
disable_login_form = false
disable_signout_menu = false

[auth.anonymous]
enabled = false

[security]
admin_user = admin
admin_password = ${GRAFANA_ADMIN_PASSWORD}
secret_key = ${GRAFANA_SECRET_KEY}

[users]
allow_sign_up = false
allow_org_create = false
auto_assign_org = true
auto_assign_org_role = Viewer

[snapshots]
external_enabled = false

[alerting]
enabled = true
execute_alerts = true

[unified_alerting]
enabled = true

[log]
mode = console file
level = info
```

---

#### 8.2.2 Grafanaãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š

```yaml
# grafana/provisioning/datasources/datasource.yml

apiVersion: 1

datasources:
  # Prometheus
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      httpMethod: POST
      timeInterval: 1m
  
  # Loki
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: false
    jsonData:
      maxLines: 1000
  
  # Jaeger
  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger:16686
    editable: false
  
  # PostgreSQL
  - name: PostgreSQL
    type: postgres
    access: proxy
    url: postgres:5432
    database: ts_forecast_system
    user: grafana_readonly
    secureJsonData:
      password: ${POSTGRES_READONLY_PASSWORD}
    jsonData:
      sslmode: require
      maxOpenConns: 10
      maxIdleConns: 10
      connMaxLifetime: 14400
```

---

### 8.3 Lokiè¨­å®š

#### 8.3.1 Lokiè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# loki/loki.yml

auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

schema_config:
  configs:
    - from: 2023-01-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://alertmanager:9093

# ãƒ­ã‚°ã®ä¿æŒæœŸé–“
limits_config:
  retention_period: 90d
  max_streams_per_user: 0
  max_query_length: 721h
  max_query_parallelism: 16
```

---

### 8.4 Docker Composeè¨­å®š

#### 8.4.1 ç›£è¦–ã‚¹ã‚¿ãƒƒã‚¯Docker Compose

```yaml
# docker-compose.monitoring.yml

version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    networks:
      - monitoring
    restart: unless-stopped
  
  # Alert Manager
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    ports:
      - "9093:9093"
    networks:
      - monitoring
    restart: unless-stopped
  
  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - ./monitoring/grafana:/etc/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    ports:
      - "3000:3000"
    networks:
      - monitoring
    restart: unless-stopped
  
  # Loki
  loki:
    image: grafana/loki:latest
    container_name: loki
    volumes:
      - ./monitoring/loki:/etc/loki
      - loki_data:/loki
    command: -config.file=/etc/loki/loki.yml
    ports:
      - "3100:3100"
    networks:
      - monitoring
    restart: unless-stopped
  
  # Promtail
  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    volumes:
      - ./monitoring/promtail:/etc/promtail
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/promtail.yml
    networks:
      - monitoring
    restart: unless-stopped
  
  # Jaeger
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    networks:
      - monitoring
    restart: unless-stopped
  
  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    command:
      - '--path.rootfs=/host'
    volumes:
      - '/:/host:ro,rslave'
    ports:
      - "9100:9100"
    networks:
      - monitoring
    restart: unless-stopped
  
  # Postgres Exporter
  postgres-exporter:
    image: wrouesnel/postgres_exporter:latest
    container_name: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://exporter:${POSTGRES_EXPORTER_PASSWORD}@postgres:5432/ts_forecast_system?sslmode=require
    ports:
      - "9187:9187"
    networks:
      - monitoring
    restart: unless-stopped
  
  # cAdvisor
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    ports:
      - "8080:8080"
    networks:
      - monitoring
    restart: unless-stopped

networks:
  monitoring:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
  loki_data:
```

---

## 9. é‹ç”¨ç›£è¦–æ‰‹é †

### 9.1 æ—¥æ¬¡ç›£è¦–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### 9.1.1 æœã®ãƒã‚§ãƒƒã‚¯ï¼ˆæ¯æ—¥9:00ï¼‰

```markdown
## æ—¥æ¬¡ç›£è¦–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹
- [ ] Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèª
  - System Overview: ç¨¼åƒç‡ã€ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
  - Active Runs: å®Ÿè¡Œä¸­Runæ•°ãŒæ­£å¸¸ç¯„å›²å†…ã‹
  - Error Rate: éå»24æ™‚é–“ã®ã‚¨ãƒ©ãƒ¼ç‡ <10%

### ã‚¢ãƒ©ãƒ¼ãƒˆç¢ºèª
- [ ] Alert Managerç¢ºèª
  - æœªè§£æ±ºã®Criticalã‚¢ãƒ©ãƒ¼ãƒˆãŒãªã„ã‹
  - ç¹°ã‚Šè¿”ã—ç™ºç”Ÿã—ã¦ã„ã‚‹ã‚¢ãƒ©ãƒ¼ãƒˆãŒãªã„ã‹

### ãƒ­ã‚°ç¢ºèª
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ãƒ¬ãƒ“ãƒ¥ãƒ¼
  - CRITICALãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ãŒãªã„ã‹
  - ERRORé »åº¦ã®ç•°å¸¸ãªå¢—åŠ ãŒãªã„ã‹

### ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
- [ ] ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹
  - CPUä½¿ç”¨ç‡ <80%
  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ <80%
  - ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡ <85%
  - GPUä½¿ç”¨ç‡ï¼ˆGPUãŒã‚ã‚‹å ´åˆï¼‰

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¢ºèª
- [ ] PostgreSQLçŠ¶æ…‹
  - æ¥ç¶šæ•°ãŒæ­£å¸¸ç¯„å›²å†…ã‹
  - ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åŠ£åŒ–ãŒãªã„ã‹
  - ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒååˆ†ã‹

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèª
- [ ] å‰æ—¥ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒæˆåŠŸã—ãŸã‹
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºãŒæ­£å¸¸ã‹

### ãƒ“ã‚¸ãƒã‚¹KPI
- [ ] å‰æ—¥ã®Runå®Ÿè¡Œæ•°ãŒé€šå¸¸ç¯„å›²å†…ã‹
- [ ] å‰æ—¥ã®æˆåŠŸç‡ãŒ95%ä»¥ä¸Šã‹
```

---

### 9.2 é€±æ¬¡ç›£è¦–ã‚¿ã‚¹ã‚¯

#### 9.2.1 é€±æ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæ¯é€±æœˆæ›œ10:00ï¼‰

```markdown
## é€±æ¬¡ç›£è¦–ã‚¿ã‚¹ã‚¯

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- [ ] éå»1é€±é–“ã®å­¦ç¿’æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºèª
- [ ] éå»1é€±é–“ã®äºˆæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºèª
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã®ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯ç‰¹å®š

### ã‚¨ãƒ©ãƒ¼åˆ†æ
- [ ] é€±æ¬¡ã‚¨ãƒ©ãƒ¼çµ±è¨ˆãƒ¬ãƒ“ãƒ¥ãƒ¼
  - ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
  - ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
  - å†ç™ºé˜²æ­¢ç­–æ¤œè¨

### ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- [ ] é€±æ¬¡å¹³å‡ç²¾åº¦ã®ç¢ºèª
- [ ] ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®å…†å€™æ¤œå‡º

### ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨å‚¾å‘ã®åˆ†æ
- [ ] ä»Šå¾Œ2é€±é–“ã®éœ€è¦äºˆæ¸¬
- [ ] ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¿…è¦æ€§ã®è©•ä¾¡

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- [ ] ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ç•°å¸¸ãªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
- [ ] è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³çµæœç¢ºèª

### ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹
- [ ] ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹VACUUMå®Ÿè¡Œç¢ºèª
- [ ] å¤ã„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
- [ ] é‹ç”¨ãƒ­ã‚°ã®æ›´æ–°
- [ ] ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆï¼ˆå¿…è¦ãªå ´åˆï¼‰
- [ ] ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®æ›´æ–°
```

---

### 9.3 æœˆæ¬¡ç›£è¦–ã‚¿ã‚¹ã‚¯

#### 9.3.1 æœˆæ¬¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæ¯æœˆ1æ—¥ï¼‰

```markdown
## æœˆæ¬¡ç›£è¦–ã‚¿ã‚¹ã‚¯

### SLO/SLI ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] æœˆé–“ç¨¼åƒç‡ã®è¨ˆç®—ã¨è©•ä¾¡
- [ ] SLOé”æˆçŠ¶æ³ã®ç¢ºèª
- [ ] SLOæœªé”æˆã®å ´åˆã®åŸå› åˆ†æ

### å®¹é‡ç®¡ç†
- [ ] ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºå¢—åŠ ç‡ç¢ºèª
- [ ] ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ‹¡å¼µè¨ˆç”»ã®è¦‹ç›´ã—

### ã‚³ã‚¹ãƒˆåˆ†æ
- [ ] æœˆæ¬¡é‹ç”¨ã‚³ã‚¹ãƒˆã®é›†è¨ˆ
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨åŠ¹ç‡ã®è©•ä¾¡
- [ ] ã‚³ã‚¹ãƒˆæœ€é©åŒ–æ©Ÿä¼šã®ç‰¹å®š

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§ãƒ†ã‚¹ãƒˆ
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œå…¨æ€§ã®æ¤œè¨¼
- [ ] ãƒªã‚«ãƒãƒªæ‰‹é †ã®è¦‹ç›´ã—

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
- [ ] æœˆæ¬¡ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ­ã‚°ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®æ£šå¸ã—
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒé©ç”¨çŠ¶æ³ç¢ºèª

### ä¾å­˜é–¢ä¿‚æ›´æ–°
- [ ] Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯
- [ ] é‡è¦ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã®é©ç”¨
- [ ] ä¾å­˜é–¢ä¿‚ã®æœ€æ–°åŒ–

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- [ ] ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
- [ ] ã‚¯ã‚¨ãƒªæœ€é©åŒ–æ©Ÿä¼šã®ç‰¹å®š
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®è¦‹ç›´ã—

### ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- [ ] æœˆæ¬¡é‹ç”¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
- [ ] çµŒå–¶å±¤å‘ã‘ã‚µãƒãƒªãƒ¼ä½œæˆ
- [ ] æ”¹å–„ææ¡ˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–
```

---

## 10. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 10.1 ä¸€èˆ¬çš„ãªå•é¡Œã¨å¯¾å‡¦æ³•

#### 10.1.1 ã‚·ã‚¹ãƒ†ãƒ ãƒ€ã‚¦ãƒ³

**ç—‡çŠ¶**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿œç­”ã—ãªã„

**è¨ºæ–­æ‰‹é †**:
1. ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
```bash
ps aux | grep nf-runner
systemctl status nf-runner
```

2. ãƒ­ã‚°ç¢ºèª
```bash
tail -100 /var/log/nf-runner/error.log
journalctl -u nf-runner -n 100
```

3. ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
```bash
top
free -h
df -h
```

**å¯¾å‡¦æ³•**:
```bash
# 1. ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•
systemctl restart nf-runner

# 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
cat /etc/nf-runner/config.yml

# 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
psql -U postgres -d ts_forecast_system -c "SELECT 1"

# 4. ä¾å­˜ã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
systemctl status postgresql
systemctl status prometheus
```

---

#### 10.1.2 é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡

**ç—‡çŠ¶**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ90%ä»¥ä¸Š

**è¨ºæ–­æ‰‹é †**:
1. ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ç¢ºèª
```bash
free -h
ps aux --sort=-%mem | head -20
```

2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒ¢ãƒªç¢ºèª
```python
import psutil

process = psutil.Process()
print(f"Memory: {process.memory_info().rss / 1024 / 1024:.2f} MB")
```

3. Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
```promql
process_resident_memory_bytes{job="nf-runner"}
```

**å¯¾å‡¦æ³•**:
```bash
# 1. ä¸è¦ãªãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†
pkill -f "old_process"

# 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å†èµ·å‹•
systemctl restart nf-runner

# 3. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯èª¿æŸ»
# ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
python -m memory_profiler script.py

# 4. ã‚¹ãƒ¯ãƒƒãƒ—ä½¿ç”¨çŠ¶æ³ç¢ºèª
swapon --show
```

---

#### 10.1.3 é«˜ã‚¨ãƒ©ãƒ¼ç‡

**ç—‡çŠ¶**: ã‚¨ãƒ©ãƒ¼ç‡ãŒ10%ã‚’è¶…ãˆã‚‹

**è¨ºæ–­æ‰‹é †**:
1. ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç¢ºèª
```bash
grep ERROR /var/log/nf-runner/app.log | tail -100
```

2. ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
```bash
grep ERROR /var/log/nf-runner/app.log | \
  jq -r '.error_type' | \
  sort | uniq -c | sort -rn
```

3. Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
```promql
sum(rate(nf_errors_total[5m])) by (error_type)
```

**å¯¾å‡¦æ³•**:
```bash
# 1. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
python -m nf_auto_runner.utils.data_validator

# 2. è¨­å®šç¢ºèª
python -m nf_auto_runner.utils.config_validator

# 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
psql -U postgres -d ts_forecast_system -c "SELECT version()"

# 4. è©³ç´°ãƒ­ã‚°æœ‰åŠ¹åŒ–
# config.ymlã§log_levelã‚’DEBUGã«å¤‰æ›´
```

---

#### 10.1.4 é…ã„å®Ÿè¡Œæ™‚é–“

**ç—‡çŠ¶**: å­¦ç¿’æ™‚é–“ãŒé€šå¸¸ã®2å€ä»¥ä¸Š

**è¨ºæ–­æ‰‹é †**:
1. CPU/GPUä½¿ç”¨ç‡ç¢ºèª
```bash
top
nvidia-smi  # GPUãŒã‚ã‚‹å ´åˆ
```

2. I/Oå¾…ã¡ç¢ºèª
```bash
iostat -x 1
```

3. ãƒˆãƒ¬ãƒ¼ã‚¹ç¢ºèªï¼ˆJaegerï¼‰
- Jaeger UIã§é…ã„ã‚¹ãƒ‘ãƒ³ã‚’ç‰¹å®š

4. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
```python
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()

# ... å‡¦ç† ...

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)
```

**å¯¾å‡¦æ³•**:
```bash
# 1. ä¸¦åˆ—å®Ÿè¡Œæ•°èª¿æ•´
# config.ymlã§max_parallel_runsã‚’èª¿æ•´

# 2. ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¢ºèª
SELECT COUNT(*) FROM time_series_data;

# 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª
\d+ time_series_data

# 4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
python -m nf_auto_runner.utils.cache_cleaner
```

---

### 10.2 ã‚¢ãƒ©ãƒ¼ãƒˆå¯¾å¿œãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
graph TD
    A[ã‚¢ãƒ©ãƒ¼ãƒˆå—ä¿¡] --> B{é‡è¦åº¦ã¯?}
    
    B -->|Critical| C[å³åº§ã«å¯¾å¿œé–‹å§‹]
    B -->|High| D[2æ™‚é–“ä»¥å†…ã«å¯¾å¿œ]
    B -->|Medium| E[ç¿Œå–¶æ¥­æ—¥ã«ç¢ºèª]
    
    C --> F[ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèª]
    D --> F
    E --> F
    
    F --> G[ãƒ­ã‚°ç¢ºèª]
    G --> H[ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ]
    H --> I{åŸå› ç‰¹å®š?}
    
    I -->|Yes| J[å¯¾å‡¦å®Ÿæ–½]
    I -->|No| K[ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    
    J --> L{è§£æ±º?}
    L -->|Yes| M[ã‚¢ãƒ©ãƒ¼ãƒˆè§£é™¤]
    L -->|No| K
    
    K --> N[ä¸Šä½è€…ã«é€£çµ¡]
    N --> O[è©³ç´°èª¿æŸ»]
    O --> J
    
    M --> P[äº‹å¾Œãƒ¬ãƒãƒ¼ãƒˆä½œæˆ]
    P --> Q[ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ›´æ–°]
    Q --> R[å®Œäº†]
```

---

### 10.3 ãƒ‡ãƒãƒƒã‚°ã‚³ãƒãƒ³ãƒ‰é›†

#### 10.3.1 ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­

```bash
#!/bin/bash
# system_diagnostic.sh

echo "=== System Diagnostic ==="
echo ""

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
echo "--- System Information ---"
uname -a
uptime

# CPUæƒ…å ±
echo ""
echo "--- CPU Usage ---"
top -bn1 | grep "Cpu(s)"

# ãƒ¡ãƒ¢ãƒªæƒ…å ±
echo ""
echo "--- Memory Usage ---"
free -h

# ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±
echo ""
echo "--- Disk Usage ---"
df -h

# ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
echo ""
echo "--- NF Runner Process ---"
ps aux | grep nf-runner | grep -v grep

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æƒ…å ±
echo ""
echo "--- Network Connections ---"
netstat -tuln | grep -E ':(8000|9090|5432)'

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
echo ""
echo "--- Database Connection ---"
psql -U postgres -d ts_forecast_system -c "SELECT version();"

# Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹
echo ""
echo "--- Prometheus Metrics ---"
curl -s http://localhost:9090/api/v1/query?query=up | jq '.data.result'

echo ""
echo "=== Diagnostic Complete ==="
```

---

## 11. ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 11.1 ç›£è¦–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### 11.1.1 The Four Golden Signals

å¸¸ã«ä»¥ä¸‹ã®4ã¤ã®æŒ‡æ¨™ã‚’ç›£è¦–:

1. **Latency** (ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·)
   - å­¦ç¿’æ™‚é–“ã€äºˆæ¸¬æ™‚é–“ã®åˆ†å¸ƒï¼ˆp50, p95, p99ï¼‰
   - ç›®æ¨™: p95 < 10ç§’

2. **Traffic** (ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯)
   - å®Ÿè¡Œä¸­Runæ•°ã€API QPS
   - ç›®æ¨™: å®‰å®šã—ãŸå‡¦ç†é‡

3. **Errors** (ã‚¨ãƒ©ãƒ¼)
   - å¤±æ•—Runç‡
   - ç›®æ¨™: <10%

4. **Saturation** (é£½å’Œåº¦)
   - CPU/ãƒ¡ãƒ¢ãƒª/ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
   - ç›®æ¨™: <80%

---

#### 11.1.2 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆåŸå‰‡

**Good Alert**:
- âœ… å®Ÿéš›ã®å•é¡Œã‚’ç¤ºã™
- âœ… å¯¾å¿œå¯èƒ½ã§ã‚ã‚‹
- âœ… ç·Šæ€¥æ€§ãŒæ˜ç¢º
- âœ… å¯¾å‡¦æ³•ãŒæ˜ç¢º

**Bad Alert**:
- âŒ ãƒã‚¤ã‚ºãŒå¤šã„
- âŒ å¯¾å¿œä¸å¯èƒ½
- âŒ èª¤æ¤œçŸ¥ãŒå¤šã„
- âŒ å¯¾å‡¦æ³•ãŒä¸æ˜

---

#### 11.1.3 ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆåŸå‰‡

1. **éšå±¤åŒ–**
   - Overview â†’ Detail â†’ Debug
   - ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã¯å…¨ä½“åƒã‚’1ç”»é¢ã§

2. **æ¨™æº–åŒ–**
   - åŒã˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯åŒã˜è‰²
   - é–¾å€¤ã¯æ˜ç¤ºçš„ã«è¡¨ç¤º

3. **ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯èƒ½æ€§**
   - å•é¡Œç™ºè¦‹ â†’ åŸå› ç‰¹å®š â†’ å¯¾å‡¦
   - é–¢é€£ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¸ã®ãƒªãƒ³ã‚¯

---

### 11.2 ãƒ­ã‚®ãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### 11.2.1 ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®ä½¿ã„åˆ†ã‘

```python
# è‰¯ã„ä¾‹
logger.debug(f"Processing item {item_id}")  # é–‹ç™ºæ™‚ã®ã¿
logger.info(f"Model training started: {model_name}")  # æ­£å¸¸å‹•ä½œ
logger.warning(f"Memory usage high: {memory_percent}%")  # è­¦å‘Š
logger.error(f"Model training failed: {error}")  # ã‚¨ãƒ©ãƒ¼
logger.critical(f"Database connection lost")  # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«

# æ‚ªã„ä¾‹
logger.info("Error occurred")  # ã‚¨ãƒ©ãƒ¼ãªã®ã«INFO
logger.error("Processing started")  # æ­£å¸¸å‹•ä½œãªã®ã«ERROR
```

---

#### 11.2.2 æ§‹é€ åŒ–ãƒ­ã‚®ãƒ³ã‚°

```python
# è‰¯ã„ä¾‹: æ§‹é€ åŒ–
logger.info(
    'model_training_completed',
    run_id='abc123',
    model_name='AutoLSTM',
    duration_seconds=245.3,
    accuracy=0.92
)

# æ‚ªã„ä¾‹: éæ§‹é€ åŒ–
logger.info(
    f"Training completed for AutoLSTM in 245.3 seconds with accuracy 0.92"
)
```

---

### 11.3 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### 11.3.1 ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‘½åè¦å‰‡

```python
# è‰¯ã„ä¾‹
training_duration_seconds  # å˜ä½ãŒæ˜ç¢º
http_requests_total  # totalã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹
memory_usage_bytes  # å˜ä½ãŒæ˜ç¢º

# æ‚ªã„ä¾‹
training_time  # å˜ä½ä¸æ˜
requests  # ç´¯ç©ã‹ç¾åœ¨å€¤ã‹ä¸æ˜
memory  # å˜ä½ä¸æ˜
```

---

#### 11.3.2 ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ç®¡ç†

```python
# è‰¯ã„ä¾‹: ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£
model_type = Gauge(
    'model_type',
    'Model type',
    ['model_family']  # 'AutoLSTM', 'AutoNHITS' ãªã©æ•°ç¨®é¡
)

# æ‚ªã„ä¾‹: é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£
model_id = Gauge(
    'model_id',
    'Model ID',
    ['run_id']  # ç„¡é™ã«å¢—ãˆã‚‹å¯èƒ½æ€§
)
```

---

## 12. ä»˜éŒ²

### 12.1 ç”¨èªé›†

| ç”¨èª | èª¬æ˜ |
|-----|------|
| **SLO** | Service Level Objective - ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ç›®æ¨™ |
| **SLI** | Service Level Indicator - ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«æŒ‡æ¨™ |
| **MTBF** | Mean Time Between Failures - å¹³å‡æ•…éšœé–“éš” |
| **MTTR** | Mean Time To Repair - å¹³å‡ä¿®å¾©æ™‚é–“ |
| **MTTD** | Mean Time To Detect - å¹³å‡æ¤œå‡ºæ™‚é–“ |
| **RTO** | Recovery Time Objective - ç›®æ¨™å¾©æ—§æ™‚é–“ |
| **RPO** | Recovery Point Objective - ç›®æ¨™å¾©æ—§æ™‚ç‚¹ |
| **p50/p95/p99** | ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼ˆä¸­å¤®å€¤/95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«/99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰ |

---

### 12.2 å‚è€ƒè³‡æ–™

#### 12.2.1 å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Loki Documentation](https://grafana.com/docs/loki/)
- [Jaeger Documentation](https://www.jaegertracing.io/docs/)
- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)

---

#### 12.2.2 æ›¸ç±

- **Site Reliability Engineering** - Google
- **The Art of Monitoring** - James Turnbull
- **Distributed Systems Observability** - Cindy Sridharan

---

#### 12.2.3 é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- ğŸ“„ `00_INTEGRATED_DESIGN_OVERVIEW.md` - çµ±åˆè¨­è¨ˆæ¦‚è¦
- ğŸ“„ `02_NON_FUNCTIONAL_REQUIREMENTS.md` - éæ©Ÿèƒ½è¦ä»¶
- ğŸ“„ `03_ARCHITECTURE_DESIGN_DETAILED.md` - ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
- ğŸ“„ `09_TESTING_STRATEGY.md` - ãƒ†ã‚¹ãƒˆæˆ¦ç•¥
- ğŸ“„ `10_DEPLOYMENT_GUIDE.md` - ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰
- ğŸ“„ `11_OPERATIONS_RUNBOOK.md` - é‹ç”¨æ‰‹é †æ›¸

---

### 12.3 ç›£è¦–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### 12.3.1 åˆæœŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
## ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### Prometheus
- [ ] Prometheusè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
- [ ] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—å¯¾è±¡ã®è¨­å®š
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ã®ä½œæˆ
- [ ] ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä¿æŒæœŸé–“ã®è¨­å®š
- [ ] Alert Manageré€£æºè¨­å®š

### Grafana
- [ ] Grafanaè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨©é™ã®è¨­å®š
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ã®è¨­å®š

### Alert Manager
- [ ] Alert Managerè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
- [ ] å—ä¿¡è€…ã®è¨­å®š
- [ ] é€šçŸ¥ãƒãƒ£ãƒãƒ«ã®è¨­å®šï¼ˆEmail, Slackï¼‰
- [ ] ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã®è¨­å®š
- [ ] æŠ‘åˆ¶ãƒ«ãƒ¼ãƒ«ã®è¨­å®š

### Loki
- [ ] Lokiè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
- [ ] ãƒ­ã‚°ä¿æŒæœŸé–“ã®è¨­å®š
- [ ] Promtailè¨­å®š
- [ ] ãƒ­ã‚°ãƒ©ãƒ™ãƒ«ã®è¨­å®š

### Application Instrumentation
- [ ] Prometheusã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®çµ±åˆ
- [ ] æ§‹é€ åŒ–ãƒ­ã‚®ãƒ³ã‚°ã®å®Ÿè£…
- [ ] ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®å®Ÿè£…
- [ ] ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®èµ·å‹•

### Testing
- [ ] ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®å‹•ä½œç¢ºèª
- [ ] ãƒ­ã‚°è»¢é€ã®å‹•ä½œç¢ºèª
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«ã®ãƒ†ã‚¹ãƒˆ
- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºã®ç¢ºèª

### Documentation
- [ ] ç›£è¦–ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
- [ ] Runbookã®ä½œæˆ
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆå¯¾å¿œæ‰‹é †ã®ä½œæˆ
- [ ] ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰ã®ä½œæˆ
```

---

## âœ¨ ã¾ã¨ã‚

æœ¬ç›£è¦–ã‚¬ã‚¤ãƒ‰ã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãªç›£è¦–æˆ¦ç•¥ã‚’æä¾›ã—ã¾ã™ã€‚

### ç›£è¦–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ

- âœ… **3ã¤ã®æŸ±**: Metrics, Logs, Traces ã®çµ±åˆ
- âœ… **4ã¤ã®Golden Signals**: Latency, Traffic, Errors, Saturation
- âœ… **éšå±¤çš„ç›£è¦–**: ã‚¤ãƒ³ãƒ•ãƒ© â†’ ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  â†’ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ â†’ ãƒ“ã‚¸ãƒã‚¹
- âœ… **ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå¯¾å¿œ**: äºˆé˜²çš„ãªç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ
- âœ… **è‡ªå‹•åŒ–**: ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»å¯¾å¿œã®è‡ªå‹•åŒ–

### ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®æˆåŠŸè¦å› 

1. **æ˜ç¢ºãªSLO/SLI**: æ¸¬å®šå¯èƒ½ãªç›®æ¨™è¨­å®š
2. **é©åˆ‡ãªã‚¢ãƒ©ãƒ¼ãƒˆ**: ãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã—ã€å®Ÿéš›ã®å•é¡Œã«é›†ä¸­
3. **åŒ…æ‹¬çš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: å…¨ä½“åƒã‹ã‚‰è©³ç´°ã¾ã§
4. **åŠ¹ç‡çš„ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**: è¿…é€ŸãªåŸå› ç‰¹å®šã¨å¯¾å¿œ
5. **ç¶™ç¶šçš„ãªæ”¹å–„**: ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ã®æ”¹å–„

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… **æœ¬ç›£è¦–ã‚¬ã‚¤ãƒ‰ã‚’èª­äº†**
2. ğŸ”§ ç›£è¦–ã‚¹ã‚¿ãƒƒã‚¯ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
   - Prometheus, Grafana, Loki, Jaeger
3. ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ
   - System Overview, Application Metrics ãªã©
4. ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆã®è¨­å®š
   - Critical, High, Medium ã‚¢ãƒ©ãƒ¼ãƒˆ
5. ğŸ“ é‹ç”¨æ‰‹é †ã®ç¢ºç«‹
   - æ—¥æ¬¡/é€±æ¬¡/æœˆæ¬¡ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
6. ğŸ”„ ç¶™ç¶šçš„ãªæ”¹å–„
   - ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ ã€ã‚¢ãƒ©ãƒ¼ãƒˆèª¿æ•´ã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ”¹å–„

---

**Observe, Alert, Act - Build Reliable Systems! ğŸ“ŠğŸš¨ğŸ”§**

---
**End of Document**
