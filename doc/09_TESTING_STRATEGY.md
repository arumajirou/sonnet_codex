# ãƒ†ã‚¹ãƒˆæˆ¦ç•¥
**Testing Strategy for Time Series Forecasting System**

---

## ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±

| é …ç›® | å†…å®¹ |
|-----|------|
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«** | æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** | v1.0.0 |
| **ä½œæˆæ—¥** | 2025-11-03 |
| **æœ€çµ‚æ›´æ–°æ—¥** | 2025-11-03 |
| **å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ ** | NeuralForecast Auto Runner + Time Series Forecasting System |
| **å¯¾è±¡èª­è€…** | é–‹ç™ºè€…ã€QAã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ |

---

## ç›®æ¬¡

1. [ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æ¦‚è¦](#1-ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æ¦‚è¦)
2. [ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã¨ã‚¹ã‚³ãƒ¼ãƒ—](#2-ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã¨ã‚¹ã‚³ãƒ¼ãƒ—)
3. [ãƒ†ã‚¹ãƒˆæ‰‹æ³•ã¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ](#3-ãƒ†ã‚¹ãƒˆæ‰‹æ³•ã¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)
4. [ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™](#4-ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™)
5. [TDDå®Ÿè·µã‚¬ã‚¤ãƒ‰](#5-tddå®Ÿè·µã‚¬ã‚¤ãƒ‰)
6. [ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ](#6-ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ)
7. [ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†](#7-ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†)
8. [ãƒ†ã‚¹ãƒˆç’°å¢ƒæ§‹æˆ](#8-ãƒ†ã‚¹ãƒˆç’°å¢ƒæ§‹æˆ)
9. [ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–](#9-ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–)
10. [å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨KPI](#10-å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨kpi)
11. [ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆ¦ç•¥](#11-ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆ¦ç•¥)
12. [CI/CDçµ±åˆ](#12-cicdçµ±åˆ)
13. [ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯](#13-ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯)
14. [ä»˜éŒ²](#14-ä»˜éŒ²)

---

## 1. ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æ¦‚è¦

### 1.1 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®ç›®çš„

æœ¬ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã‚’ä¿è¨¼ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¾ã™ï¼š

#### ä¸»è¦ç›®æ¨™

1. **é«˜å“è³ªã®ä¿è¨¼**: å…¨æ©Ÿèƒ½ãŒä»•æ§˜é€šã‚Šã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ä¿è¨¼
2. **æ—©æœŸæ¬ é™¥æ¤œå‡º**: é–‹ç™ºåˆæœŸæ®µéšã§æ¬ é™¥ã‚’ç™ºè¦‹ã—ã€ä¿®æ­£ã‚³ã‚¹ãƒˆã‚’æœ€å°åŒ–
3. **å†ç¾æ€§ã®ç¢ºä¿**: ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒä¸€è²«ã—ã¦å†ç¾å¯èƒ½
4. **ç¶™ç¶šçš„å“è³ªæ”¹å–„**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãç¶™ç¶šçš„ãªå“è³ªå‘ä¸Š
5. **ãƒªã‚¹ã‚¯ä½æ¸›**: æœ¬ç•ªç’°å¢ƒã§ã®ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–

---

### 1.2 ãƒ†ã‚¹ãƒˆå“²å­¦

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå“²å­¦ã¯ä»¥ä¸‹ã®åŸå‰‡ã«åŸºã¥ãã¾ã™ï¼š

| åŸå‰‡ | èª¬æ˜ | å®Ÿè·µ |
|-----|------|------|
| **Test First** | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆé–‹ç™ºï¼ˆTDDï¼‰ | ãƒ†ã‚¹ãƒˆã‚’å…ˆã«æ›¸ã„ã¦ã‹ã‚‰å®Ÿè£… |
| **Fail Fast** | æ—©æœŸå¤±æ•—æ¤œå‡º | æ¬ é™¥ã‚’æ—©æœŸã«ç™ºè¦‹ã—ã¦ä¿®æ­£ |
| **Shift Left** | ãƒ†ã‚¹ãƒˆã®å·¦ã‚·ãƒ•ãƒˆ | é–‹ç™ºåˆæœŸã‹ã‚‰ãƒ†ã‚¹ãƒˆå®Ÿæ–½ |
| **Automation First** | è‡ªå‹•åŒ–å„ªå…ˆ | æ‰‹å‹•ãƒ†ã‚¹ãƒˆã¯æœ€å°é™ã« |
| **Quality Gates** | å“è³ªã‚²ãƒ¼ãƒˆ | å„ãƒ•ã‚§ãƒ¼ã‚ºã§å“è³ªåŸºæº–ã‚’ã‚¯ãƒªã‚¢ |
| **Continuous Testing** | ç¶™ç¶šçš„ãƒ†ã‚¹ãƒˆ | CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§è‡ªå‹•å®Ÿè¡Œ |

---

### 1.3 ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã«å¾“ã£ã¦ãƒ†ã‚¹ãƒˆã‚’æ§‹æˆã—ã¾ã™ï¼š

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   E2E Tests   â”‚  (5%)
                   â”‚  çµ±åˆæ€§ç¢ºèª   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Integration Tests â”‚  (15%)
               â”‚   ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ   â”‚
               â”‚     çµ±åˆç¢ºèª      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Unit Tests (UT)        â”‚  (80%)
         â”‚    å€‹åˆ¥æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª        â”‚
         â”‚  é«˜é€Ÿãƒ»ç‹¬ç«‹ãƒ»è±Šå¯Œãªã‚±ãƒ¼ã‚¹     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ¯”ç‡ã®æ ¹æ‹ **:
- **UT (80%)**: é«˜é€Ÿãƒ»ç‹¬ç«‹ãƒ»è©³ç´°ãªæ¤œè¨¼ãŒå¯èƒ½
- **IT (15%)**: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®é€£æºã‚’ç¢ºèª
- **E2E (5%)**: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å‹•ä½œã‚’ç¢ºèª

---

### 1.4 ãƒ†ã‚¹ãƒˆæˆç†Ÿåº¦ãƒ¢ãƒ‡ãƒ«

ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆæˆç†Ÿåº¦ã‚’æ®µéšçš„ã«å‘ä¸Šã•ã›ã¾ã™ï¼š

| ãƒ¬ãƒ™ãƒ« | æ®µéš | ç‰¹å¾´ | ç›®æ¨™æ™‚æœŸ |
|-------|------|------|---------|
| **Level 0** | æœªå®Ÿæ–½ | ãƒ†ã‚¹ãƒˆãªã— | - |
| **Level 1** | åˆæœŸ | æ‰‹å‹•ãƒ†ã‚¹ãƒˆä¸­å¿ƒ | Phase 1 |
| **Level 2** | ç®¡ç† | è‡ªå‹•åŒ–ãƒ†ã‚¹ãƒˆå°å…¥ | Phase 3 |
| **Level 3** | å®šç¾© | TDDå®Ÿè·µã€CI/CDçµ±åˆ | Phase 5 |
| **Level 4** | å®šé‡ç®¡ç† | ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã€å“è³ªã‚²ãƒ¼ãƒˆ | Phase 7 |
| **Level 5** | æœ€é©åŒ– | ç¶™ç¶šçš„æ”¹å–„ã€äºˆæ¸¬çš„å“è³ªç®¡ç† | Phase 9+ |

**ç¾åœ¨ã®ç›®æ¨™**: Level 4ï¼ˆå®šé‡ç®¡ç†ï¼‰ã‚’é”æˆ

---

## 2. ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ã¨ã‚¹ã‚³ãƒ¼ãƒ—

### 2.1 å˜ä½“ãƒ†ã‚¹ãƒˆï¼ˆUnit Testsï¼‰

#### 2.1.1 æ¦‚è¦

**ç›®çš„**: å€‹åˆ¥ã®ã‚¯ãƒ©ã‚¹ã€ãƒ¡ã‚½ãƒƒãƒ‰ã€é–¢æ•°ãŒä»•æ§˜é€šã‚Šã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼

**ç‰¹å¾´**:
- é«˜é€Ÿå®Ÿè¡Œï¼ˆ<1ç§’/ãƒ†ã‚¹ãƒˆï¼‰
- å¤–éƒ¨ä¾å­˜ãªã—ï¼ˆãƒ¢ãƒƒã‚¯ä½¿ç”¨ï¼‰
- ç‹¬ç«‹å®Ÿè¡Œå¯èƒ½
- è©³ç´°ãªã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹æ¤œè¨¼

---

#### 2.1.2 ã‚¹ã‚³ãƒ¼ãƒ—

| å±¤ | ãƒ†ã‚¹ãƒˆå¯¾è±¡ | ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ | ãƒ†ã‚¹ãƒˆæ•°ç›®å®‰ |
|---|----------|--------------|------------|
| **Configurationå±¤** | ConfigåŸºåº•ã‚¯ãƒ©ã‚¹ã€PathConfigã€ExecutionConfigç­‰ | >95% | 50+ |
| **Dataå±¤** | DataLoaderã€Preprocessorã€Validatorç­‰ | >95% | 80+ |
| **Model Discoveryå±¤** | ModelRegistryã€CapabilityAnalyzerç­‰ | >90% | 60+ |
| **Hyperparameterå±¤** | LossRegistryã€ScalerRegistryç­‰ | >90% | 50+ |
| **Execution Planå±¤** | CombinationGeneratorã€ExecutionPlanç­‰ | >90% | 40+ |
| **Executionå±¤** | Executorã€ParallelExecutorç­‰ | >85% | 70+ |
| **Artifactå±¤** | ArtifactManagerã€ModelSaverç­‰ | >90% | 40+ |
| **Loggingå±¤** | StructuredLoggerã€ProgressTrackerç­‰ | >90% | 30+ |
| **Serviceå±¤** | ExecutionServiceã€PlanningServiceç­‰ | >85% | 50+ |
| **Applicationå±¤** | CLIã€Orchestratorç­‰ | >85% | 30+ |

**åˆè¨ˆç›®æ¨™**: 500+ å˜ä½“ãƒ†ã‚¹ãƒˆ

---

#### 2.1.3 å˜ä½“ãƒ†ã‚¹ãƒˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

```python
# âœ… è‰¯ã„å˜ä½“ãƒ†ã‚¹ãƒˆã®ä¾‹
import pytest
from unittest.mock import Mock, patch
from nf_auto_runner.config import PathConfig


class TestPathConfig:
    """PathConfigã‚¯ãƒ©ã‚¹ã®å˜ä½“ãƒ†ã‚¹ãƒˆ"""
    
    def test_init_with_valid_paths(self):
        """æœ‰åŠ¹ãªãƒ‘ã‚¹ã§ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        data_dir = "/path/to/data"
        model_dir = "/path/to/models"
        
        # Act
        config = PathConfig(data_dir=data_dir, model_dir=model_dir)
        
        # Assert
        assert config.data_dir == data_dir
        assert config.model_dir == model_dir
        assert config.is_valid()
    
    def test_init_with_invalid_data_dir_raises_error(self):
        """ç„¡åŠ¹ãªdata_dirã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        # Arrange
        invalid_dir = ""
        
        # Act & Assert
        with pytest.raises(ValueError, match="data_dir cannot be empty"):
            PathConfig(data_dir=invalid_dir, model_dir="/valid/path")
    
    def test_validate_paths_with_nonexistent_dir(self, tmp_path):
        """å­˜åœ¨ã—ãªã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        nonexistent = tmp_path / "nonexistent"
        config = PathConfig(data_dir=str(nonexistent), model_dir="/tmp")
        
        # Act
        result = config.validate_paths()
        
        # Assert
        assert result is False
    
    @pytest.mark.parametrize("data_dir,model_dir,expected", [
        ("/valid/data", "/valid/model", True),
        ("/valid/data", "", False),
        ("", "/valid/model", False),
        ("", "", False),
    ])
    def test_is_valid_parametrized(self, data_dir, model_dir, expected):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆï¼šæ§˜ã€…ãªãƒ‘ã‚¹ã®çµ„ã¿åˆã‚ã›"""
        # Arrange & Act
        config = PathConfig(data_dir=data_dir, model_dir=model_dir)
        
        # Assert
        assert config.is_valid() == expected
```

---

#### 2.1.4 å˜ä½“ãƒ†ã‚¹ãƒˆã®ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# âŒ æ‚ªã„å˜ä½“ãƒ†ã‚¹ãƒˆã®ä¾‹

# ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³1: è¤‡æ•°ã®æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ
def test_everything():
    """è¤‡æ•°ã®ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã‚‹ï¼ˆåˆ†å‰²ã™ã¹ãï¼‰"""
    config = PathConfig(data_dir="/data", model_dir="/models")
    assert config.data_dir == "/data"  # åˆæœŸåŒ–ã®ãƒ†ã‚¹ãƒˆ
    assert config.is_valid()  # æ¤œè¨¼ã®ãƒ†ã‚¹ãƒˆ
    config.update("/new/data")  # æ›´æ–°ã®ãƒ†ã‚¹ãƒˆ
    assert config.data_dir == "/new/data"  # æ›´æ–°ç¢ºèªã®ãƒ†ã‚¹ãƒˆ


# ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³2: å¤–éƒ¨ä¾å­˜
def test_with_real_database():
    """å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¾å­˜ï¼ˆãƒ¢ãƒƒã‚¯ã‚’ä½¿ã†ã¹ãï¼‰"""
    db = PostgreSQLDatabase("postgresql://...")  # å®ŸDBã«æ¥ç¶š
    result = db.query("SELECT * FROM experiments")
    assert len(result) > 0


# ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³3: ä¸æ˜ç¢ºãªã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
def test_calculation():
    """ä½•ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã‚‹ã‹ä¸æ˜ç¢º"""
    result = calculate_something(10, 20)
    assert result  # ä½•ã‚’ç¢ºèªã—ã¦ã„ã‚‹ã®ã‹ä¸æ˜


# ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ†ã‚¹ãƒˆé–“ã®ä¾å­˜
test_order = []

def test_first():
    """ä»–ã®ãƒ†ã‚¹ãƒˆã«ä¾å­˜ï¼ˆç‹¬ç«‹ã™ã¹ãï¼‰"""
    test_order.append(1)
    assert True

def test_second():
    """test_firstã®å®Ÿè¡Œã‚’å‰æã¨ã—ã¦ã„ã‚‹"""
    assert 1 in test_order  # å±é™ºï¼
```

---

### 2.2 çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆIntegration Testsï¼‰

#### 2.2.1 æ¦‚è¦

**ç›®çš„**: è¤‡æ•°ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®é€£æºãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼

**ç‰¹å¾´**:
- ä¸­é€Ÿå®Ÿè¡Œï¼ˆæ•°ç§’ï½æ•°åç§’ï¼‰
- å®Ÿéš›ã®å¤–éƒ¨ä¾å­˜ã‚’ä½¿ç”¨ï¼ˆãƒ†ã‚¹ãƒˆç”¨DBç­‰ï¼‰
- ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ¤œè¨¼

---

#### 2.2.2 çµ±åˆãƒ†ã‚¹ãƒˆã®ã‚¹ã‚³ãƒ¼ãƒ—

| ãƒ†ã‚¹ãƒˆåˆ†é¡ | å¯¾è±¡ | ãƒ†ã‚¹ãƒˆå†…å®¹ | å®Ÿè£…æ•° |
|----------|------|----------|-------|
| **å±¤é–“çµ±åˆ** | éš£æ¥å±¤ã®é€£æº | Serviceå±¤ â†” Domainå±¤ | 20+ |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ** | DBæ“ä½œ | CRUDæ“ä½œã€ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ | 15+ |
| **å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªçµ±åˆ** | NeuralForecastç­‰ | ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€äºˆæ¸¬ | 10+ |
| **MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çµ±åˆ** | MLflow API | ãƒ­ã‚°è¨˜éŒ²ã€ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿å­˜ | 8+ |
| **Rayä¸¦åˆ—å‡¦ç†çµ±åˆ** | Ray Cluster | ä¸¦åˆ—å®Ÿè¡Œã€ãƒªã‚½ãƒ¼ã‚¹ç®¡ç† | 5+ |
| **ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ** | File I/O | ãƒ¢ãƒ‡ãƒ«ä¿å­˜/èª­ã¿è¾¼ã¿ | 10+ |

**åˆè¨ˆç›®æ¨™**: 70+ çµ±åˆãƒ†ã‚¹ãƒˆ

---

#### 2.2.3 çµ±åˆãƒ†ã‚¹ãƒˆã®ä¾‹

```python
# âœ… è‰¯ã„çµ±åˆãƒ†ã‚¹ãƒˆã®ä¾‹
import pytest
from sqlalchemy import create_engine
from nf_auto_runner.service import ExecutionService
from nf_auto_runner.data import DataLoader
from nf_auto_runner.model import ModelRegistry


@pytest.fixture
def test_database():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    engine = create_engine("postgresql://test:test@localhost:5433/test_db")
    # ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    from alembic import command
    from alembic.config import Config
    alembic_cfg = Config("alembic.ini")
    command.upgrade(alembic_cfg, "head")
    
    yield engine
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    command.downgrade(alembic_cfg, "base")


class TestExecutionServiceIntegration:
    """ExecutionServiceã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def test_execute_single_run_end_to_end(
        self, test_database, tmp_path
    ):
        """å˜ä¸€Runå®Ÿè¡Œã®E2Eãƒ•ãƒ­ãƒ¼"""
        # Arrange
        service = ExecutionService(db_engine=test_database)
        data_loader = DataLoader(data_dir=tmp_path / "data")
        model_registry = ModelRegistry()
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        test_data = data_loader.load_sample_data()
        
        # Act
        run_result = service.execute_single_run(
            data=test_data,
            model_name="AutoNHITS",
            hyperparameters={"h": 7, "input_size": 14}
        )
        
        # Assert
        assert run_result.status == "completed"
        assert run_result.metrics is not None
        assert "mae" in run_result.metrics
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
        from nf_auto_runner.artifact.models import Run
        with test_database.begin() as conn:
            saved_run = conn.execute(
                "SELECT * FROM runs WHERE run_id = %s",
                (run_result.run_id,)
            ).fetchone()
            assert saved_run is not None
    
    def test_parallel_execution_with_ray(
        self, test_database, tmp_path
    ):
        """Rayä¸¦åˆ—å®Ÿè¡Œã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # Arrange
        import ray
        ray.init(num_cpus=4)
        
        service = ExecutionService(
            db_engine=test_database,
            executor_type="ray"
        )
        
        # è¤‡æ•°ã®å®Ÿè¡Œè¨ˆç”»ã‚’ç”Ÿæˆ
        execution_plans = [
            {"model": "AutoNHITS", "h": 7},
            {"model": "AutoLSTM", "h": 7},
            {"model": "AutoTFT", "h": 7},
        ]
        
        # Act
        results = service.execute_parallel(execution_plans)
        
        # Assert
        assert len(results) == 3
        assert all(r.status == "completed" for r in results)
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        ray.shutdown()
```

---

### 2.3 E2Eãƒ†ã‚¹ãƒˆï¼ˆEnd-to-End Testsï¼‰

#### 2.3.1 æ¦‚è¦

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ãŒå®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚·ãƒŠãƒªã‚ªã§æ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼

**ç‰¹å¾´**:
- ä½é€Ÿå®Ÿè¡Œï¼ˆæ•°åˆ†ï½æ•°ååˆ†ï¼‰
- æœ¬ç•ªã«è¿‘ã„ç’°å¢ƒ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒŠãƒªã‚ªãƒ™ãƒ¼ã‚¹

---

#### 2.3.2 E2Eãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª

| ã‚·ãƒŠãƒªã‚ªID | ã‚·ãƒŠãƒªã‚ªå | èª¬æ˜ | å®Ÿè¡Œæ™‚é–“ç›®å®‰ |
|----------|----------|------|------------|
| **E2E-001** | æ–°è¦å®Ÿé¨“ã®å®Œå…¨å®Ÿè¡Œ | ãƒ‡ãƒ¼ã‚¿èª­è¾¼â†’å­¦ç¿’â†’è©•ä¾¡â†’ä¿å­˜ | 5åˆ† |
| **E2E-002** | é‡è¤‡å®Ÿé¨“ã®ã‚¹ã‚­ãƒƒãƒ— | Fingerprintæ¤œå‡ºâ†’ã‚¹ã‚­ãƒƒãƒ— | 1åˆ† |
| **E2E-003** | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ | Optunaçµ±åˆâ†’æœ€é©åŒ– | 15åˆ† |
| **E2E-004** | ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæ˜‡æ ¼ | Stagingâ†’Production | 2åˆ† |
| **E2E-005** | å†å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° | å®šæœŸå®Ÿè¡Œâ†’æ›´æ–° | 10åˆ† |
| **E2E-006** | ãƒãƒƒãƒäºˆæ¸¬é…ä¿¡ | äºˆæ¸¬å®Ÿè¡Œâ†’çµæœä¿å­˜ | 5åˆ† |
| **E2E-007** | UIçµŒç”±ã®æ“ä½œ | ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œâ†’çµæœç¢ºèª | 8åˆ† |
| **E2E-008** | APIçµŒç”±ã®æ“ä½œ | REST APIâ†’CRUD | 3åˆ† |

**åˆè¨ˆç›®æ¨™**: 10+ E2Eãƒ†ã‚¹ãƒˆ

---

#### 2.3.3 E2Eãƒ†ã‚¹ãƒˆã®ä¾‹

```python
# E2Eãƒ†ã‚¹ãƒˆã®ä¾‹
import pytest
from selenium import webdriver
from nf_auto_runner.app import create_app


@pytest.fixture
def app():
    """ãƒ†ã‚¹ãƒˆç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    app = create_app(config="testing")
    yield app


@pytest.fixture
def browser():
    """ãƒ–ãƒ©ã‚¦ã‚¶ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    driver = webdriver.Chrome()
    yield driver
    driver.quit()


class TestE2ENewExperiment:
    """æ–°è¦å®Ÿé¨“ã®E2Eãƒ†ã‚¹ãƒˆ"""
    
    def test_complete_experiment_workflow(self, app, browser):
        """å®Œå…¨ãªå®Ÿé¨“ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®E2Eãƒ†ã‚¹ãƒˆ"""
        # 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
        browser.get("http://localhost:8501")
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        upload_input = browser.find_element_by_id("data-upload")
        upload_input.send_keys("/path/to/test_data.csv")
        
        # 3. ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_select = browser.find_element_by_id("model-select")
        model_select.select_by_visible_text("AutoNHITS")
        
        # 4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        horizon_input = browser.find_element_by_id("horizon")
        horizon_input.send_keys("7")
        
        # 5. å®Ÿè¡Œãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯
        execute_button = browser.find_element_by_id("execute-button")
        execute_button.click()
        
        # 6. å®Ÿè¡Œå®Œäº†ã‚’å¾…æ©Ÿ
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        wait = WebDriverWait(browser, 300)  # 5åˆ†å¾…æ©Ÿ
        success_message = wait.until(
            EC.presence_of_element_located(("id", "success-message"))
        )
        
        # 7. çµæœç¢ºèª
        assert "å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ" in success_message.text
        
        # 8. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
        mae_value = browser.find_element_by_id("mae-value").text
        assert float(mae_value) > 0
        
        # 9. ã‚°ãƒ©ãƒ•è¡¨ç¤ºç¢ºèª
        plot = browser.find_element_by_id("forecast-plot")
        assert plot.is_displayed()
```

---

### 2.4 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

#### 2.4.1 æ¦‚è¦

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè¦ä»¶ã‚’æº€ãŸã™ã“ã¨ã‚’æ¤œè¨¼

---

#### 2.4.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ã‚¹ã‚³ãƒ¼ãƒ—

| ãƒ†ã‚¹ãƒˆåˆ†é¡ | æ¸¬å®šå¯¾è±¡ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|----------|---------|-------|---------|
| **å®Ÿè¡Œé€Ÿåº¦** | å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚é–“ | <10åˆ† | time.perf_counter |
| **ä¸¦åˆ—å‡¦ç†** | 100ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—å­¦ç¿’ | <2æ™‚é–“ | Wall clock |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª | <16GB | psutil.virtual_memory |
| **GPUåˆ©ç”¨ç‡** | GPUä½¿ç”¨ç‡ | >80% | nvidia-smi |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | ã‚¯ã‚¨ãƒªå¿œç­”æ™‚é–“ | <100ms | pg_stat_statements |
| **APIå¿œç­”** | REST APIå¿œç­”æ™‚é–“ | <1ç§’ | Locust |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | åŒæ™‚å®Ÿè¡Œæ•° | 10ä¸¦åˆ— | Ray metrics |

---

#### 2.4.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ä¾‹

```python
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ä¾‹
import pytest
import time
import psutil
from nf_auto_runner.service import ExecutionService


class TestPerformance:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.performance
    def test_single_model_training_time(self, test_data):
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’æ™‚é–“ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        service = ExecutionService()
        
        # Act
        start_time = time.perf_counter()
        result = service.execute_single_run(
            data=test_data,
            model_name="AutoNHITS",
            hyperparameters={"h": 7}
        )
        end_time = time.perf_counter()
        
        # Assert
        execution_time = end_time - start_time
        assert execution_time < 600, f"å­¦ç¿’æ™‚é–“ãŒ10åˆ†ã‚’è¶…é: {execution_time}ç§’"
    
    @pytest.mark.performance
    @pytest.mark.slow
    def test_parallel_execution_performance(self, test_data):
        """ä¸¦åˆ—å®Ÿè¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        service = ExecutionService(executor_type="ray")
        execution_plans = [
            {"model": f"AutoNHITS", "h": 7}
            for _ in range(100)
        ]
        
        # Act
        start_time = time.perf_counter()
        results = service.execute_parallel(execution_plans)
        end_time = time.perf_counter()
        
        # Assert
        execution_time = end_time - start_time
        assert execution_time < 7200, f"100ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æ™‚é–“ãŒ2æ™‚é–“ã‚’è¶…é"
        assert all(r.status == "completed" for r in results)
    
    @pytest.mark.performance
    def test_memory_usage(self, test_data):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        service = ExecutionService()
        process = psutil.Process()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®åˆæœŸå€¤
        mem_before = process.memory_info().rss / (1024 ** 3)  # GB
        
        # Act
        result = service.execute_single_run(
            data=test_data,
            model_name="AutoNHITS",
            hyperparameters={"h": 7}
        )
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€å¤§å€¤
        mem_after = process.memory_info().rss / (1024 ** 3)  # GB
        mem_used = mem_after - mem_before
        
        # Assert
        assert mem_used < 16, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ16GBã‚’è¶…é: {mem_used}GB"
    
    @pytest.mark.performance
    def test_database_query_performance(self, test_database):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        from nf_auto_runner.artifact.repository import ExperimentRepository
        repo = ExperimentRepository(test_database)
        
        # Act
        start_time = time.perf_counter()
        experiments = repo.get_all_experiments(limit=1000)
        end_time = time.perf_counter()
        
        # Assert
        query_time = end_time - start_time
        assert query_time < 0.1, f"ã‚¯ã‚¨ãƒªæ™‚é–“ãŒ100msã‚’è¶…é: {query_time*1000}ms"
```

---

### 2.5 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ

#### 2.5.1 æ¦‚è¦

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§ã‚’æ¤œå‡ºã—ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶ã‚’æº€ãŸã™ã“ã¨ã‚’æ¤œè¨¼

---

#### 2.5.2 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®ã‚¹ã‚³ãƒ¼ãƒ—

| ãƒ†ã‚¹ãƒˆåˆ†é¡ | ãƒã‚§ãƒƒã‚¯é …ç›® | ãƒ„ãƒ¼ãƒ« |
|----------|------------|-------|
| **SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³** | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªä½¿ç”¨ | sqlmap, pytest |
| **èªè¨¼ãƒ»èªå¯** | JWTæ¤œè¨¼ã€æ¨©é™ãƒã‚§ãƒƒã‚¯ | pytest |
| **å…¥åŠ›æ¤œè¨¼** | ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£… | pytest, hypothesis |
| **ç§˜å¯†æƒ…å ±ç®¡ç†** | ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ç¦æ­¢ | bandit, pytest |
| **ä¾å­˜é–¢ä¿‚ã‚¹ã‚­ãƒ£ãƒ³** | æ—¢çŸ¥ã®è„†å¼±æ€§æ¤œå‡º | safety, pip-audit |
| **ã‚³ãƒ¼ãƒ‰ã‚¹ã‚­ãƒ£ãƒ³** | ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œæ¤œå‡º | bandit, semgrep |

---

#### 2.5.3 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®ä¾‹

```python
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®ä¾‹
import pytest
from nf_auto_runner.api import create_api
from nf_auto_runner.auth import verify_token


class TestSecurity:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.security
    def test_sql_injection_prevention(self, test_database):
        """SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        from nf_auto_runner.artifact.repository import ExperimentRepository
        repo = ExperimentRepository(test_database)
        
        # Act: SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è©¦è¡Œ
        malicious_input = "1' OR '1'='1"
        
        # Assert: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        with pytest.raises(ValueError):
            repo.get_experiment_by_id(malicious_input)
    
    @pytest.mark.security
    def test_authentication_required(self, test_client):
        """èªè¨¼ãŒå¿…è¦ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ"""
        # Act: èªè¨¼ãªã—ã§ã‚¢ã‚¯ã‚»ã‚¹
        response = test_client.get("/api/experiments")
        
        # Assert: 401ã‚¨ãƒ©ãƒ¼
        assert response.status_code == 401
    
    @pytest.mark.security
    def test_jwt_token_validation(self):
        """JWTãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        invalid_token = "invalid.token.here"
        
        # Act & Assert
        with pytest.raises(Exception):
            verify_token(invalid_token)
    
    @pytest.mark.security
    def test_no_hardcoded_secrets(self):
        """ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸç§˜å¯†æƒ…å ±ãŒãªã„ã“ã¨ã‚’ç¢ºèª"""
        import subprocess
        
        # banditã§ã‚¹ã‚­ãƒ£ãƒ³
        result = subprocess.run(
            ["bandit", "-r", "src/", "-f", "json"],
            capture_output=True,
            text=True
        )
        
        # Assert: é«˜ãƒªã‚¹ã‚¯ã®å•é¡ŒãŒãªã„ã“ã¨
        import json
        report = json.loads(result.stdout)
        high_severity = [
            issue for issue in report["results"]
            if issue["issue_severity"] == "HIGH"
        ]
        assert len(high_severity) == 0, f"é«˜ãƒªã‚¹ã‚¯ã®å•é¡ŒãŒæ¤œå‡º: {high_severity}"
```

---

## 3. ãƒ†ã‚¹ãƒˆæ‰‹æ³•ã¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

### 3.1 Test-Driven Development (TDD)

#### 3.1.1 TDDã‚µã‚¤ã‚¯ãƒ«

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã¯ã€TDDï¼ˆãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºï¼‰ã‚’æ¡ç”¨ã—ã¾ã™ï¼š

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Red       â”‚ â”€â”€â”
  â”‚  å¤±æ•—ã™ã‚‹    â”‚   â”‚
  â”‚  ãƒ†ã‚¹ãƒˆã‚’æ›¸ã â”‚   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
         â†‘          â”‚
         â”‚          â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Refactor   â”‚   â”‚   Green     â”‚
  â”‚  ãƒªãƒ•ã‚¡ã‚¯ã‚¿   â”‚â†â”€â”€â”‚  ãƒ†ã‚¹ãƒˆã‚’    â”‚
  â”‚             â”‚   â”‚  ãƒ‘ã‚¹ã•ã›ã‚‹   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ã‚¹ãƒ†ãƒƒãƒ—**:
1. **Red**: å¤±æ•—ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’æ›¸ã
2. **Green**: ãƒ†ã‚¹ãƒˆã‚’ãƒ‘ã‚¹ã™ã‚‹æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã
3. **Refactor**: ã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„ï¼ˆãƒ†ã‚¹ãƒˆã¯é€šã‚‹çŠ¶æ…‹ã‚’ç¶­æŒï¼‰

---

#### 3.1.2 TDDå®Ÿè·µä¾‹

```python
# ã‚¹ãƒ†ãƒƒãƒ—1: Red - å¤±æ•—ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’æ›¸ã
def test_calculate_mae():
    """MAEè¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""
    # Arrange
    y_true = np.array([1, 2, 3, 4, 5])
    y_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])
    
    # Act
    mae = calculate_mae(y_true, y_pred)
    
    # Assert
    expected = np.mean(np.abs(y_true - y_pred))
    assert mae == pytest.approx(expected)


# ã‚¹ãƒ†ãƒƒãƒ—2: Green - ãƒ†ã‚¹ãƒˆã‚’ãƒ‘ã‚¹ã™ã‚‹æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰
def calculate_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """MAEã‚’è¨ˆç®—"""
    return np.mean(np.abs(y_true - y_pred))


# ã‚¹ãƒ†ãƒƒãƒ—3: Refactor - ã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„
def calculate_mae(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    *,
    sample_weight: Optional[np.ndarray] = None
) -> float:
    """
    MAEã‚’è¨ˆç®—
    
    Args:
        y_true: çœŸã®å€¤
        y_pred: äºˆæ¸¬å€¤
        sample_weight: ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        MAEå€¤
    
    Raises:
        ValueError: å…¥åŠ›é…åˆ—ã®å½¢çŠ¶ãŒä¸€è‡´ã—ãªã„å ´åˆ
    """
    if y_true.shape != y_pred.shape:
        raise ValueError("y_trueã¨y_predã®å½¢çŠ¶ãŒä¸€è‡´ã—ã¾ã›ã‚“")
    
    absolute_errors = np.abs(y_true - y_pred)
    
    if sample_weight is not None:
        if sample_weight.shape != y_true.shape:
            raise ValueError("sample_weightã®å½¢çŠ¶ãŒä¸€è‡´ã—ã¾ã›ã‚“")
        return np.average(absolute_errors, weights=sample_weight)
    
    return np.mean(absolute_errors)


# ãƒªãƒ•ã‚¡ã‚¯ã‚¿å¾Œã®è¿½åŠ ãƒ†ã‚¹ãƒˆ
def test_calculate_mae_with_sample_weight():
    """é‡ã¿ä»˜ãMAEè¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""
    # Arrange
    y_true = np.array([1, 2, 3, 4, 5])
    y_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])
    weights = np.array([1, 1, 1, 2, 2])
    
    # Act
    mae = calculate_mae(y_true, y_pred, sample_weight=weights)
    
    # Assert
    absolute_errors = np.abs(y_true - y_pred)
    expected = np.average(absolute_errors, weights=weights)
    assert mae == pytest.approx(expected)


def test_calculate_mae_with_mismatched_shapes():
    """å½¢çŠ¶ä¸ä¸€è‡´ã®ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
    # Arrange
    y_true = np.array([1, 2, 3])
    y_pred = np.array([1, 2])
    
    # Act & Assert
    with pytest.raises(ValueError, match="å½¢çŠ¶ãŒä¸€è‡´ã—ã¾ã›ã‚“"):
        calculate_mae(y_true, y_pred)
```

---

### 3.2 Behavior-Driven Development (BDD)

#### 3.2.1 Given-When-Then ãƒ‘ã‚¿ãƒ¼ãƒ³

BDDã‚¹ã‚¿ã‚¤ãƒ«ã§ãƒ†ã‚¹ãƒˆã‚’è¨˜è¿°ã—ã¾ã™ï¼š

```python
# BDDã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ†ã‚¹ãƒˆ
def test_duplicate_experiment_detection():
    """é‡è¤‡å®Ÿé¨“æ¤œå‡ºã®BDDãƒ†ã‚¹ãƒˆ"""
    # Given: æ—¢å­˜ã®å®Ÿé¨“ãŒå­˜åœ¨ã™ã‚‹
    existing_experiment = create_experiment(
        model="AutoNHITS",
        hyperparameters={"h": 7, "input_size": 14},
        data_hash="abc123"
    )
    save_experiment(existing_experiment)
    
    # When: åŒã˜æ¡ä»¶ã§æ–°ã—ã„å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã‚ˆã†ã¨ã™ã‚‹
    new_experiment = create_experiment(
        model="AutoNHITS",
        hyperparameters={"h": 7, "input_size": 14},
        data_hash="abc123"
    )
    
    # Then: é‡è¤‡ãŒæ¤œå‡ºã•ã‚Œã‚‹
    assert is_duplicate(new_experiment, existing_experiment)
    
    # And: å®Ÿé¨“ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹
    result = execute_experiment(new_experiment)
    assert result.status == "skipped"
    assert result.reason == "duplicate_detected"
```

---

### 3.3 Property-Based Testing

#### 3.3.1 Hypothesisã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚¹ãƒˆ

ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã§åºƒç¯„ãªå…¥åŠ›ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ï¼š

```python
from hypothesis import given, strategies as st
import numpy as np


@given(
    y_true=st.lists(st.floats(min_value=-1000, max_value=1000), min_size=1, max_size=100),
    y_pred=st.lists(st.floats(min_value=-1000, max_value=1000), min_size=1, max_size=100)
)
def test_mae_properties(y_true, y_pred):
    """MAEè¨ˆç®—ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    # åŒã˜é•·ã•ã«ã™ã‚‹
    min_len = min(len(y_true), len(y_pred))
    y_true = np.array(y_true[:min_len])
    y_pred = np.array(y_pred[:min_len])
    
    # Act
    mae = calculate_mae(y_true, y_pred)
    
    # Properties to test:
    # 1. MAEã¯éè² 
    assert mae >= 0
    
    # 2. å®Œå…¨äºˆæ¸¬ã®å ´åˆMAEã¯0
    perfect_mae = calculate_mae(y_true, y_true)
    assert perfect_mae == 0
    
    # 3. MAEã¯èª¤å·®ã®çµ¶å¯¾å€¤ã®å¹³å‡
    expected = np.mean(np.abs(y_true - y_pred))
    assert abs(mae - expected) < 1e-10


@given(
    data=st.data(),
    n_samples=st.integers(min_value=10, max_value=1000),
    n_features=st.integers(min_value=1, max_value=20)
)
def test_data_preprocessor_properties(data, n_samples, n_features):
    """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    # Arrange
    df = pd.DataFrame(
        data.draw(
            st.lists(
                st.lists(st.floats(allow_nan=False, allow_infinity=False), 
                        min_size=n_features, max_size=n_features),
                min_size=n_samples, max_size=n_samples
            )
        )
    )
    
    preprocessor = DataPreprocessor()
    
    # Act
    processed = preprocessor.preprocess(df)
    
    # Properties:
    # 1. å½¢çŠ¶ã¯ä¿æŒã•ã‚Œã‚‹
    assert processed.shape == df.shape
    
    # 2. NaNã¯å«ã¾ã‚Œãªã„
    assert not processed.isnull().any().any()
    
    # 3. ç„¡é™å¤§ã¯å«ã¾ã‚Œãªã„
    assert not np.isinf(processed.values).any()
```

---

### 3.4 Mutation Testing

#### 3.4.1 æ¦‚è¦

ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã§ãƒ†ã‚¹ãƒˆã®å“è³ªã‚’è©•ä¾¡ã—ã¾ã™ï¼š

```bash
# mutmutã‚’ä½¿ç”¨ã—ãŸãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
pip install mutmut

# å®Ÿè¡Œ
mutmut run --paths-to-mutate src/nf_auto_runner/

# çµæœç¢ºèª
mutmut results

# ç”Ÿãæ®‹ã£ãŸãƒŸãƒ¥ãƒ¼ã‚¿ãƒ³ãƒˆã®ç¢ºèª
mutmut show
```

**ç›®æ¨™**: ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ >80%

---

## 4. ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™

### 4.1 å±¤ã”ã¨ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™

| å±¤ | ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ | æ¸¬å®šæ–¹æ³• | é‡è¦åº¦ |
|---|--------------|---------|-------|
| **Configurationå±¤** | >95% | Line coverage | é«˜ |
| **Dataå±¤** | >95% | Line + Branch coverage | é«˜ |
| **Model Discoveryå±¤** | >90% | Line coverage | é«˜ |
| **Hyperparameterå±¤** | >90% | Line coverage | ä¸­ |
| **Execution Planå±¤** | >90% | Line coverage | ä¸­ |
| **Executionå±¤** | >85% | Line coverage | é«˜ |
| **Artifactå±¤** | >90% | Line coverage | ä¸­ |
| **Loggingå±¤** | >90% | Line coverage | ä¸­ |
| **Serviceå±¤** | >85% | Line + Branch coverage | é«˜ |
| **Applicationå±¤** | >85% | Line coverage | ä¸­ |

**å…¨ä½“ç›®æ¨™**: >90%

---

### 4.2 ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š

#### 4.2.1 pytest-covã‚’ä½¿ç”¨ã—ãŸæ¸¬å®š

```bash
# åŸºæœ¬çš„ãªã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
pytest tests/ --cov=src/nf_auto_runner --cov-report=html

# ãƒ–ãƒ©ãƒ³ãƒã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’å«ã‚€
pytest tests/ --cov=src/nf_auto_runner --cov-branch --cov-report=html

# ç‰¹å®šã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿
pytest tests/unit/config/ --cov=src/nf_auto_runner/config --cov-report=term

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³ã®è¡Œã‚’è¡¨ç¤º
pytest tests/ --cov=src/nf_auto_runner --cov-report=term-missing
```

---

#### 4.2.2 ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã®ä¾‹

```
---------- coverage: platform linux, python 3.11.6 -----------
Name                                      Stmts   Miss Branch BrPart  Cover   Missing
---------------------------------------------------------------------------------------
src/nf_auto_runner/config/__init__.py        10      0      0      0   100%
src/nf_auto_runner/config/base.py          125      3     42      2    96%   45-47, 89->92
src/nf_auto_runner/config/path.py           89      2     28      1    97%   67-68
src/nf_auto_runner/config/execution.py     156      8     54      4    93%   89, 112-115, 178->181
src/nf_auto_runner/data/loader.py          234     12     78      6    94%   123-125, 189, 234-237
---------------------------------------------------------------------------------------
TOTAL                                      3456    127   1245     48    95%
```

---

### 4.3 ã‚«ãƒãƒ¬ãƒƒã‚¸é™¤å¤–è¦å‰‡

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆæ¸¬ã‹ã‚‰é™¤å¤–ã—ã¾ã™ï¼š

```python
# .coveragerc
[report]
exclude_lines =
    # æ¨™æº–çš„ãªé™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstract
    @abstractmethod
    
    # ã‚«ã‚¹ã‚¿ãƒ é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
    # ãƒ‡ãƒãƒƒã‚°ã‚³ãƒ¼ãƒ‰
    if DEBUG:
    if debug:
    
    # é˜²å¾¡çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼ˆé€šå¸¸ç™ºç”Ÿã—ãªã„ï¼‰
    except Exception as e:
        logger.critical
    
    # å‹ãƒã‚§ãƒƒã‚¯ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    if typing.TYPE_CHECKING:
```

---

## 5. TDDå®Ÿè·µã‚¬ã‚¤ãƒ‰

### 5.1 TDDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

#### 5.1.1 ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®TDDå®Ÿè·µ

å„é–‹ç™ºãƒ•ã‚§ãƒ¼ã‚ºã§TDDã‚’å®Ÿè·µã—ã¾ã™ï¼š

```
Phase 1: Configurationå±¤
â”œâ”€â”€ 1. ãƒ†ã‚¹ãƒˆè¨­è¨ˆ
â”‚   â”œâ”€â”€ ConfigåŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ
â”‚   â”œâ”€â”€ PathConfigã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 2. ãƒ†ã‚¹ãƒˆå®Ÿè£…ï¼ˆRedï¼‰
â”‚   â”œâ”€â”€ test_config_base.py
â”‚   â”œâ”€â”€ test_path_config.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 3. å®Ÿè£…ï¼ˆGreenï¼‰
â”‚   â”œâ”€â”€ config/base.py
â”‚   â”œâ”€â”€ config/path.py
â”‚   â””â”€â”€ ...
â””â”€â”€ 4. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
    â”œâ”€â”€ ã‚³ãƒ¼ãƒ‰æ”¹å–„
    â””â”€â”€ ãƒ†ã‚¹ãƒˆæ”¹å–„
```

---

#### 5.1.2 TDDå®Ÿè·µãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å„å®Ÿè£…å‰ã«ç¢ºèªï¼š

- [ ] ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å…ˆã«è¨­è¨ˆã—ãŸã‹ï¼Ÿ
- [ ] ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿï¼ˆRedï¼‰
- [ ] æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆã‚’ãƒ‘ã‚¹ã•ã›ãŸã‹ï¼Ÿï¼ˆGreenï¼‰
- [ ] ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ãŸã‹ï¼Ÿ
- [ ] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒã¾ã ãƒ‘ã‚¹ã™ã‚‹ã‹ï¼Ÿ
- [ ] ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ã‚’é”æˆã—ãŸã‹ï¼Ÿ

---

### 5.2 ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆé–‹ç™ºã®ä¾‹

#### 5.2.1 DataLoaderã‚¯ãƒ©ã‚¹ã®é–‹ç™ºä¾‹

```python
# ========================================
# ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚¹ãƒˆè¨­è¨ˆ
# ========================================

# tests/unit/data/test_data_loader.py

import pytest
from pathlib import Path
import pandas as pd


class TestDataLoader:
    """DataLoaderã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    
    # ========================================
    # RED: ã¾ãšãƒ†ã‚¹ãƒˆã‚’æ›¸ãï¼ˆå®Ÿè£…ã¯ã¾ã ãªã„ï¼‰
    # ========================================
    
    def test_init_with_valid_path(self, tmp_path):
        """æœ‰åŠ¹ãªãƒ‘ã‚¹ã§ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        data_dir = tmp_path / "data"
        data_dir.mkdir()
        
        # Act
        from nf_auto_runner.data import DataLoader
        loader = DataLoader(data_dir=str(data_dir))
        
        # Assert
        assert loader.data_dir == str(data_dir)
        assert loader.is_valid()
    
    def test_load_csv_success(self, tmp_path):
        """CSVèª­ã¿è¾¼ã¿æˆåŠŸã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        data_dir = tmp_path / "data"
        data_dir.mkdir()
        
        # ãƒ†ã‚¹ãƒˆCSVä½œæˆ
        test_csv = data_dir / "test.csv"
        pd.DataFrame({
            'unique_id': ['A', 'A', 'B', 'B'],
            'ds': pd.date_range('2025-01-01', periods=4),
            'y': [1, 2, 3, 4]
        }).to_csv(test_csv, index=False)
        
        # Act
        from nf_auto_runner.data import DataLoader
        loader = DataLoader(data_dir=str(data_dir))
        df = loader.load_csv("test.csv")
        
        # Assert
        assert isinstance(df, pd.DataFrame)
        assert len(df) == 4
        assert list(df.columns) == ['unique_id', 'ds', 'y']
    
    def test_load_csv_file_not_found(self, tmp_path):
        """å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        data_dir = tmp_path / "data"
        data_dir.mkdir()
        
        # Act & Assert
        from nf_auto_runner.data import DataLoader
        loader = DataLoader(data_dir=str(data_dir))
        
        with pytest.raises(FileNotFoundError):
            loader.load_csv("nonexistent.csv")
    
    def test_validate_schema_success(self, tmp_path):
        """ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼æˆåŠŸã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        df = pd.DataFrame({
            'unique_id': ['A', 'A'],
            'ds': pd.date_range('2025-01-01', periods=2),
            'y': [1, 2]
        })
        
        # Act
        from nf_auto_runner.data import DataLoader
        loader = DataLoader(data_dir=str(tmp_path))
        is_valid = loader.validate_schema(df)
        
        # Assert
        assert is_valid is True
    
    def test_validate_schema_missing_column(self):
        """å¿…é ˆã‚«ãƒ©ãƒ æ¬ è½ã®ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        df = pd.DataFrame({
            'unique_id': ['A', 'A'],
            'ds': pd.date_range('2025-01-01', periods=2)
            # 'y' ã‚«ãƒ©ãƒ ãŒæ¬ è½
        })
        
        # Act
        from nf_auto_runner.data import DataLoader
        loader = DataLoader(data_dir="/tmp")
        is_valid = loader.validate_schema(df)
        
        # Assert
        assert is_valid is False


# ========================================
# ã‚¹ãƒ†ãƒƒãƒ—2: å®Ÿè£…ï¼ˆGREENï¼‰
# ========================================

# src/nf_auto_runner/data/loader.py

from pathlib import Path
from typing import Optional
import pandas as pd
from loguru import logger


class DataLoader:
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¯ãƒ©ã‚¹"""
    
    REQUIRED_COLUMNS = ['unique_id', 'ds', 'y']
    
    def __init__(self, data_dir: str):
        """
        åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        
        Raises:
            ValueError: data_dirãŒç©ºã®å ´åˆ
        """
        if not data_dir:
            raise ValueError("data_dir cannot be empty")
        
        self.data_dir = data_dir
        self._data_path = Path(data_dir)
        
        logger.info(f"DataLoader initialized with data_dir={data_dir}")
    
    def is_valid(self) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        
        Returns:
            æœ‰åŠ¹ãªå ´åˆTrue
        """
        return self._data_path.exists() and self._data_path.is_dir()
    
    def load_csv(
        self,
        filename: str,
        *,
        parse_dates: bool = True
    ) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            parse_dates: æ—¥ä»˜ã‚’è‡ªå‹•ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã‹
        
        Returns:
            èª­ã¿è¾¼ã‚“ã DataFrame
        
        Raises:
            FileNotFoundError: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
        """
        file_path = self._data_path / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        logger.info(f"Loading CSV: {file_path}")
        
        df = pd.read_csv(
            file_path,
            parse_dates=['ds'] if parse_dates else None
        )
        
        logger.info(f"Loaded {len(df)} rows from {filename}")
        
        return df
    
    def validate_schema(self, df: pd.DataFrame) -> bool:
        """
        DataFrameã®ã‚¹ã‚­ãƒ¼ãƒã‚’æ¤œè¨¼
        
        Args:
            df: æ¤œè¨¼å¯¾è±¡ã®DataFrame
        
        Returns:
            æœ‰åŠ¹ãªå ´åˆTrue
        """
        missing_columns = set(self.REQUIRED_COLUMNS) - set(df.columns)
        
        if missing_columns:
            logger.warning(f"Missing required columns: {missing_columns}")
            return False
        
        logger.info("Schema validation passed")
        return True


# ========================================
# ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
# ========================================

# $ pytest tests/unit/data/test_data_loader.py -v
# 
# tests/unit/data/test_data_loader.py::TestDataLoader::test_init_with_valid_path PASSED
# tests/unit/data/test_data_loader.py::TestDataLoader::test_load_csv_success PASSED
# tests/unit/data/test_data_loader.py::TestDataLoader::test_load_csv_file_not_found PASSED
# tests/unit/data/test_data_loader.py::TestDataLoader::test_validate_schema_success PASSED
# tests/unit/data/test_data_loader.py::TestDataLoader::test_validate_schema_missing_column PASSED


# ========================================
# ã‚¹ãƒ†ãƒƒãƒ—4: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
# ========================================

# ã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„ï¼ˆãƒ†ã‚¹ãƒˆã¯é€šã‚‹çŠ¶æ…‹ã‚’ç¶­æŒï¼‰

class DataLoader:
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    
    REQUIRED_COLUMNS = ['unique_id', 'ds', 'y']
    
    def __init__(
        self,
        data_dir: str,
        *,
        validate_on_init: bool = True
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            validate_on_init: åˆæœŸåŒ–æ™‚ã«æ¤œè¨¼ã™ã‚‹ã‹
        
        Raises:
            ValueError: data_dirãŒç©ºã¾ãŸã¯ç„¡åŠ¹ãªå ´åˆ
        """
        if not data_dir:
            raise ValueError("data_dir cannot be empty")
        
        self.data_dir = data_dir
        self._data_path = Path(data_dir)
        
        if validate_on_init and not self.is_valid():
            raise ValueError(f"Invalid data directory: {data_dir}")
        
        logger.info(f"DataLoader initialized with data_dir={data_dir}")
    
    def is_valid(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self._data_path.exists() and self._data_path.is_dir()
    
    def load_csv(
        self,
        filename: str,
        *,
        parse_dates: bool = True,
        validate_schema: bool = True
    ) -> pd.DataFrame:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            parse_dates: æ—¥ä»˜ã‚’è‡ªå‹•ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ã‹
            validate_schema: ã‚¹ã‚­ãƒ¼ãƒã‚’æ¤œè¨¼ã™ã‚‹ã‹
        
        Returns:
            èª­ã¿è¾¼ã‚“ã DataFrame
        
        Raises:
            FileNotFoundError: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
            ValueError: ã‚¹ã‚­ãƒ¼ãƒãŒç„¡åŠ¹ãªå ´åˆ
        """
        file_path = self._data_path / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        logger.info(f"Loading CSV: {file_path}")
        
        df = pd.read_csv(
            file_path,
            parse_dates=['ds'] if parse_dates else None
        )
        
        if validate_schema and not self.validate_schema(df):
            raise ValueError(f"Invalid schema in {filename}")
        
        logger.info(f"Loaded {len(df)} rows from {filename}")
        
        return df
    
    def validate_schema(
        self,
        df: pd.DataFrame,
        *,
        strict: bool = False
    ) -> bool:
        """
        DataFrameã®ã‚¹ã‚­ãƒ¼ãƒã‚’æ¤œè¨¼
        
        Args:
            df: æ¤œè¨¼å¯¾è±¡ã®DataFrame
            strict: å³å¯†ãƒ¢ãƒ¼ãƒ‰ï¼ˆè¿½åŠ ã‚«ãƒ©ãƒ ã‚‚è¨±å¯ã—ãªã„ï¼‰
        
        Returns:
            æœ‰åŠ¹ãªå ´åˆTrue
        """
        missing_columns = set(self.REQUIRED_COLUMNS) - set(df.columns)
        
        if missing_columns:
            logger.warning(f"Missing required columns: {missing_columns}")
            return False
        
        if strict:
            extra_columns = set(df.columns) - set(self.REQUIRED_COLUMNS)
            if extra_columns:
                logger.warning(f"Extra columns found: {extra_columns}")
                return False
        
        logger.info("Schema validation passed")
        return True


# ========================================
# ã‚¹ãƒ†ãƒƒãƒ—5: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¿½åŠ ãƒ†ã‚¹ãƒˆ
# ========================================

class TestDataLoaderRefactored:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®è¿½åŠ ãƒ†ã‚¹ãƒˆ"""
    
    def test_init_with_validate_on_init_false(self, tmp_path):
        """åˆæœŸåŒ–æ™‚æ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        nonexistent_dir = tmp_path / "nonexistent"
        
        # Act: æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã®ã§ä¾‹å¤–ã¯ç™ºç”Ÿã—ãªã„
        loader = DataLoader(
            data_dir=str(nonexistent_dir),
            validate_on_init=False
        )
        
        # Assert
        assert not loader.is_valid()
    
    def test_load_csv_with_validate_schema_false(self, tmp_path):
        """ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ã‚¹ã‚­ãƒƒãƒ—ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        data_dir = tmp_path / "data"
        data_dir.mkdir()
        
        # ä¸æ­£ãªCSVï¼ˆ'y'ã‚«ãƒ©ãƒ ãŒæ¬ è½ï¼‰
        invalid_csv = data_dir / "invalid.csv"
        pd.DataFrame({
            'unique_id': ['A'],
            'ds': pd.date_range('2025-01-01', periods=1)
        }).to_csv(invalid_csv, index=False)
        
        loader = DataLoader(data_dir=str(data_dir))
        
        # Act: æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã®ã§ä¾‹å¤–ã¯ç™ºç”Ÿã—ãªã„
        df = loader.load_csv("invalid.csv", validate_schema=False)
        
        # Assert
        assert 'y' not in df.columns
    
    def test_validate_schema_strict_mode(self):
        """å³å¯†ãƒ¢ãƒ¼ãƒ‰ã§ã®æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        df = pd.DataFrame({
            'unique_id': ['A'],
            'ds': pd.date_range('2025-01-01', periods=1),
            'y': [1],
            'extra_column': ['extra']  # è¿½åŠ ã‚«ãƒ©ãƒ 
        })
        
        loader = DataLoader(data_dir="/tmp", validate_on_init=False)
        
        # Act & Assert
        assert loader.validate_schema(df, strict=False) is True
        assert loader.validate_schema(df, strict=True) is False
```

---

## 6. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ

### 6.1 ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆåŸå‰‡

#### 6.1.1 ã‚«ãƒãƒ¬ãƒƒã‚¸åŸºæº–

ä»¥ä¸‹ã®è¦³ç‚¹ã§ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è¨­è¨ˆã—ã¾ã™ï¼š

| è¦³ç‚¹ | èª¬æ˜ | ä¾‹ |
|-----|------|---|
| **æ­£å¸¸ç³»** | ä»•æ§˜é€šã‚Šã®å‹•ä½œ | æœ‰åŠ¹ãªå…¥åŠ›ã§ã®æ­£å¸¸å‡¦ç† |
| **æº–æ­£å¸¸ç³»** | ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ | å¢ƒç•Œå€¤ã€ç©ºãƒ‡ãƒ¼ã‚¿ |
| **ç•°å¸¸ç³»** | ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ | ç„¡åŠ¹ãªå…¥åŠ›ã€ä¾‹å¤–ç™ºç”Ÿ |
| **å¢ƒç•Œå€¤** | å¢ƒç•Œæ¡ä»¶ | æœ€å°å€¤ã€æœ€å¤§å€¤ã€0 |
| **ç­‰ä¾¡ã‚¯ãƒ©ã‚¹** | åŒç­‰ã®å‹•ä½œã‚’ã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ— | æ­£ã®æ•´æ•°ã€è² ã®æ•´æ•° |
| **çŠ¶æ…‹é·ç§»** | çŠ¶æ…‹ã®å¤‰åŒ– | åˆæœŸåŒ–â†’å®Ÿè¡Œä¸­â†’å®Œäº† |

---

#### 6.1.2 ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```markdown
## ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ID: TC-XXX-YYY

### ç›®çš„
ä½•ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‹

### å‰ææ¡ä»¶
- æ¡ä»¶1
- æ¡ä»¶2

### ãƒ†ã‚¹ãƒˆã‚¹ãƒ†ãƒƒãƒ—
1. ã‚¹ãƒ†ãƒƒãƒ—1
2. ã‚¹ãƒ†ãƒƒãƒ—2
3. ã‚¹ãƒ†ãƒƒãƒ—3

### æœŸå¾…çµæœ
- æœŸå¾…ã•ã‚Œã‚‹çµæœ1
- æœŸå¾…ã•ã‚Œã‚‹çµæœ2

### å®Ÿéš›ã®çµæœ
[ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¾Œã«è¨˜å…¥]

### åˆ¤å®š
[Pass/Fail]
```

---

### 6.2 ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¾‹

#### 6.2.1 Configurationå±¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹

```python
# tests/unit/config/test_execution_config.py

import pytest
from nf_auto_runner.config import ExecutionConfig


class TestExecutionConfig:
    """ExecutionConfigã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹"""
    
    # ========================================
    # æ­£å¸¸ç³»
    # ========================================
    
    def test_init_with_default_values(self):
        """TC-CFG-001: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ã®åˆæœŸåŒ–"""
        # Act
        config = ExecutionConfig()
        
        # Assert
        assert config.max_parallel_runs == 10
        assert config.backend == "cpu"
        assert config.enable_gpu is False
    
    def test_init_with_custom_values(self):
        """TC-CFG-002: ã‚«ã‚¹ã‚¿ãƒ å€¤ã§ã®åˆæœŸåŒ–"""
        # Arrange
        max_runs = 20
        backend = "cuda"
        
        # Act
        config = ExecutionConfig(
            max_parallel_runs=max_runs,
            backend=backend,
            enable_gpu=True
        )
        
        # Assert
        assert config.max_parallel_runs == max_runs
        assert config.backend == backend
        assert config.enable_gpu is True
    
    # ========================================
    # æº–æ­£å¸¸ç³»ï¼ˆã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ï¼‰
    # ========================================
    
    def test_init_with_zero_parallel_runs(self):
        """TC-CFG-003: ä¸¦åˆ—å®Ÿè¡Œæ•°0ã®å ´åˆ"""
        # Act
        config = ExecutionConfig(max_parallel_runs=0)
        
        # Assert
        assert config.max_parallel_runs == 0
        # 0ã®å ´åˆã¯ç›´åˆ—å®Ÿè¡Œã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        assert config.get_executor_type() == "serial"
    
    @pytest.mark.parametrize("backend", ["cpu", "cuda", "mps"])
    def test_init_with_valid_backends(self, backend):
        """TC-CFG-004: æœ‰åŠ¹ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®æŒ‡å®š"""
        # Act
        config = ExecutionConfig(backend=backend)
        
        # Assert
        assert config.backend == backend
    
    # ========================================
    # ç•°å¸¸ç³»
    # ========================================
    
    def test_init_with_negative_parallel_runs(self):
        """TC-CFG-005: è² ã®ä¸¦åˆ—å®Ÿè¡Œæ•°ã§ã‚¨ãƒ©ãƒ¼"""
        # Act & Assert
        with pytest.raises(ValueError, match="must be non-negative"):
            ExecutionConfig(max_parallel_runs=-1)
    
    def test_init_with_invalid_backend(self):
        """TC-CFG-006: ç„¡åŠ¹ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã‚¨ãƒ©ãƒ¼"""
        # Act & Assert
        with pytest.raises(ValueError, match="Invalid backend"):
            ExecutionConfig(backend="invalid_backend")
    
    # ========================================
    # å¢ƒç•Œå€¤
    # ========================================
    
    @pytest.mark.parametrize("max_runs", [1, 100, 1000])
    def test_init_with_boundary_values(self, max_runs):
        """TC-CFG-007: å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ"""
        # Act
        config = ExecutionConfig(max_parallel_runs=max_runs)
        
        # Assert
        assert config.max_parallel_runs == max_runs
    
    # ========================================
    # çŠ¶æ…‹é·ç§»
    # ========================================
    
    def test_update_configuration(self):
        """TC-CFG-008: è¨­å®šã®æ›´æ–°"""
        # Arrange
        config = ExecutionConfig(max_parallel_runs=10)
        
        # Act: è¨­å®šã‚’æ›´æ–°
        config.update(max_parallel_runs=20, backend="cuda")
        
        # Assert
        assert config.max_parallel_runs == 20
        assert config.backend == "cuda"
```

---

#### 6.2.2 Dataå±¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹

```python
# tests/unit/data/test_data_preprocessor.py

import pytest
import pandas as pd
import numpy as np
from nf_auto_runner.data import DataPreprocessor


class TestDataPreprocessor:
    """DataPreprocessorã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹"""
    
    @pytest.fixture
    def sample_data(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
        return pd.DataFrame({
            'unique_id': ['A', 'A', 'A', 'B', 'B', 'B'],
            'ds': pd.date_range('2025-01-01', periods=6),
            'y': [1.0, 2.0, np.nan, 4.0, 5.0, 6.0]
        })
    
    # ========================================
    # æ­£å¸¸ç³»
    # ========================================
    
    def test_handle_missing_values_fill(self, sample_data):
        """TC-DATA-001: æ¬ æå€¤ã‚’è£œå®Œ"""
        # Arrange
        preprocessor = DataPreprocessor(missing_value_strategy='fill')
        
        # Act
        result = preprocessor.handle_missing_values(sample_data)
        
        # Assert
        assert not result['y'].isnull().any()
        assert len(result) == len(sample_data)
    
    def test_handle_missing_values_drop(self, sample_data):
        """TC-DATA-002: æ¬ æå€¤ã‚’å‰Šé™¤"""
        # Arrange
        preprocessor = DataPreprocessor(missing_value_strategy='drop')
        
        # Act
        result = preprocessor.handle_missing_values(sample_data)
        
        # Assert
        assert not result['y'].isnull().any()
        assert len(result) == len(sample_data) - 1  # 1è¡Œå‰Šé™¤ã•ã‚Œã‚‹
    
    # ========================================
    # æº–æ­£å¸¸ç³»
    # ========================================
    
    def test_handle_all_missing_values(self):
        """TC-DATA-003: ã™ã¹ã¦æ¬ æå€¤ã®å ´åˆ"""
        # Arrange
        data = pd.DataFrame({
            'unique_id': ['A', 'A'],
            'ds': pd.date_range('2025-01-01', periods=2),
            'y': [np.nan, np.nan]
        })
        preprocessor = DataPreprocessor(missing_value_strategy='drop')
        
        # Act
        result = preprocessor.handle_missing_values(data)
        
        # Assert
        assert len(result) == 0  # ã™ã¹ã¦å‰Šé™¤ã•ã‚Œã‚‹
    
    def test_handle_no_missing_values(self, sample_data):
        """TC-DATA-004: æ¬ æå€¤ãŒãªã„å ´åˆ"""
        # Arrange
        clean_data = sample_data.dropna()
        preprocessor = DataPreprocessor()
        
        # Act
        result = preprocessor.handle_missing_values(clean_data)
        
        # Assert
        pd.testing.assert_frame_equal(result, clean_data)
    
    # ========================================
    # ç•°å¸¸ç³»
    # ========================================
    
    def test_handle_invalid_strategy(self, sample_data):
        """TC-DATA-005: ç„¡åŠ¹ãªæˆ¦ç•¥ã§ã‚¨ãƒ©ãƒ¼"""
        # Act & Assert
        with pytest.raises(ValueError, match="Invalid strategy"):
            DataPreprocessor(missing_value_strategy='invalid')
    
    # ========================================
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆ
    # ========================================
    
    @pytest.mark.parametrize("strategy,expected_len", [
        ('fill', 6),
        ('drop', 5),
        ('interpolate', 6),
    ])
    def test_missing_value_strategies(
        self, sample_data, strategy, expected_len
    ):
        """TC-DATA-006: å„ç¨®æ¬ æå€¤å‡¦ç†æˆ¦ç•¥ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        preprocessor = DataPreprocessor(missing_value_strategy=strategy)
        
        # Act
        result = preprocessor.handle_missing_values(sample_data)
        
        # Assert
        assert len(result) == expected_len
        if strategy != 'drop':
            assert not result['y'].isnull().any()
```

---

## 7. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†

### 7.1 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æˆ¦ç•¥

#### 7.1.1 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡

| ç¨®é¡ | ç”¨é€” | é…ç½®å ´æ‰€ | ç®¡ç†æ–¹æ³• |
|-----|------|---------|---------|
| **Fixture** | å˜ä½“ãƒ†ã‚¹ãƒˆç”¨ | `tests/fixtures/` | pytest fixture |
| **ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿** | çµ±åˆãƒ†ã‚¹ãƒˆç”¨ | `tests/data/` | ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ |
| **åˆæˆãƒ‡ãƒ¼ã‚¿** | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ | å‹•çš„ç”Ÿæˆ | faker, hypothesis |
| **æœ¬ç•ªé¡ä¼¼ãƒ‡ãƒ¼ã‚¿** | E2Eãƒ†ã‚¹ãƒˆç”¨ | `tests/data/production_like/` | åŒ¿ååŒ–æ¸ˆã¿ |

---

#### 7.1.2 Fixtureã®æ´»ç”¨

```python
# tests/conftest.py

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
from sqlalchemy import create_engine


# ========================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£Fixture
# ========================================

@pytest.fixture(scope="session")
def test_database_url():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URL"""
    return "postgresql://test:test@localhost:5433/test_db"


@pytest.fixture(scope="session")
def test_engine(test_database_url):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³"""
    engine = create_engine(test_database_url)
    
    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    from alembic import command
    from alembic.config import Config
    alembic_cfg = Config("alembic.ini")
    command.upgrade(alembic_cfg, "head")
    
    yield engine
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    command.downgrade(alembic_cfg, "base")
    engine.dispose()


@pytest.fixture
def db_session(test_engine):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³"""
    from sqlalchemy.orm import sessionmaker
    Session = sessionmaker(bind=test_engine)
    session = Session()
    
    yield session
    
    # ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
    session.rollback()
    session.close()


# ========================================
# ãƒ‡ãƒ¼ã‚¿é–¢é€£Fixture
# ========================================

@pytest.fixture
def sample_timeseries_data():
    """ã‚µãƒ³ãƒ—ãƒ«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿"""
    return pd.DataFrame({
        'unique_id': ['A'] * 100 + ['B'] * 100,
        'ds': pd.date_range('2025-01-01', periods=100).tolist() * 2,
        'y': np.random.randn(200).cumsum()
    })


@pytest.fixture
def sample_data_with_exog():
    """å¤–ç”Ÿå¤‰æ•°ã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿"""
    dates = pd.date_range('2025-01-01', periods=100)
    return pd.DataFrame({
        'unique_id': ['A'] * 100,
        'ds': dates,
        'y': np.random.randn(100).cumsum(),
        'temperature': np.random.uniform(10, 30, 100),
        'is_holiday': np.random.choice([0, 1], 100)
    })


@pytest.fixture
def large_dataset():
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    n_series = 1000
    n_points = 365
    
    data = []
    for i in range(n_series):
        data.append(pd.DataFrame({
            'unique_id': [f'series_{i}'] * n_points,
            'ds': pd.date_range('2024-01-01', periods=n_points),
            'y': np.random.randn(n_points).cumsum()
        }))
    
    return pd.concat(data, ignore_index=True)


# ========================================
# ãƒ‘ã‚¹é–¢é€£Fixture
# ========================================

@pytest.fixture
def test_data_dir(tmp_path):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    data_dir = tmp_path / "data"
    data_dir.mkdir()
    return data_dir


@pytest.fixture
def test_model_dir(tmp_path):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    model_dir = tmp_path / "models"
    model_dir.mkdir()
    return model_dir


@pytest.fixture
def test_output_dir(tmp_path):
    """ãƒ†ã‚¹ãƒˆç”¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    output_dir = tmp_path / "outputs"
    output_dir.mkdir()
    return output_dir


# ========================================
# è¨­å®šé–¢é€£Fixture
# ========================================

@pytest.fixture
def test_config(test_data_dir, test_model_dir, test_output_dir):
    """ãƒ†ã‚¹ãƒˆç”¨è¨­å®š"""
    from nf_auto_runner.config import PathConfig, ExecutionConfig
    
    return {
        'path': PathConfig(
            data_dir=str(test_data_dir),
            model_dir=str(test_model_dir),
            output_dir=str(test_output_dir)
        ),
        'execution': ExecutionConfig(
            max_parallel_runs=2,
            backend='cpu'
        )
    }


# ========================================
# ãƒ¢ãƒƒã‚¯é–¢é€£Fixture
# ========================================

@pytest.fixture
def mock_mlflow():
    """MLflowã®ãƒ¢ãƒƒã‚¯"""
    from unittest.mock import MagicMock, patch
    
    with patch('mlflow.log_metric') as mock_log_metric, \
         patch('mlflow.log_param') as mock_log_param, \
         patch('mlflow.log_artifact') as mock_log_artifact:
        
        yield {
            'log_metric': mock_log_metric,
            'log_param': mock_log_param,
            'log_artifact': mock_log_artifact
        }


@pytest.fixture
def mock_ray():
    """Rayã®ãƒ¢ãƒƒã‚¯"""
    from unittest.mock import MagicMock, patch
    
    with patch('ray.init') as mock_init, \
         patch('ray.shutdown') as mock_shutdown:
        
        yield {
            'init': mock_init,
            'shutdown': mock_shutdown
        }
```

---

### 7.2 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ

#### 7.2.1 fakerã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

```python
# tests/utils/data_generator.py

from faker import Faker
import pandas as pd
import numpy as np
from datetime import datetime, timedelta


class TimeSeriesDataGenerator:
    """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    def __init__(self, seed: int = 42):
        """
        åˆæœŸåŒ–
        
        Args:
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        self.faker = Faker()
        Faker.seed(seed)
        np.random.seed(seed)
    
    def generate_simple_series(
        self,
        n_points: int = 100,
        freq: str = 'D',
        trend: float = 0.1,
        seasonality: bool = True,
        noise_level: float = 0.1
    ) -> pd.DataFrame:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Args:
            n_points: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°
            freq: é »åº¦
            trend: ãƒˆãƒ¬ãƒ³ãƒ‰ä¿‚æ•°
            seasonality: å­£ç¯€æ€§ã‚’å«ã‚€ã‹
            noise_level: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸDataFrame
        """
        dates = pd.date_range(
            start='2025-01-01',
            periods=n_points,
            freq=freq
        )
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰
        trend_component = np.arange(n_points) * trend
        
        # å­£ç¯€æ€§
        if seasonality:
            seasonal_component = 10 * np.sin(
                2 * np.pi * np.arange(n_points) / 365
            )
        else:
            seasonal_component = 0
        
        # ãƒã‚¤ã‚º
        noise = np.random.normal(0, noise_level, n_points)
        
        # åˆæˆ
        y = trend_component + seasonal_component + noise
        
        return pd.DataFrame({
            'unique_id': ['series_1'] * n_points,
            'ds': dates,
            'y': y
        })
    
    def generate_multiple_series(
        self,
        n_series: int = 10,
        n_points: int = 100,
        **kwargs
    ) -> pd.DataFrame:
        """
        è¤‡æ•°ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Args:
            n_series: ç³»åˆ—æ•°
            n_points: å„ç³»åˆ—ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°
            **kwargs: generate_simple_seriesã¸ã®è¿½åŠ å¼•æ•°
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸDataFrame
        """
        series_list = []
        
        for i in range(n_series):
            df = self.generate_simple_series(n_points, **kwargs)
            df['unique_id'] = f'series_{i}'
            series_list.append(df)
        
        return pd.concat(series_list, ignore_index=True)
    
    def generate_with_exogenous(
        self,
        n_points: int = 100,
        n_exog: int = 3
    ) -> pd.DataFrame:
        """
        å¤–ç”Ÿå¤‰æ•°ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Args:
            n_points: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°
            n_exog: å¤–ç”Ÿå¤‰æ•°ã®æ•°
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸDataFrame
        """
        base_df = self.generate_simple_series(n_points)
        
        # å¤–ç”Ÿå¤‰æ•°ã‚’è¿½åŠ 
        for i in range(n_exog):
            base_df[f'exog_{i}'] = np.random.randn(n_points)
        
        return base_df


# ========================================
# ä½¿ç”¨ä¾‹
# ========================================

# tests/unit/data/test_with_generated_data.py

import pytest
from tests.utils.data_generator import TimeSeriesDataGenerator


@pytest.fixture
def data_generator():
    """ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    return TimeSeriesDataGenerator(seed=42)


def test_with_generated_data(data_generator):
    """ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚¹ãƒˆ"""
    # Arrange
    data = data_generator.generate_simple_series(n_points=100)
    
    # Act
    from nf_auto_runner.data import DataPreprocessor
    preprocessor = DataPreprocessor()
    result = preprocessor.preprocess(data)
    
    # Assert
    assert len(result) == 100
    assert not result['y'].isnull().any()
```

---

### 7.3 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

#### 7.3.1 DVCï¼ˆData Version Controlï¼‰ã®ä½¿ç”¨

```bash
# DVCã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install dvc

# DVCåˆæœŸåŒ–
dvc init

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’DVCã§ç®¡ç†
dvc add tests/data/sample_data.csv

# ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
dvc remote add -d storage s3://my-bucket/dvc-storage

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’push
dvc push

# ä»–ã®ç’°å¢ƒã§pull
dvc pull
```

---

## 8. ãƒ†ã‚¹ãƒˆç’°å¢ƒæ§‹æˆ

### 8.1 ç’°å¢ƒã®ç¨®é¡

#### 8.1.1 ç’°å¢ƒæ§‹æˆ

| ç’°å¢ƒ | ç”¨é€” | æ§‹æˆ | ãƒ‡ãƒ¼ã‚¿ |
|-----|------|------|-------|
| **Local Dev** | é–‹ç™ºè€…ã®ãƒ­ãƒ¼ã‚«ãƒ« | Docker Compose | æœ€å°é™ |
| **CI** | CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | GitHub Actions | ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ |
| **Test** | çµ±åˆãƒ†ã‚¹ãƒˆ | Kubernetes | åŒ¿ååŒ–æ¸ˆã¿ |
| **Staging** | ãƒªãƒªãƒ¼ã‚¹å‰æ¤œè¨¼ | æœ¬ç•ªé¡ä¼¼ | æœ¬ç•ªé¡ä¼¼ |
| **Production** | æœ¬ç•ªç’°å¢ƒ | é«˜å¯ç”¨æ€§æ§‹æˆ | æœ¬ç•ªãƒ‡ãƒ¼ã‚¿ |

---

#### 8.1.2 Docker Composeã‚’ä½¿ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ

```yaml
# docker-compose.test.yml

version: '3.8'

services:
  # PostgreSQLï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
  postgres_test:
    image: postgres:14
    environment:
      POSTGRES_DB: test_db
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
    ports:
      - "5433:5432"
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test"]
      interval: 5s
      timeout: 5s
      retries: 5
  
  # MLflowï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
  mlflow_test:
    image: ghcr.io/mlflow/mlflow:latest
    environment:
      BACKEND_STORE_URI: postgresql://test:test@postgres_test:5432/test_db
      ARTIFACT_ROOT: /mlflow/artifacts
    ports:
      - "5001:5000"
    volumes:
      - mlflow_test_artifacts:/mlflow/artifacts
    depends_on:
      postgres_test:
        condition: service_healthy
  
  # Ray Headï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
  ray_head_test:
    image: rayproject/ray:latest
    command: ray start --head --dashboard-host=0.0.0.0
    ports:
      - "6380:6379"
      - "8266:8265"
    environment:
      RAY_memory_monitor_refresh_ms: 0
  
  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚³ãƒ³ãƒ†ãƒŠ
  test_runner:
    build:
      context: .
      dockerfile: Dockerfile.test
    depends_on:
      - postgres_test
      - mlflow_test
      - ray_head_test
    environment:
      DATABASE_URL: postgresql://test:test@postgres_test:5432/test_db
      MLFLOW_TRACKING_URI: http://mlflow_test:5000
      RAY_ADDRESS: ray://ray_head_test:10001
    volumes:
      - .:/app
      - test_cache:/app/.pytest_cache
    command: pytest tests/ -v --cov=src --cov-report=html

volumes:
  postgres_test_data:
  mlflow_test_artifacts:
  test_cache:
```

**ä½¿ç”¨æ–¹æ³•**:

```bash
# ãƒ†ã‚¹ãƒˆç’°å¢ƒèµ·å‹•
docker-compose -f docker-compose.test.yml up -d

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
docker-compose -f docker-compose.test.yml run test_runner

# ç’°å¢ƒåœæ­¢
docker-compose -f docker-compose.test.yml down -v
```

---

### 8.2 ãƒ†ã‚¹ãƒˆç’°å¢ƒã®åˆ†é›¢

#### 8.2.1 pytest-envã§ã®ç’°å¢ƒå¤‰æ•°ç®¡ç†

```ini
# pytest.ini

[pytest]
env_files =
    .env.test

[pytest-env]
DATABASE_URL = postgresql://test:test@localhost:5433/test_db
MLFLOW_TRACKING_URI = http://localhost:5001
RAY_ADDRESS = ray://localhost:10001
ENABLE_GPU = false
LOG_LEVEL = DEBUG
```

---

#### 8.2.2 ç’°å¢ƒã”ã¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```python
# tests/config.py

from dataclasses import dataclass
import os


@dataclass
class TestConfig:
    """ãƒ†ã‚¹ãƒˆç’°å¢ƒè¨­å®š"""
    
    database_url: str
    mlflow_tracking_uri: str
    ray_address: str
    enable_gpu: bool
    log_level: str
    
    @classmethod
    def from_env(cls, env: str = "local") -> "TestConfig":
        """
        ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            env: ç’°å¢ƒåï¼ˆlocal, ci, testï¼‰
        
        Returns:
            TestConfig ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        configs = {
            'local': cls(
                database_url="postgresql://test:test@localhost:5433/test_db",
                mlflow_tracking_uri="http://localhost:5001",
                ray_address="ray://localhost:10001",
                enable_gpu=False,
                log_level="DEBUG"
            ),
            'ci': cls(
                database_url="postgresql://test:test@postgres:5432/test_db",
                mlflow_tracking_uri="http://mlflow:5000",
                ray_address="ray://ray_head:10001",
                enable_gpu=False,
                log_level="INFO"
            ),
            'test': cls(
                database_url=os.getenv("DATABASE_URL"),
                mlflow_tracking_uri=os.getenv("MLFLOW_TRACKING_URI"),
                ray_address=os.getenv("RAY_ADDRESS"),
                enable_gpu=os.getenv("ENABLE_GPU", "false").lower() == "true",
                log_level=os.getenv("LOG_LEVEL", "INFO")
            )
        }
        
        return configs.get(env, configs['local'])


# ========================================
# ä½¿ç”¨ä¾‹
# ========================================

# tests/conftest.py

@pytest.fixture(scope="session")
def test_config():
    """ãƒ†ã‚¹ãƒˆè¨­å®š"""
    env = os.getenv("TEST_ENV", "local")
    return TestConfig.from_env(env)
```

---

## 9. ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–

### 9.1 Makefileã§ã®ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–

```makefile
# Makefile

.PHONY: test test-unit test-integration test-e2e test-performance
.PHONY: coverage lint typecheck format check
.PHONY: test-watch test-parallel test-slow

# ========================================
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
# ========================================

# ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
test:
	pytest tests/ -v

# å˜ä½“ãƒ†ã‚¹ãƒˆã®ã¿
test-unit:
	pytest tests/unit/ -v

# çµ±åˆãƒ†ã‚¹ãƒˆã®ã¿
test-integration:
	pytest tests/integration/ -v

# E2Eãƒ†ã‚¹ãƒˆã®ã¿
test-e2e:
	pytest tests/e2e/ -v --maxfail=1

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ã¿
test-performance:
	pytest tests/ -v -m performance

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®ã¿
test-security:
	pytest tests/ -v -m security

# é…ã„ãƒ†ã‚¹ãƒˆã‚’é™¤å¤–
test-fast:
	pytest tests/ -v -m "not slow"

# ä¸¦åˆ—å®Ÿè¡Œ
test-parallel:
	pytest tests/ -v -n auto

# ã‚¦ã‚©ãƒƒãƒãƒ¢ãƒ¼ãƒ‰
test-watch:
	pytest-watch tests/

# ========================================
# ã‚«ãƒãƒ¬ãƒƒã‚¸
# ========================================

# ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
coverage:
	pytest tests/ --cov=src/nf_auto_runner --cov-report=html --cov-report=term

# ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
coverage-report:
	open htmlcov/index.html  # macOS
	# xdg-open htmlcov/index.html  # Linux

# ã‚«ãƒãƒ¬ãƒƒã‚¸è©³ç´°
coverage-detail:
	pytest tests/ --cov=src/nf_auto_runner --cov-report=term-missing

# ========================================
# é™çš„è§£æ
# ========================================

# Lintãƒã‚§ãƒƒã‚¯
lint:
	pylint src/nf_auto_runner/
	flake8 src/ tests/

# å‹ãƒã‚§ãƒƒã‚¯
typecheck:
	mypy src/nf_auto_runner/ --strict

# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
format-check:
	black --check src/ tests/
	isort --check-only src/ tests/

# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨
format:
	black src/ tests/
	isort src/ tests/

# ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯
check: format-check lint typecheck test

# ========================================
# CI/CDç”¨
# ========================================

# CIç”¨ãƒ†ã‚¹ãƒˆ
ci-test:
	pytest tests/ -v --cov=src/nf_auto_runner --cov-report=xml --cov-report=term

# ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
mutation-test:
	mutmut run --paths-to-mutate src/nf_auto_runner/
	mutmut results

# ========================================
# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
# ========================================

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
clean:
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type d -name ".pytest_cache" -exec rm -rf {} +
	find . -type d -name ".mypy_cache" -exec rm -rf {} +
	find . -type d -name "htmlcov" -exec rm -rf {} +
	find . -type f -name ".coverage" -delete

# ========================================
# ãƒ˜ãƒ«ãƒ—
# ========================================

help:
	@echo "Available targets:"
	@echo "  test              - ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"
	@echo "  test-unit         - å˜ä½“ãƒ†ã‚¹ãƒˆã®ã¿"
	@echo "  test-integration  - çµ±åˆãƒ†ã‚¹ãƒˆã®ã¿"
	@echo "  test-e2e          - E2Eãƒ†ã‚¹ãƒˆã®ã¿"
	@echo "  test-performance  - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ã¿"
	@echo "  test-parallel     - ä¸¦åˆ—å®Ÿè¡Œ"
	@echo "  coverage          - ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š"
	@echo "  lint              - Lintãƒã‚§ãƒƒã‚¯"
	@echo "  typecheck         - å‹ãƒã‚§ãƒƒã‚¯"
	@echo "  format            - ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"
	@echo "  check             - ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯"
	@echo "  clean             - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"
```

---

### 9.2 pytestè¨­å®š

#### 9.2.1 pytest.ini

```ini
# pytest.ini

[pytest]
# ========================================
# åŸºæœ¬è¨­å®š
# ========================================
minversion = 7.0
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# ========================================
# ãƒãƒ¼ã‚«ãƒ¼
# ========================================
markers =
    unit: å˜ä½“ãƒ†ã‚¹ãƒˆ
    integration: çµ±åˆãƒ†ã‚¹ãƒˆ
    e2e: E2Eãƒ†ã‚¹ãƒˆ
    performance: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    security: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
    slow: å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ãƒ†ã‚¹ãƒˆï¼ˆ>1åˆ†ï¼‰
    gpu: GPUå¿…é ˆã®ãƒ†ã‚¹ãƒˆ
    requires_database: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¿…é ˆã®ãƒ†ã‚¹ãƒˆ
    requires_mlflow: MLflowå¿…é ˆã®ãƒ†ã‚¹ãƒˆ
    requires_ray: Rayå¿…é ˆã®ãƒ†ã‚¹ãƒˆ

# ========================================
# ã‚ªãƒ—ã‚·ãƒ§ãƒ³
# ========================================
addopts =
    -ra
    --strict-markers
    --strict-config
    --showlocals
    --tb=short
    --disable-warnings

# ========================================
# ã‚«ãƒãƒ¬ãƒƒã‚¸
# ========================================
[coverage:run]
source = src/nf_auto_runner
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */site-packages/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstract

[coverage:html]
directory = htmlcov
```

---

### 9.3 pytest-watchè¨­å®š

```ini
# pytest.ini ã«è¿½åŠ 

[pytest-watch]
# ã‚¦ã‚©ãƒƒãƒå¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
patterns =
    *.py

# ç„¡è¦–ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
ignore =
    *.pyc
    __pycache__
    .git

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¼•æ•°
runner = pytest
options = -v
```

**ä½¿ç”¨æ–¹æ³•**:

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install pytest-watch

# å®Ÿè¡Œ
ptw  # ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’ç›£è¦–

# ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿
ptw tests/unit/

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ã
ptw -- --cov=src/nf_auto_runner
```

---

## 10. å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨KPI

### 10.1 ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 10.1.1 ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• | é »åº¦ |
|----------|-------|---------|------|
| **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸** | >90% | pytest-cov | CIæ¯ |
| **ãƒ†ã‚¹ãƒˆæˆåŠŸç‡** | >99% | CI/CD | CIæ¯ |
| **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“** | <10åˆ† | CI/CD | CIæ¯ |
| **ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢** | >80% | mutmut | é€±æ¬¡ |
| **æ¬ é™¥æ¤œå‡ºç‡** | >95% | ãƒã‚°ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° | æœˆæ¬¡ |
| **æ¬ é™¥ä¿®æ­£æ™‚é–“** | <2æ—¥ | ãƒã‚°ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚° | æœˆæ¬¡ |

---

#### 10.1.2 å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```python
# scripts/generate_quality_report.py

import subprocess
import json
from datetime import datetime
from pathlib import Path


def generate_quality_report():
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'metrics': {}
    }
    
    # 1. ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
    result = subprocess.run(
        ['pytest', 'tests/', '--cov=src', '--cov-report=json'],
        capture_output=True
    )
    with open('.coverage.json', 'r') as f:
        coverage_data = json.load(f)
        report['metrics']['coverage'] = coverage_data['totals']['percent_covered']
    
    # 2. Pylintã‚¹ã‚³ã‚¢
    result = subprocess.run(
        ['pylint', 'src/', '--output-format=json'],
        capture_output=True,
        text=True
    )
    pylint_data = json.loads(result.stdout)
    report['metrics']['pylint_score'] = 10 - len(pylint_data) * 0.1
    
    # 3. MyPyã‚¨ãƒ©ãƒ¼æ•°
    result = subprocess.run(
        ['mypy', 'src/', '--strict'],
        capture_output=True,
        text=True
    )
    mypy_errors = len(result.stdout.split('\n'))
    report['metrics']['mypy_errors'] = mypy_errors
    
    # 4. ãƒ†ã‚¹ãƒˆæ•°
    result = subprocess.run(
        ['pytest', 'tests/', '--collect-only', '-q'],
        capture_output=True,
        text=True
    )
    test_count = len([l for l in result.stdout.split('\n') if 'test_' in l])
    report['metrics']['test_count'] = test_count
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_path = Path('reports/quality_report.json')
    report_path.parent.mkdir(exist_ok=True)
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"Quality report generated: {report_path}")
    print(json.dumps(report['metrics'], indent=2))
    
    return report


if __name__ == '__main__':
    generate_quality_report()
```

---

### 10.2 ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¯è¦–åŒ–

#### 10.2.1 Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

```yaml
# monitoring/grafana/dashboards/test_metrics.json

{
  "dashboard": {
    "title": "Test Metrics Dashboard",
    "panels": [
      {
        "title": "Test Coverage Trend",
        "type": "graph",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "test_coverage_percentage"
          }
        ]
      },
      {
        "title": "Test Success Rate",
        "type": "stat",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "test_success_rate"
          }
        ]
      },
      {
        "title": "Test Execution Time",
        "type": "graph",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "test_execution_time_seconds"
          }
        ]
      },
      {
        "title": "Failed Tests",
        "type": "table",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "failed_tests"
          }
        ]
      }
    ]
  }
}
```

---

## 11. ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### 11.1 ãƒªã‚¹ã‚¯åˆ†æ

#### 11.1.1 ãƒªã‚¹ã‚¯ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | å½±éŸ¿åº¦ | ç™ºç”Ÿç¢ºç‡ | ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ« | ãƒ†ã‚¹ãƒˆå„ªå…ˆåº¦ |
|-------------|-------|---------|------------|------------|
| **Dataå±¤** | é«˜ | ä¸­ | é«˜ | é«˜ |
| **Modelå±¤** | é«˜ | é«˜ | éå¸¸ã«é«˜ | éå¸¸ã«é«˜ |
| **Executionå±¤** | é«˜ | ä¸­ | é«˜ | é«˜ |
| **Configurationå±¤** | ä¸­ | ä½ | ä¸­ | ä¸­ |
| **Loggingå±¤** | ä½ | ä½ | ä½ | ä½ |

**å½±éŸ¿åº¦**: ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å½±éŸ¿ã®å¤§ãã•
**ç™ºç”Ÿç¢ºç‡**: æ¬ é™¥ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§
**ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«**: å½±éŸ¿åº¦ Ã— ç™ºç”Ÿç¢ºç‡

---

#### 11.1.2 ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆé…åˆ†

```
ãƒ†ã‚¹ãƒˆãƒªã‚½ãƒ¼ã‚¹ã®é…åˆ†:

é«˜ãƒªã‚¹ã‚¯ï¼ˆ60%ã®ãƒªã‚½ãƒ¼ã‚¹ï¼‰:
â”œâ”€â”€ Modelå±¤: 30%
â”‚   â”œâ”€â”€ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®æ­£ç¢ºæ€§
â”‚   â”œâ”€â”€ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
â”‚   â””â”€â”€ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
â”œâ”€â”€ Dataå±¤: 20%
â”‚   â”œâ”€â”€ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
â”‚   â”œâ”€â”€ å‰å‡¦ç†
â”‚   â””â”€â”€ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
â””â”€â”€ Executionå±¤: 10%
    â”œâ”€â”€ ä¸¦åˆ—å®Ÿè¡Œ
    â””â”€â”€ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†

ä¸­ãƒªã‚¹ã‚¯ï¼ˆ30%ã®ãƒªã‚½ãƒ¼ã‚¹ï¼‰:
â”œâ”€â”€ Configurationå±¤: 15%
â””â”€â”€ Serviceå±¤: 15%

ä½ãƒªã‚¹ã‚¯ï¼ˆ10%ã®ãƒªã‚½ãƒ¼ã‚¹ï¼‰:
â”œâ”€â”€ Loggingå±¤: 5%
â””â”€â”€ UIå±¤: 5%
```

---

### 11.2 ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ãƒ†ã‚¹ãƒˆ

#### 11.2.1 ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã®ç‰¹å®š

```python
# tests/critical_path/test_end_to_end_critical.py

import pytest
from nf_auto_runner.app import Orchestrator


@pytest.mark.critical
class TestCriticalPath:
    """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã®ãƒ†ã‚¹ãƒˆ"""
    
    def test_critical_path_data_to_prediction(
        self,
        test_database,
        sample_data,
        test_config
    ):
        """
        ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹: ãƒ‡ãƒ¼ã‚¿â†’å­¦ç¿’â†’äºˆæ¸¬â†’ä¿å­˜
        
        ã“ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆã€ã‚·ã‚¹ãƒ†ãƒ ã®ä¸»è¦æ©Ÿèƒ½ãŒå‹•ä½œã—ãªã„
        """
        # Arrange
        orchestrator = Orchestrator(
            database=test_database,
            config=test_config
        )
        
        # Act: ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹å…¨ä½“ã‚’å®Ÿè¡Œ
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data = orchestrator.load_data(sample_data)
        assert data is not None, "ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—"
        
        # 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = orchestrator.train_model(
            data=data,
            model_name="AutoNHITS",
            hyperparameters={"h": 7}
        )
        assert model is not None, "ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«å¤±æ•—"
        
        # 3. äºˆæ¸¬å®Ÿè¡Œ
        predictions = orchestrator.predict(
            model=model,
            data=data,
            horizon=7
        )
        assert predictions is not None, "äºˆæ¸¬å®Ÿè¡Œã«å¤±æ•—"
        assert len(predictions) > 0, "äºˆæ¸¬çµæœãŒç©º"
        
        # 4. çµæœä¿å­˜
        save_result = orchestrator.save_results(
            model=model,
            predictions=predictions
        )
        assert save_result.success, "çµæœä¿å­˜ã«å¤±æ•—"
        
        # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¢ºèª
        saved_run = test_database.query(
            "SELECT * FROM runs WHERE run_id = ?",
            (save_result.run_id,)
        ).fetchone()
        assert saved_run is not None, "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ãªã„"
    
    @pytest.mark.critical
    def test_critical_path_parallel_execution(
        self,
        test_database,
        sample_data,
        test_config
    ):
        """
        ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹: ä¸¦åˆ—å®Ÿè¡Œ
        
        ã“ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸå ´åˆã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«å•é¡Œã‚ã‚Š
        """
        # Arrange
        orchestrator = Orchestrator(
            database=test_database,
            config=test_config
        )
        
        execution_plans = [
            {"model": "AutoNHITS", "h": 7},
            {"model": "AutoLSTM", "h": 7},
            {"model": "AutoTFT", "h": 7},
        ]
        
        # Act
        results = orchestrator.execute_parallel(
            data=sample_data,
            execution_plans=execution_plans
        )
        
        # Assert
        assert len(results) == 3, "ã™ã¹ã¦ã®å®Ÿè¡Œè¨ˆç”»ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„"
        assert all(r.status == "completed" for r in results), "å®Ÿè¡Œã«å¤±æ•—"
```

---

## 12. CI/CDçµ±åˆ

### 12.1 GitHub Actionsè¨­å®š

#### 12.1.1 ãƒ†ã‚¹ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```yaml
# .github/workflows/test.yml

name: Test

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  # ========================================
  # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
  # ========================================
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements-dev.txt') }}
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Run unit tests
        run: |
          pytest tests/unit/ -v \
            --cov=src/nf_auto_runner \
            --cov-report=xml \
            --cov-report=term
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
  
  # ========================================
  # çµ±åˆãƒ†ã‚¹ãƒˆ
  # ========================================
  integration-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Run database migrations
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
        run: |
          alembic upgrade head
      
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://test:test@localhost:5432/test_db
        run: |
          pytest tests/integration/ -v \
            --cov=src/nf_auto_runner \
            --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: integration
  
  # ========================================
  # é™çš„è§£æ
  # ========================================
  static-analysis:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pylint flake8 mypy black isort bandit
      
      - name: Run pylint
        run: pylint src/nf_auto_runner/ --fail-under=8.5
      
      - name: Run flake8
        run: flake8 src/ tests/ --max-line-length=100
      
      - name: Run mypy
        run: mypy src/nf_auto_runner/ --strict
      
      - name: Check formatting
        run: |
          black --check src/ tests/
          isort --check-only src/ tests/
      
      - name: Run security scan
        run: bandit -r src/nf_auto_runner/ -f json -o bandit-report.json
      
      - name: Upload bandit report
        uses: actions/upload-artifact@v3
        with:
          name: bandit-report
          path: bandit-report.json
  
  # ========================================
  # E2Eãƒ†ã‚¹ãƒˆ
  # ========================================
  e2e-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Start test environment
        run: docker-compose -f docker-compose.test.yml up -d
      
      - name: Wait for services
        run: |
          timeout 60 bash -c 'until docker-compose -f docker-compose.test.yml ps | grep healthy; do sleep 5; done'
      
      - name: Run E2E tests
        run: |
          docker-compose -f docker-compose.test.yml run test_runner \
            pytest tests/e2e/ -v --maxfail=1
      
      - name: Cleanup
        if: always()
        run: docker-compose -f docker-compose.test.yml down -v
  
  # ========================================
  # ãƒ†ã‚¹ãƒˆçµæœé›†ç´„
  # ========================================
  test-summary:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, static-analysis]
    if: always()
    
    steps:
      - name: Test Summary
        run: |
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Static Analysis: ${{ needs.static-analysis.result }}" >> $GITHUB_STEP_SUMMARY
```

---

### 12.2 å“è³ªã‚²ãƒ¼ãƒˆè¨­å®š

#### 12.2.1 SonarQubeã¨ã®çµ±åˆ

```yaml
# .github/workflows/quality-gate.yml

name: Quality Gate

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  sonarqube:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Run tests with coverage
        run: |
          pytest tests/ -v \
            --cov=src/nf_auto_runner \
            --cov-report=xml
      
      - name: SonarQube Scan
        uses: sonarsource/sonarqube-scan-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        with:
          args: >
            -Dsonar.projectKey=time-series-forecasting-system
            -Dsonar.python.coverage.reportPaths=coverage.xml
            -Dsonar.qualitygate.wait=true
      
      - name: Quality Gate Status
        run: |
          # SonarQubeã®å“è³ªã‚²ãƒ¼ãƒˆçµæœã‚’ç¢ºèª
          # å¤±æ•—ã—ãŸå ´åˆã¯ãƒ“ãƒ«ãƒ‰ã‚’å¤±æ•—ã•ã›ã‚‹
          echo "Quality Gate Status: Passed"
```

---

### 12.3 ãƒãƒ¼ã‚¸å‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```markdown
## PR Checklist

### ã‚³ãƒ¼ãƒ‰å“è³ª
- [ ] Pylintã‚¹ã‚³ã‚¢ â‰¥ 8.5
- [ ] MyPy strict mode ãƒ‘ã‚¹
- [ ] Black/isort ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé©ç”¨æ¸ˆã¿
- [ ] Flake8 ã‚¨ãƒ©ãƒ¼ãªã—

### ãƒ†ã‚¹ãƒˆ
- [ ] æ–°è¦ãƒ†ã‚¹ãƒˆè¿½åŠ ï¼ˆæ–°æ©Ÿèƒ½ã®å ´åˆï¼‰
- [ ] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹
- [ ] ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ >90%
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹
- [ ] E2Eãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [ ] docstringè¿½åŠ /æ›´æ–°
- [ ] READMEæ›´æ–°ï¼ˆå¿…è¦ãªå ´åˆï¼‰
- [ ] APIä»•æ§˜æ›´æ–°ï¼ˆå¿…è¦ãªå ´åˆï¼‰
- [ ] CHANGELOGæ›´æ–°

### ãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰

### ãã®ä»–
- [ ] ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆDBã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´ã®å ´åˆï¼‰
- [ ] ç ´å£Šçš„å¤‰æ›´ã®æ–‡æ›¸åŒ–ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- [ ] ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆè¨˜è¼‰ï¼ˆå¿…è¦ãªå ´åˆï¼‰
```

---

## 13. ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

### 13.1 ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

#### 13.1.1 ä¸»è¦ãƒ„ãƒ¼ãƒ«

| ãƒ„ãƒ¼ãƒ« | ç”¨é€” | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« |
|-------|------|------------|
| **pytest** | ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | `pip install pytest` |
| **pytest-cov** | ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š | `pip install pytest-cov` |
| **pytest-xdist** | ä¸¦åˆ—å®Ÿè¡Œ | `pip install pytest-xdist` |
| **pytest-mock** | ãƒ¢ãƒƒã‚¯ä½œæˆ | `pip install pytest-mock` |
| **pytest-asyncio** | éåŒæœŸãƒ†ã‚¹ãƒˆ | `pip install pytest-asyncio` |
| **pytest-timeout** | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š | `pip install pytest-timeout` |
| **pytest-benchmark** | ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ | `pip install pytest-benchmark` |
| **pytest-watch** | ã‚¦ã‚©ãƒƒãƒãƒ¢ãƒ¼ãƒ‰ | `pip install pytest-watch` |

---

#### 13.1.2 æ¨å¥¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³

```bash
# requirements-test.txt

pytest>=7.4.0
pytest-cov>=4.1.0
pytest-xdist>=3.3.1
pytest-mock>=3.11.1
pytest-asyncio>=0.21.1
pytest-timeout>=2.1.0
pytest-benchmark>=4.0.0
pytest-watch>=4.2.0
pytest-sugar>=0.9.7  # è¦‹ã‚„ã™ã„å‡ºåŠ›
pytest-clarity>=1.0.1  # è©³ç´°ãªã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³å‡ºåŠ›
pytest-testmon>=2.0.0  # å¤‰æ›´ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
pytest-randomly>=3.13.0  # ãƒ©ãƒ³ãƒ€ãƒ é †åºå®Ÿè¡Œ
```

---

### 13.2 ãƒ¢ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«

#### 13.2.1 unittestãƒ¢ãƒƒã‚¯ã®ä½¿ç”¨

```python
from unittest.mock import Mock, MagicMock, patch, call


# ========================================
# Mockä½¿ç”¨ä¾‹
# ========================================

def test_with_mock():
    """Mockã®åŸºæœ¬çš„ãªä½¿ç”¨"""
    # Arrange
    mock_service = Mock()
    mock_service.get_data.return_value = {"key": "value"}
    
    # Act
    result = mock_service.get_data("param")
    
    # Assert
    assert result == {"key": "value"}
    mock_service.get_data.assert_called_once_with("param")


# ========================================
# Patchä½¿ç”¨ä¾‹
# ========================================

@patch('nf_auto_runner.data.DataLoader')
def test_with_patch(mock_loader):
    """Patchã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒƒã‚¯"""
    # Arrange
    mock_loader.return_value.load_csv.return_value = pd.DataFrame()
    
    # Act
    from nf_auto_runner.service import DataService
    service = DataService()
    result = service.load_and_process("data.csv")
    
    # Assert
    assert isinstance(result, pd.DataFrame)
    mock_loader.return_value.load_csv.assert_called_once()


# ========================================
# MagicMockä½¿ç”¨ä¾‹
# ========================================

def test_with_magic_mock():
    """MagicMockã®ä½¿ç”¨ï¼ˆãƒã‚¸ãƒƒã‚¯ãƒ¡ã‚½ãƒƒãƒ‰å¯¾å¿œï¼‰"""
    # Arrange
    mock_obj = MagicMock()
    mock_obj.__len__.return_value = 10
    mock_obj.__getitem__.return_value = "item"
    
    # Act & Assert
    assert len(mock_obj) == 10
    assert mock_obj[0] == "item"
```

---

### 13.3 ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼

#### 13.3.1 fakerã®ä½¿ç”¨

```python
from faker import Faker
import pandas as pd


def generate_sample_data(n_rows: int = 100) -> pd.DataFrame:
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        n_rows: ç”Ÿæˆã™ã‚‹è¡Œæ•°
    
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸDataFrame
    """
    fake = Faker()
    Faker.seed(42)
    
    return pd.DataFrame({
        'unique_id': [fake.uuid4() for _ in range(n_rows)],
        'ds': [fake.date_time_between(start_date='-1y') for _ in range(n_rows)],
        'y': [fake.random_int(min=1, max=100) for _ in range(n_rows)],
        'category': [fake.random_element(['A', 'B', 'C']) for _ in range(n_rows)]
    })
```

---

#### 13.3.2 Hypothesisã®ä½¿ç”¨

```python
from hypothesis import given, strategies as st


@given(
    x=st.integers(min_value=0, max_value=100),
    y=st.integers(min_value=0, max_value=100)
)
def test_addition_commutative(x, y):
    """åŠ ç®—ã®äº¤æ›æ³•å‰‡ã®ãƒ†ã‚¹ãƒˆ"""
    assert x + y == y + x


@given(
    data=st.lists(
        st.floats(allow_nan=False, allow_infinity=False),
        min_size=1,
        max_size=1000
    )
)
def test_mean_calculation(data):
    """å¹³å‡è¨ˆç®—ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    import numpy as np
    
    mean = np.mean(data)
    
    # Properties:
    assert min(data) <= mean <= max(data)
```

---

## 14. ä»˜éŒ²

### 14.1 ãƒ†ã‚¹ãƒˆå‘½åè¦å‰‡

#### 14.1.1 å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# ãƒ‘ã‚¿ãƒ¼ãƒ³1: test_<method>_<scenario>_<expected>
def test_calculate_mae_with_valid_input_returns_float():
    """MAEè¨ˆç®—: æœ‰åŠ¹ãªå…¥åŠ› â†’ floatè¿”å´"""
    pass

# ãƒ‘ã‚¿ãƒ¼ãƒ³2: test_<method>_when_<condition>_then_<result>
def test_load_data_when_file_not_found_then_raises_error():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ â†’ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ"""
    pass

# ãƒ‘ã‚¿ãƒ¼ãƒ³3: test_<method>_should_<expected>
def test_validate_schema_should_return_true_for_valid_dataframe():
    """ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼: æœ‰åŠ¹ãªDataFrame â†’ Trueã‚’è¿”ã™"""
    pass

# ãƒ‘ã‚¿ãƒ¼ãƒ³4: given_when_then (BDDã‚¹ã‚¿ã‚¤ãƒ«)
def test_given_existing_experiment_when_duplicate_created_then_detected():
    """æ—¢å­˜å®Ÿé¨“ã‚ã‚Š & é‡è¤‡ä½œæˆ â†’ æ¤œå‡ºã•ã‚Œã‚‹"""
    pass
```

---

### 14.2 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³

#### 14.2.1 Builder Pattern

```python
class ExperimentBuilder:
    """å®Ÿé¨“ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ“ãƒ«ãƒ€ãƒ¼"""
    
    def __init__(self):
        self._experiment = {
            'experiment_id': 'exp_001',
            'model_name': 'AutoNHITS',
            'hyperparameters': {},
            'status': 'pending'
        }
    
    def with_id(self, experiment_id: str):
        self._experiment['experiment_id'] = experiment_id
        return self
    
    def with_model(self, model_name: str):
        self._experiment['model_name'] = model_name
        return self
    
    def with_hyperparameters(self, params: dict):
        self._experiment['hyperparameters'] = params
        return self
    
    def with_status(self, status: str):
        self._experiment['status'] = status
        return self
    
    def build(self):
        return self._experiment


# ä½¿ç”¨ä¾‹
def test_with_builder():
    """ãƒ“ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚¹ãƒˆ"""
    experiment = (
        ExperimentBuilder()
        .with_id("exp_test_001")
        .with_model("AutoLSTM")
        .with_hyperparameters({"h": 7, "input_size": 14})
        .with_status("completed")
        .build()
    )
    
    assert experiment['experiment_id'] == "exp_test_001"
```

---

### 14.3 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### 14.3.1 ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–

| å•é¡Œ | åŸå›  | è§£æ±ºç­– |
|-----|------|-------|
| **ãƒ†ã‚¹ãƒˆãŒé…ã„** | ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã€ãƒ•ã‚¡ã‚¤ãƒ«I/O | ãƒ¢ãƒƒã‚¯ä½¿ç”¨ã€ä¸¦åˆ—å®Ÿè¡Œ |
| **ãƒ†ã‚¹ãƒˆãŒä¸å®‰å®š** | å¤–éƒ¨ä¾å­˜ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°å•é¡Œ | å›ºå®šãƒ‡ãƒ¼ã‚¿ã€retryãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ |
| **ã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³** | ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸è¶³ | å¢ƒç•Œå€¤ã€ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹è¿½åŠ  |
| **ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯** | ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾å¿˜ã‚Œ | fixtureã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— |
| **ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼** | å…±æœ‰ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆ | ãƒ†ã‚¹ãƒˆåˆ†é›¢ã€ãƒ­ãƒƒã‚¯ä½¿ç”¨ |

---

#### 14.3.2 ãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«

```python
# pytest-pdb: ãƒ‡ãƒãƒƒã‚¬ãƒ¼
pytest tests/ --pdb  # å¤±æ•—æ™‚ã«ãƒ‡ãƒãƒƒã‚¬ãƒ¼èµ·å‹•

# pytest-sugar: è¦‹ã‚„ã™ã„å‡ºåŠ›
pip install pytest-sugar

# pytest-clarity: è©³ç´°ãªã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
pip install pytest-clarity

# pytest-html: HTMLãƒ¬ãƒãƒ¼ãƒˆ
pytest tests/ --html=report.html
```

---

### 14.4 å‚è€ƒè³‡æ–™

#### 14.4.1 å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- pytest: https://docs.pytest.org/
- Coverage.py: https://coverage.readthedocs.io/
- Hypothesis: https://hypothesis.readthedocs.io/
- unittest.mock: https://docs.python.org/3/library/unittest.mock.html

---

#### 14.4.2 æ¨å¥¨æ›¸ç±

- "Test Driven Development: By Example" by Kent Beck
- "Growing Object-Oriented Software, Guided by Tests" by Steve Freeman
- "The Art of Unit Testing" by Roy Osherove
- "Python Testing with pytest" by Brian Okken

---

#### 14.4.3 é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- ğŸ“„ `01_REQUIREMENTS_SPECIFICATION_DETAILED.md` - è¦ä»¶å®šç¾©
- ğŸ“„ `07_IMPLEMENTATION_GUIDE.md` - å®Ÿè£…ã‚¬ã‚¤ãƒ‰
- ğŸ“„ `08_CODING_STANDARDS.md` - ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„
- ğŸ“„ `10_DEPLOYMENT_GUIDE.md` - ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰

---

## âœ¨ ã¾ã¨ã‚

æœ¬ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æä¾›ã—ã¾ã™ã€‚

### ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ

- âœ… **éšå±¤çš„ãƒ†ã‚¹ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: å˜ä½“â†’çµ±åˆâ†’E2E
- âœ… **TDDå®Ÿè·µ**: ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã®é–‹ç™º
- âœ… **é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™**: >90%ã®ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸
- âœ… **è‡ªå‹•åŒ–å„ªå…ˆ**: CI/CDçµ±åˆã«ã‚ˆã‚‹ç¶™ç¶šçš„ãƒ†ã‚¹ãƒˆ
- âœ… **ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹**: é‡è¦åº¦ã«å¿œã˜ãŸãƒªã‚½ãƒ¼ã‚¹é…åˆ†
- âœ… **å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹**: ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®å“è³ªæ”¹å–„

### æˆåŠŸã®ãŸã‚ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ

1. **ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ**: å®Ÿè£…å‰ã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ã
2. **ç¶™ç¶šçš„å®Ÿè¡Œ**: CI/CDã§è‡ªå‹•å®Ÿè¡Œ
3. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–**: å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¶™ç¶šçš„ã«ç›£è¦–
4. **ãƒªã‚¹ã‚¯é‡è¦–**: é«˜ãƒªã‚¹ã‚¯é ˜åŸŸã«é‡ç‚¹çš„ã«ãƒ†ã‚¹ãƒˆ
5. **ãƒãƒ¼ãƒ æ–‡åŒ–**: ãƒ†ã‚¹ãƒˆã‚’æ›¸ãã“ã¨ã‚’å½“ãŸã‚Šå‰ã®æ–‡åŒ–ã«

---

**Good Testing! ğŸ§ª**

---
**End of Document**
