# è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆæ›¸
**Detailed Database Design for Time Series Forecasting System**

---

## ðŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±

| é …ç›® | å†…å®¹ |
|-----|------|
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«** | æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆæ›¸ |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³** | v1.0.0 |
| **ä½œæˆæ—¥** | 2025-11-03 |
| **æœ€çµ‚æ›´æ–°æ—¥** | 2025-11-03 |
| **å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ ** | NeuralForecast Auto Runner + Time Series Forecasting System |
| **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹** | PostgreSQL 14+ |
| **ORM** | SQLAlchemy 2.0+ |

---

## ç›®æ¬¡

1. [ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¦‚è¦](#1-ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¦‚è¦)
2. [ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©](#2-ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©)
3. [ERå›³](#3-erå›³)
4. [ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ](#4-ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ)
5. [ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»](#5-ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»)
6. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#6-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
7. [ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥](#7-ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥)
8. [ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£](#8-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£)
9. [é‹ç”¨æ‰‹é †](#9-é‹ç”¨æ‰‹é †)
10. [ä»˜éŒ²](#10-ä»˜éŒ²)

---

## 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¦‚è¦

### 1.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆ

#### 1.1.1 åŸºæœ¬æƒ…å ±

```yaml
Database:
  Name: ts_forecast_system
  Engine: PostgreSQL
  Version: 14+
  Encoding: UTF8
  Locale: en_US.UTF-8
  Timezone: UTC
  
Connection Pool:
  Min: 2
  Max: 20
  Overflow: 10
  Timeout: 30s
  Recycle: 3600s
```

---

#### 1.1.2 ã‚¹ã‚­ãƒ¼ãƒžæ§‹æˆ

| ã‚¹ã‚­ãƒ¼ãƒžå | ç”¨é€” | ä¸»è¦ãƒ†ãƒ¼ãƒ–ãƒ«æ•° |
|----------|------|--------------|
| **public** | ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ | 15 |
| **audit** | ç›£æŸ»ãƒ­ã‚° | 3 |
| **staging** | ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿ | 2 |
| **archive** | ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– | 5 |

---

### 1.2 è¨­è¨ˆåŽŸå‰‡

#### 1.2.1 æ­£è¦åŒ–ãƒ¬ãƒ™ãƒ«

- **ç¬¬3æ­£è¦å½¢ (3NF)** ã‚’åŸºæœ¬ã¨ã™ã‚‹
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãŒå¿…è¦ãªéƒ¨åˆ†ã¯**æ„å›³çš„ãªéžæ­£è¦åŒ–**ã‚’è¨±å®¹
- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã¯**ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°**ã‚’æ´»ç”¨

#### 1.2.2 å‘½åè¦å‰‡

```yaml
Tables:
  Format: snake_case
  Singular: false  # è¤‡æ•°å½¢ã‚’ä½¿ç”¨
  Prefix: ãªã—
  
Columns:
  Format: snake_case
  Reserved Words: å›žé¿
  
Primary Keys:
  Format: "{table_name}_id" or "id"
  Type: BIGSERIAL or UUID
  
Foreign Keys:
  Format: "{referenced_table}_id"
  
Indexes:
  Format: "idx_{table}_{columns}"
  
Constraints:
  Format: "ck_{table}_{column}_{condition}"
  
Sequences:
  Format: "{table}_{column}_seq"
```

---

### 1.3 ãƒ‡ãƒ¼ã‚¿åž‹ãƒãƒªã‚·ãƒ¼

| PostgreSQLåž‹ | ç”¨é€” | å‚™è€ƒ |
|-------------|------|------|
| **BIGSERIAL** | è‡ªå‹•å¢—åˆ†ID | ä¸»ã‚­ãƒ¼ã§ä½¿ç”¨ |
| **UUID** | ã‚°ãƒ­ãƒ¼ãƒãƒ«ID | åˆ†æ•£ç’°å¢ƒç”¨ |
| **TIMESTAMP** | æ—¥æ™‚ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãªã—ï¼‰ | UTCã§çµ±ä¸€ |
| **TIMESTAMPTZ** | æ—¥æ™‚ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚ã‚Šï¼‰ | ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ç”¨ |
| **NUMERIC(p,s)** | é«˜ç²¾åº¦æ•°å€¤ | é‡‘é¡ã€è©•ä¾¡æŒ‡æ¨™ |
| **DOUBLE PRECISION** | æµ®å‹•å°æ•°ç‚¹ | æ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
| **TEXT** | å¯å¤‰é•·æ–‡å­—åˆ— | é•·ã•åˆ¶é™ãªã— |
| **VARCHAR(n)** | åˆ¶é™ä»˜ãæ–‡å­—åˆ— | åå‰ã€ã‚³ãƒ¼ãƒ‰ |
| **JSONB** | JSON | æŸ”è»Ÿãªãƒ‡ãƒ¼ã‚¿ |
| **BOOLEAN** | çœŸå½å€¤ | ãƒ•ãƒ©ã‚° |
| **BYTEA** | ãƒã‚¤ãƒŠãƒª | ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå°ï¼‰ |

---

## 2. ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©

### 2.1 å®Ÿé¨“ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.1.1 experiments (å®Ÿé¨“)

**ç›®çš„**: å®Ÿé¨“ã®åŸºæœ¬æƒ…å ±ã‚’ç®¡ç†

```sql
CREATE TABLE experiments (
    -- ä¸»ã‚­ãƒ¼
    experiment_id BIGSERIAL PRIMARY KEY,
    
    -- åŸºæœ¬æƒ…å ±
    experiment_name VARCHAR(255) NOT NULL,
    experiment_description TEXT,
    experiment_type VARCHAR(50) NOT NULL DEFAULT 'training',
    
    -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    status VARCHAR(50) NOT NULL DEFAULT 'active',
    lifecycle_stage VARCHAR(50) NOT NULL DEFAULT 'active',
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    created_by VARCHAR(255),
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    deleted_at TIMESTAMP,
    
    -- ã‚¿ã‚°ãƒ»å±žæ€§
    tags JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- åˆ¶ç´„
    CONSTRAINT ck_experiments_status CHECK (
        status IN ('active', 'archived', 'deleted')
    ),
    CONSTRAINT ck_experiments_lifecycle CHECK (
        lifecycle_stage IN ('active', 'deleted', 'archived')
    ),
    CONSTRAINT ck_experiments_type CHECK (
        experiment_type IN ('training', 'testing', 'production', 'development')
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_experiments_name ON experiments(experiment_name);
CREATE INDEX idx_experiments_created_at ON experiments(created_at DESC);
CREATE INDEX idx_experiments_status ON experiments(status) WHERE status = 'active';
CREATE INDEX idx_experiments_tags ON experiments USING GIN(tags);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE experiments IS 'å®Ÿé¨“ã®åŸºæœ¬æƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON COLUMN experiments.experiment_id IS 'å®Ÿé¨“ã®ä¸€æ„è­˜åˆ¥å­';
COMMENT ON COLUMN experiments.experiment_name IS 'å®Ÿé¨“åï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šï¼‰';
COMMENT ON COLUMN experiments.lifecycle_stage IS 'MLflowã¨ã®äº’æ›æ€§ã®ãŸã‚ã®ã‚¹ãƒ†ãƒ¼ã‚¸';
COMMENT ON COLUMN experiments.tags IS 'ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°ï¼ˆJSONBå½¢å¼ï¼‰';
```

---

#### 2.1.2 runs (å®Ÿè¡Œ)

**ç›®çš„**: å„å®Ÿé¨“ã®å€‹åˆ¥å®Ÿè¡Œã‚’è¨˜éŒ²

```sql
CREATE TABLE runs (
    -- ä¸»ã‚­ãƒ¼
    run_id BIGSERIAL PRIMARY KEY,
    run_uuid UUID NOT NULL UNIQUE DEFAULT gen_random_uuid(),
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    experiment_id BIGINT NOT NULL REFERENCES experiments(experiment_id) ON DELETE CASCADE,
    parent_run_id BIGINT REFERENCES runs(run_id) ON DELETE SET NULL,
    
    -- åŸºæœ¬æƒ…å ±
    run_name VARCHAR(255) NOT NULL,
    run_description TEXT,
    
    -- Fingerprintï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰
    run_fingerprint VARCHAR(64) NOT NULL,
    
    -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    status VARCHAR(50) NOT NULL DEFAULT 'running',
    lifecycle_stage VARCHAR(50) NOT NULL DEFAULT 'active',
    
    -- å®Ÿè¡Œæƒ…å ±
    start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    end_time TIMESTAMP,
    execution_duration_seconds NUMERIC(12, 3),
    
    -- ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰æƒ…å ±
    source_type VARCHAR(50),
    source_name TEXT,
    git_commit_hash VARCHAR(40),
    git_branch VARCHAR(255),
    git_repo_url TEXT,
    
    -- ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
    user_id VARCHAR(255),
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    tags JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    deleted_at TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_runs_status CHECK (
        status IN ('running', 'completed', 'failed', 'killed', 'scheduled')
    ),
    CONSTRAINT ck_runs_lifecycle CHECK (
        lifecycle_stage IN ('active', 'deleted')
    ),
    CONSTRAINT ck_runs_source_type CHECK (
        source_type IN ('notebook', 'script', 'cli', 'api', 'scheduled')
    ),
    CONSTRAINT ck_runs_duration CHECK (
        execution_duration_seconds IS NULL OR execution_duration_seconds >= 0
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_runs_experiment_id ON runs(experiment_id);
CREATE INDEX idx_runs_fingerprint ON runs(run_fingerprint);
CREATE INDEX idx_runs_status ON runs(status);
CREATE INDEX idx_runs_start_time ON runs(start_time DESC);
CREATE INDEX idx_runs_parent_run ON runs(parent_run_id) WHERE parent_run_id IS NOT NULL;
CREATE INDEX idx_runs_tags ON runs USING GIN(tags);
CREATE INDEX idx_runs_composite ON runs(experiment_id, status, start_time DESC);

-- ä¸€æ„åˆ¶ç´„ï¼ˆé‡è¤‡å®Ÿè¡Œé˜²æ­¢ï¼‰
CREATE UNIQUE INDEX idx_runs_unique_fingerprint 
    ON runs(experiment_id, run_fingerprint) 
    WHERE deleted_at IS NULL;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE runs IS 'å®Ÿé¨“ã®å€‹åˆ¥å®Ÿè¡Œã‚’è¨˜éŒ²ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON COLUMN runs.run_fingerprint IS 'å®Ÿè¡Œæ¡ä»¶ã®ä¸€æ„è­˜åˆ¥ç”¨ãƒãƒƒã‚·ãƒ¥ï¼ˆSHA-256ï¼‰';
COMMENT ON COLUMN runs.parent_run_id IS 'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¦ªRun';
```

---

### 2.2 ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.2.1 models (ãƒ¢ãƒ‡ãƒ«)

**ç›®çš„**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’ç®¡ç†

```sql
CREATE TABLE models (
    -- ä¸»ã‚­ãƒ¼
    model_id BIGSERIAL PRIMARY KEY,
    model_uuid UUID NOT NULL UNIQUE DEFAULT gen_random_uuid(),
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    run_id BIGINT NOT NULL REFERENCES runs(run_id) ON DELETE CASCADE,
    
    -- ãƒ¢ãƒ‡ãƒ«åŸºæœ¬æƒ…å ±
    model_name VARCHAR(255) NOT NULL,
    model_type VARCHAR(100) NOT NULL,
    model_version VARCHAR(50) NOT NULL DEFAULT '1.0.0',
    
    -- ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæƒ…å ±
    framework VARCHAR(50) NOT NULL,
    framework_version VARCHAR(50),
    
    -- ãƒ¢ãƒ‡ãƒ«è¨­å®š
    model_config JSONB NOT NULL,
    hyperparameters JSONB NOT NULL DEFAULT '{}',
    
    -- å­¦ç¿’æƒ…å ±
    training_dataset_id BIGINT REFERENCES datasets(dataset_id),
    training_samples INTEGER,
    training_duration_seconds NUMERIC(12, 3),
    
    -- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
    model_size_bytes BIGINT,
    
    -- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    model_file_path TEXT NOT NULL,
    checkpoint_path TEXT,
    
    -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    status VARCHAR(50) NOT NULL DEFAULT 'training',
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    tags JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_models_status CHECK (
        status IN ('training', 'trained', 'evaluating', 'evaluated', 'deployed', 'archived', 'failed')
    ),
    CONSTRAINT ck_models_framework CHECK (
        framework IN ('neuralforecast', 'pytorch', 'tensorflow', 'sklearn', 'darts', 'gluonts', 'prophet')
    ),
    CONSTRAINT ck_models_training_samples CHECK (
        training_samples IS NULL OR training_samples > 0
    ),
    CONSTRAINT ck_models_size CHECK (
        model_size_bytes IS NULL OR model_size_bytes > 0
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_models_run_id ON models(run_id);
CREATE INDEX idx_models_name ON models(model_name);
CREATE INDEX idx_models_type ON models(model_type);
CREATE INDEX idx_models_framework ON models(framework);
CREATE INDEX idx_models_status ON models(status);
CREATE INDEX idx_models_created_at ON models(created_at DESC);
CREATE INDEX idx_models_config ON models USING GIN(model_config);
CREATE INDEX idx_models_tags ON models USING GIN(tags);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE models IS 'å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON COLUMN models.model_type IS 'ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡žï¼ˆä¾‹: AutoNHITS, AutoLSTM, AutoTFTï¼‰';
COMMENT ON COLUMN models.model_config IS 'ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è¨­å®šï¼ˆJSONBå½¢å¼ï¼‰';
COMMENT ON COLUMN models.hyperparameters IS 'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆJSONBå½¢å¼ï¼‰';
```

---

#### 2.2.2 model_registry (ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒª)

**ç›®çš„**: ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»

```sql
CREATE TABLE model_registry (
    -- ä¸»ã‚­ãƒ¼
    registry_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    model_id BIGINT NOT NULL REFERENCES models(model_id) ON DELETE CASCADE,
    
    -- ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæƒ…å ±
    registered_model_name VARCHAR(255) NOT NULL,
    model_version INTEGER NOT NULL,
    
    -- ã‚¹ãƒ†ãƒ¼ã‚¸ç®¡ç†
    current_stage VARCHAR(50) NOT NULL DEFAULT 'staging',
    previous_stage VARCHAR(50),
    stage_transition_at TIMESTAMP,
    
    -- æ‰¿èªæƒ…å ±
    approved_by VARCHAR(255),
    approved_at TIMESTAMP,
    approval_notes TEXT,
    
    -- ãƒ‡ãƒ—ãƒ­ã‚¤æƒ…å ±
    deployed_at TIMESTAMP,
    deployment_target VARCHAR(100),
    
    -- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹è¿½è·¡
    production_metrics JSONB DEFAULT '{}',
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    description TEXT,
    tags JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_registry_stage CHECK (
        current_stage IN ('staging', 'production', 'archived', 'none')
    ),
    CONSTRAINT ck_registry_version CHECK (
        model_version > 0
    ),
    CONSTRAINT uq_registry_name_version UNIQUE (registered_model_name, model_version)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_registry_model_id ON model_registry(model_id);
CREATE INDEX idx_registry_name ON model_registry(registered_model_name);
CREATE INDEX idx_registry_stage ON model_registry(current_stage);
CREATE INDEX idx_registry_deployed_at ON model_registry(deployed_at DESC) 
    WHERE deployed_at IS NOT NULL;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE model_registry IS 'ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨ã‚¹ãƒ†ãƒ¼ã‚¸é·ç§»ã‚’ç®¡ç†';
COMMENT ON COLUMN model_registry.current_stage IS 'staging, production, archived, none';
COMMENT ON COLUMN model_registry.production_metrics IS 'ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®è©•ä¾¡æŒ‡æ¨™';
```

---

### 2.3 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.3.1 datasets (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)

**ç›®çš„**: å­¦ç¿’ãƒ»è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±

```sql
CREATE TABLE datasets (
    -- ä¸»ã‚­ãƒ¼
    dataset_id BIGSERIAL PRIMARY KEY,
    dataset_uuid UUID NOT NULL UNIQUE DEFAULT gen_random_uuid(),
    
    -- åŸºæœ¬æƒ…å ±
    dataset_name VARCHAR(255) NOT NULL,
    dataset_description TEXT,
    dataset_type VARCHAR(50) NOT NULL,
    
    -- ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    file_path TEXT NOT NULL,
    file_format VARCHAR(50) NOT NULL DEFAULT 'csv',
    file_size_bytes BIGINT,
    file_hash VARCHAR(64),
    
    -- ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
    num_rows INTEGER,
    num_columns INTEGER,
    num_time_series INTEGER,
    
    -- æ™‚ç³»åˆ—æƒ…å ±
    date_column VARCHAR(100),
    target_column VARCHAR(100),
    frequency VARCHAR(20),
    start_date TIMESTAMP,
    end_date TIMESTAMP,
    
    -- ã‚¹ã‚­ãƒ¼ãƒžæƒ…å ±
    schema_definition JSONB,
    column_types JSONB,
    
    -- ãƒ‡ãƒ¼ã‚¿å“è³ª
    missing_values_count INTEGER DEFAULT 0,
    duplicate_rows_count INTEGER DEFAULT 0,
    
    -- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    version VARCHAR(50) DEFAULT '1.0.0',
    parent_dataset_id BIGINT REFERENCES datasets(dataset_id),
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    tags JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_datasets_type CHECK (
        dataset_type IN ('raw', 'processed', 'training', 'validation', 'test', 'production')
    ),
    CONSTRAINT ck_datasets_format CHECK (
        file_format IN ('csv', 'parquet', 'json', 'feather')
    ),
    CONSTRAINT ck_datasets_rows CHECK (
        num_rows IS NULL OR num_rows >= 0
    ),
    CONSTRAINT ck_datasets_columns CHECK (
        num_columns IS NULL OR num_columns > 0
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_datasets_name ON datasets(dataset_name);
CREATE INDEX idx_datasets_type ON datasets(dataset_type);
CREATE INDEX idx_datasets_file_hash ON datasets(file_hash);
CREATE INDEX idx_datasets_created_at ON datasets(created_at DESC);
CREATE INDEX idx_datasets_parent ON datasets(parent_dataset_id) 
    WHERE parent_dataset_id IS NOT NULL;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE datasets IS 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON COLUMN datasets.file_hash IS 'ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®SHA-256ãƒãƒƒã‚·ãƒ¥';
COMMENT ON COLUMN datasets.schema_definition IS 'ã‚«ãƒ©ãƒ ã‚¹ã‚­ãƒ¼ãƒžã®å®šç¾©ï¼ˆJSONBï¼‰';
```

---

#### 2.3.2 dataset_features (ç‰¹å¾´é‡)

**ç›®çš„**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹å¾´é‡æƒ…å ±ã‚’ç®¡ç†

```sql
CREATE TABLE dataset_features (
    -- ä¸»ã‚­ãƒ¼
    feature_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    dataset_id BIGINT NOT NULL REFERENCES datasets(dataset_id) ON DELETE CASCADE,
    
    -- ç‰¹å¾´é‡æƒ…å ±
    feature_name VARCHAR(255) NOT NULL,
    feature_type VARCHAR(50) NOT NULL,
    data_type VARCHAR(50) NOT NULL,
    
    -- çµ±è¨ˆæƒ…å ±
    min_value DOUBLE PRECISION,
    max_value DOUBLE PRECISION,
    mean_value DOUBLE PRECISION,
    median_value DOUBLE PRECISION,
    std_value DOUBLE PRECISION,
    
    -- ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡
    num_unique_values INTEGER,
    unique_values JSONB,
    
    -- æ¬ æå€¤æƒ…å ±
    missing_count INTEGER DEFAULT 0,
    missing_percentage NUMERIC(5, 2) DEFAULT 0.00,
    
    -- ç‰¹å¾´é‡ã®å½¹å‰²
    feature_role VARCHAR(50),
    is_target BOOLEAN DEFAULT FALSE,
    is_exogenous BOOLEAN DEFAULT FALSE,
    exogenous_type VARCHAR(50),
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    importance_score DOUBLE PRECISION,
    correlation_with_target DOUBLE PRECISION,
    tags JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_features_type CHECK (
        feature_type IN ('numeric', 'categorical', 'datetime', 'text', 'boolean')
    ),
    CONSTRAINT ck_features_role CHECK (
        feature_role IN ('target', 'feature', 'id', 'timestamp', 'weight', 'ignore')
    ),
    CONSTRAINT ck_features_exog_type CHECK (
        exogenous_type IS NULL OR 
        exogenous_type IN ('future', 'historic', 'static')
    ),
    CONSTRAINT ck_features_missing_pct CHECK (
        missing_percentage >= 0 AND missing_percentage <= 100
    ),
    CONSTRAINT uq_dataset_feature UNIQUE (dataset_id, feature_name)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_features_dataset_id ON dataset_features(dataset_id);
CREATE INDEX idx_features_name ON dataset_features(feature_name);
CREATE INDEX idx_features_type ON dataset_features(feature_type);
CREATE INDEX idx_features_role ON dataset_features(feature_role);
CREATE INDEX idx_features_target ON dataset_features(is_target) WHERE is_target = TRUE;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE dataset_features IS 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹å¾´é‡æƒ…å ±ã‚’ç®¡ç†';
COMMENT ON COLUMN dataset_features.exogenous_type IS 'future: æœªæ¥å€¤åˆ©ç”¨å¯èƒ½, historic: éŽåŽ»å€¤ã®ã¿, static: é™çš„';
```

---

### 2.4 è©•ä¾¡æŒ‡æ¨™ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.4.1 metrics (è©•ä¾¡æŒ‡æ¨™)

**ç›®çš„**: ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨˜éŒ²

```sql
CREATE TABLE metrics (
    -- ä¸»ã‚­ãƒ¼
    metric_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    run_id BIGINT NOT NULL REFERENCES runs(run_id) ON DELETE CASCADE,
    model_id BIGINT REFERENCES models(model_id) ON DELETE CASCADE,
    
    -- è©•ä¾¡æŒ‡æ¨™æƒ…å ±
    metric_name VARCHAR(100) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    metric_step INTEGER DEFAULT 0,
    
    -- è©•ä¾¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    dataset_type VARCHAR(50) NOT NULL DEFAULT 'validation',
    evaluation_context VARCHAR(100),
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata JSONB DEFAULT '{}',
    
    -- åˆ¶ç´„
    CONSTRAINT ck_metrics_dataset_type CHECK (
        dataset_type IN ('train', 'validation', 'test', 'production')
    ),
    CONSTRAINT ck_metrics_step CHECK (
        metric_step >= 0
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_metrics_run_id ON metrics(run_id);
CREATE INDEX idx_metrics_model_id ON metrics(model_id);
CREATE INDEX idx_metrics_name ON metrics(metric_name);
CREATE INDEX idx_metrics_timestamp ON metrics(timestamp DESC);
CREATE INDEX idx_metrics_composite ON metrics(run_id, metric_name, metric_step);

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æº–å‚™ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
-- CREATE TABLE metrics_2025_q1 PARTITION OF metrics
--     FOR VALUES FROM ('2025-01-01') TO ('2025-04-01');

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE metrics IS 'ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨˜éŒ²ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«';
COMMENT ON COLUMN metrics.metric_step IS 'å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ï¼ˆã‚¨ãƒãƒƒã‚¯ã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ï¼‰';
COMMENT ON COLUMN metrics.evaluation_context IS 'è©•ä¾¡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¾‹: backtest_fold_1ï¼‰';
```

---

#### 2.4.2 metric_summaries (æŒ‡æ¨™ã‚µãƒžãƒªãƒ¼)

**ç›®çš„**: è©•ä¾¡æŒ‡æ¨™ã®é›†è¨ˆçµæžœã‚’ä¿å­˜ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–ï¼‰

```sql
CREATE TABLE metric_summaries (
    -- ä¸»ã‚­ãƒ¼
    summary_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    run_id BIGINT NOT NULL REFERENCES runs(run_id) ON DELETE CASCADE,
    model_id BIGINT REFERENCES models(model_id) ON DELETE CASCADE,
    
    -- é›†è¨ˆæƒ…å ±
    metric_name VARCHAR(100) NOT NULL,
    dataset_type VARCHAR(50) NOT NULL,
    
    -- çµ±è¨ˆé‡
    min_value DOUBLE PRECISION,
    max_value DOUBLE PRECISION,
    mean_value DOUBLE PRECISION,
    median_value DOUBLE PRECISION,
    std_value DOUBLE PRECISION,
    
    -- å››åˆ†ä½æ•°
    q1_value DOUBLE PRECISION,
    q3_value DOUBLE PRECISION,
    
    -- ã‚µãƒ³ãƒ—ãƒ«æ•°
    sample_count INTEGER NOT NULL,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT uq_summary_run_metric UNIQUE (run_id, metric_name, dataset_type)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_summary_run_id ON metric_summaries(run_id);
CREATE INDEX idx_summary_model_id ON metric_summaries(model_id);
CREATE INDEX idx_summary_metric ON metric_summaries(metric_name);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE metric_summaries IS 'è©•ä¾¡æŒ‡æ¨™ã®é›†è¨ˆçµæžœã‚’ä¿å­˜ï¼ˆé«˜é€Ÿã‚¯ã‚¨ãƒªç”¨ï¼‰';
```

---

### 2.5 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.5.1 hyperparameters (ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)

**ç›®çš„**: å„å®Ÿè¡Œã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²

```sql
CREATE TABLE hyperparameters (
    -- ä¸»ã‚­ãƒ¼
    hyperparameter_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    run_id BIGINT NOT NULL REFERENCES runs(run_id) ON DELETE CASCADE,
    
    -- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±
    param_key VARCHAR(255) NOT NULL,
    param_value TEXT NOT NULL,
    param_type VARCHAR(50) NOT NULL DEFAULT 'string',
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    is_hyperparameter BOOLEAN DEFAULT TRUE,
    param_category VARCHAR(100),
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_params_type CHECK (
        param_type IN ('string', 'integer', 'float', 'boolean', 'json')
    ),
    CONSTRAINT uq_run_param_key UNIQUE (run_id, param_key)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_params_run_id ON hyperparameters(run_id);
CREATE INDEX idx_params_key ON hyperparameters(param_key);
CREATE INDEX idx_params_category ON hyperparameters(param_category) 
    WHERE param_category IS NOT NULL;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE hyperparameters IS 'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²';
COMMENT ON COLUMN hyperparameters.is_hyperparameter IS 'TRUE: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿, FALSE: è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿';
```

---

#### 2.5.2 hyperparameter_search_spaces (æŽ¢ç´¢ç©ºé–“)

**ç›®çš„**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŽ¢ç´¢ç©ºé–“ã‚’å®šç¾©

```sql
CREATE TABLE hyperparameter_search_spaces (
    -- ä¸»ã‚­ãƒ¼
    search_space_id BIGSERIAL PRIMARY KEY,
    
    -- æŽ¢ç´¢ç©ºé–“æƒ…å ±
    space_name VARCHAR(255) NOT NULL UNIQUE,
    space_description TEXT,
    
    -- ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    model_type VARCHAR(100) NOT NULL,
    framework VARCHAR(50) NOT NULL,
    
    -- æŽ¢ç´¢ç©ºé–“å®šç¾©
    search_space_definition JSONB NOT NULL,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    created_by VARCHAR(255),
    is_active BOOLEAN DEFAULT TRUE,
    tags JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_search_space_model ON hyperparameter_search_spaces(model_type);
CREATE INDEX idx_search_space_active ON hyperparameter_search_spaces(is_active) 
    WHERE is_active = TRUE;

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE hyperparameter_search_spaces IS 'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŽ¢ç´¢ç©ºé–“å®šç¾©';
COMMENT ON COLUMN hyperparameter_search_spaces.search_space_definition IS 'Optuna/Ray Tuneå½¢å¼ã®æŽ¢ç´¢ç©ºé–“ï¼ˆJSONBï¼‰';
```

---

### 2.6 ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.6.1 artifacts (ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ)

**ç›®çš„**: å®Ÿé¨“ã§ç”Ÿæˆã•ã‚Œã‚‹ã™ã¹ã¦ã®æˆæžœç‰©ã‚’ç®¡ç†

```sql
CREATE TABLE artifacts (
    -- ä¸»ã‚­ãƒ¼
    artifact_id BIGSERIAL PRIMARY KEY,
    artifact_uuid UUID NOT NULL UNIQUE DEFAULT gen_random_uuid(),
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    run_id BIGINT NOT NULL REFERENCES runs(run_id) ON DELETE CASCADE,
    
    -- ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆæƒ…å ±
    artifact_name VARCHAR(255) NOT NULL,
    artifact_type VARCHAR(100) NOT NULL,
    artifact_path TEXT NOT NULL,
    
    -- ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    file_size_bytes BIGINT,
    file_format VARCHAR(50),
    mime_type VARCHAR(100),
    file_hash VARCHAR(64),
    
    -- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æƒ…å ±
    storage_type VARCHAR(50) NOT NULL DEFAULT 'local',
    storage_path TEXT NOT NULL,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    description TEXT,
    tags JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¢ã‚¯ã‚»ã‚¹ç®¡ç†
    is_public BOOLEAN DEFAULT FALSE,
    access_count INTEGER DEFAULT 0,
    last_accessed_at TIMESTAMP,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_artifacts_type CHECK (
        artifact_type IN (
            'model', 'checkpoint', 'prediction', 'plot', 'log', 
            'config', 'report', 'data', 'other'
        )
    ),
    CONSTRAINT ck_artifacts_storage CHECK (
        storage_type IN ('local', 's3', 'gcs', 'azure', 'mlflow', 'wandb')
    ),
    CONSTRAINT ck_artifacts_size CHECK (
        file_size_bytes IS NULL OR file_size_bytes >= 0
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_artifacts_run_id ON artifacts(run_id);
CREATE INDEX idx_artifacts_type ON artifacts(artifact_type);
CREATE INDEX idx_artifacts_name ON artifacts(artifact_name);
CREATE INDEX idx_artifacts_hash ON artifacts(file_hash);
CREATE INDEX idx_artifacts_created_at ON artifacts(created_at DESC);
CREATE INDEX idx_artifacts_tags ON artifacts USING GIN(tags);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE artifacts IS 'å®Ÿé¨“ã§ç”Ÿæˆã•ã‚Œã‚‹æˆæžœç‰©ã‚’ç®¡ç†';
COMMENT ON COLUMN artifacts.file_hash IS 'ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®SHA-256ãƒãƒƒã‚·ãƒ¥';
COMMENT ON COLUMN artifacts.storage_path IS 'å®Ÿéš›ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‘ã‚¹ï¼ˆURIå½¢å¼ï¼‰';
```

---

### 2.7 äºˆæ¸¬çµæžœãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.7.1 predictions (äºˆæ¸¬çµæžœ)

**ç›®çš„**: ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæžœã‚’è¨˜éŒ²

```sql
CREATE TABLE predictions (
    -- ä¸»ã‚­ãƒ¼
    prediction_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    run_id BIGINT NOT NULL REFERENCES runs(run_id) ON DELETE CASCADE,
    model_id BIGINT NOT NULL REFERENCES models(model_id) ON DELETE CASCADE,
    
    -- äºˆæ¸¬æƒ…å ±
    prediction_date TIMESTAMP NOT NULL,
    unique_id TEXT NOT NULL,
    horizon INTEGER NOT NULL,
    
    -- äºˆæ¸¬å€¤
    predicted_value DOUBLE PRECISION NOT NULL,
    actual_value DOUBLE PRECISION,
    
    -- ä¿¡é ¼åŒºé–“
    lower_bound DOUBLE PRECISION,
    upper_bound DOUBLE PRECISION,
    confidence_level NUMERIC(3, 2) DEFAULT 0.95,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_predictions_horizon CHECK (horizon > 0),
    CONSTRAINT ck_predictions_confidence CHECK (
        confidence_level > 0 AND confidence_level < 1
    )
) PARTITION BY RANGE (prediction_date);

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆå››åŠæœŸã”ã¨ï¼‰
CREATE TABLE predictions_2025_q1 PARTITION OF predictions
    FOR VALUES FROM ('2025-01-01') TO ('2025-04-01');
CREATE TABLE predictions_2025_q2 PARTITION OF predictions
    FOR VALUES FROM ('2025-04-01') TO ('2025-07-01');
CREATE TABLE predictions_2025_q3 PARTITION OF predictions
    FOR VALUES FROM ('2025-07-01') TO ('2025-10-01');
CREATE TABLE predictions_2025_q4 PARTITION OF predictions
    FOR VALUES FROM ('2025-10-01') TO ('2026-01-01');

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«è‡ªå‹•ä½œæˆï¼‰
CREATE INDEX idx_predictions_run_id ON predictions(run_id);
CREATE INDEX idx_predictions_model_id ON predictions(model_id);
CREATE INDEX idx_predictions_date ON predictions(prediction_date);
CREATE INDEX idx_predictions_unique_id ON predictions(unique_id);
CREATE INDEX idx_predictions_composite ON predictions(unique_id, prediction_date, horizon);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE predictions IS 'ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæžœã‚’è¨˜éŒ²ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼‰';
COMMENT ON COLUMN predictions.horizon IS 'äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆä½•ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®äºˆæ¸¬ã‹ï¼‰';
```

---

### 2.8 ãƒ­ã‚°ãƒ»ç›£æŸ»ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.8.1 system_logs (ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°)

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ­ã‚°ã‚’è¨˜éŒ²

```sql
CREATE TABLE system_logs (
    -- ä¸»ã‚­ãƒ¼
    log_id BIGSERIAL PRIMARY KEY,
    
    -- ãƒ­ã‚°æƒ…å ±
    log_level VARCHAR(20) NOT NULL,
    log_message TEXT NOT NULL,
    log_source VARCHAR(255),
    
    -- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
    run_id BIGINT REFERENCES runs(run_id) ON DELETE SET NULL,
    user_id VARCHAR(255),
    
    -- è©³ç´°æƒ…å ±
    stack_trace TEXT,
    exception_type VARCHAR(255),
    extra_data JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_logs_level CHECK (
        log_level IN ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')
    )
) PARTITION BY RANGE (created_at);

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆæœˆæ¬¡ï¼‰
CREATE TABLE system_logs_2025_01 PARTITION OF system_logs
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
CREATE TABLE system_logs_2025_02 PARTITION OF system_logs
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
CREATE TABLE system_logs_2025_03 PARTITION OF system_logs
    FOR VALUES FROM ('2025-03-01') TO ('2025-04-01');

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_logs_level ON system_logs(log_level);
CREATE INDEX idx_logs_created_at ON system_logs(created_at DESC);
CREATE INDEX idx_logs_run_id ON system_logs(run_id) WHERE run_id IS NOT NULL;
CREATE INDEX idx_logs_source ON system_logs(log_source);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE system_logs IS 'ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’è¨˜éŒ²ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼‰';
```

---

#### 2.8.2 audit_logs (ç›£æŸ»ãƒ­ã‚°)

**ç›®çš„**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã®ç›£æŸ»ãƒ­ã‚°

```sql
CREATE TABLE audit.audit_logs (
    -- ä¸»ã‚­ãƒ¼
    audit_id BIGSERIAL PRIMARY KEY,
    
    -- ç›£æŸ»æƒ…å ±
    table_name VARCHAR(255) NOT NULL,
    operation_type VARCHAR(20) NOT NULL,
    record_id BIGINT,
    
    -- å¤‰æ›´å†…å®¹
    old_values JSONB,
    new_values JSONB,
    changed_fields JSONB,
    
    -- ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
    user_id VARCHAR(255),
    user_ip VARCHAR(50),
    session_id VARCHAR(255),
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_audit_operation CHECK (
        operation_type IN ('INSERT', 'UPDATE', 'DELETE', 'TRUNCATE')
    )
) PARTITION BY RANGE (created_at);

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆå››åŠæœŸã”ã¨ï¼‰
CREATE TABLE audit.audit_logs_2025_q1 PARTITION OF audit.audit_logs
    FOR VALUES FROM ('2025-01-01') TO ('2025-04-01');

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_audit_table ON audit.audit_logs(table_name);
CREATE INDEX idx_audit_operation ON audit.audit_logs(operation_type);
CREATE INDEX idx_audit_user ON audit.audit_logs(user_id);
CREATE INDEX idx_audit_created_at ON audit.audit_logs(created_at DESC);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE audit.audit_logs IS 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã®ç›£æŸ»ãƒ­ã‚°';
```

---

### 2.9 ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»æ¨©é™ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.9.1 users (ãƒ¦ãƒ¼ã‚¶ãƒ¼)

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æƒ…å ±ã‚’ç®¡ç†

```sql
CREATE TABLE users (
    -- ä¸»ã‚­ãƒ¼
    user_id BIGSERIAL PRIMARY KEY,
    user_uuid UUID NOT NULL UNIQUE DEFAULT gen_random_uuid(),
    
    -- ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
    username VARCHAR(255) NOT NULL UNIQUE,
    email VARCHAR(255) NOT NULL UNIQUE,
    full_name VARCHAR(255),
    
    -- èªè¨¼æƒ…å ±ï¼ˆå¤–éƒ¨èªè¨¼ã‚’æƒ³å®šï¼‰
    auth_provider VARCHAR(50) DEFAULT 'local',
    external_id VARCHAR(255),
    
    -- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    is_active BOOLEAN DEFAULT TRUE,
    is_superuser BOOLEAN DEFAULT FALSE,
    
    -- æœ€çµ‚ãƒ­ã‚°ã‚¤ãƒ³
    last_login_at TIMESTAMP,
    login_count INTEGER DEFAULT 0,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    preferences JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_users_auth_provider CHECK (
        auth_provider IN ('local', 'oauth2', 'ldap', 'saml')
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_active ON users(is_active) WHERE is_active = TRUE;
CREATE INDEX idx_users_provider ON users(auth_provider);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE users IS 'ã‚·ã‚¹ãƒ†ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æƒ…å ±ã‚’ç®¡ç†';
COMMENT ON COLUMN users.preferences IS 'ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šï¼ˆUIè¨­å®šã€é€šçŸ¥è¨­å®šç­‰ï¼‰';
```

---

#### 2.9.2 user_experiments (ãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿé¨“é–¢é€£)

**ç›®çš„**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨å®Ÿé¨“ã®é–¢é€£ã‚’ç®¡ç†

```sql
CREATE TABLE user_experiments (
    -- ä¸»ã‚­ãƒ¼
    user_experiment_id BIGSERIAL PRIMARY KEY,
    
    -- å¤–éƒ¨ã‚­ãƒ¼
    user_id BIGINT NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
    experiment_id BIGINT NOT NULL REFERENCES experiments(experiment_id) ON DELETE CASCADE,
    
    -- æ¨©é™
    permission_level VARCHAR(50) NOT NULL DEFAULT 'read',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_user_exp_permission CHECK (
        permission_level IN ('read', 'write', 'admin', 'owner')
    ),
    CONSTRAINT uq_user_experiment UNIQUE (user_id, experiment_id)
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_user_exp_user_id ON user_experiments(user_id);
CREATE INDEX idx_user_exp_experiment_id ON user_experiments(experiment_id);
CREATE INDEX idx_user_exp_permission ON user_experiments(permission_level);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE user_experiments IS 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨å®Ÿé¨“ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç®¡ç†';
```

---

### 2.10 è¨­å®šãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«

#### 2.10.1 system_config (ã‚·ã‚¹ãƒ†ãƒ è¨­å®š)

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¨­å®šã‚’ç®¡ç†

```sql
CREATE TABLE system_config (
    -- ä¸»ã‚­ãƒ¼
    config_id BIGSERIAL PRIMARY KEY,
    
    -- è¨­å®šæƒ…å ±
    config_key VARCHAR(255) NOT NULL UNIQUE,
    config_value TEXT NOT NULL,
    config_type VARCHAR(50) NOT NULL DEFAULT 'string',
    
    -- èª¬æ˜Ž
    description TEXT,
    
    -- è¨­å®šã‚«ãƒ†ã‚´ãƒª
    category VARCHAR(100),
    
    -- æ¤œè¨¼
    validation_regex TEXT,
    is_secret BOOLEAN DEFAULT FALSE,
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- åˆ¶ç´„
    CONSTRAINT ck_config_type CHECK (
        config_type IN ('string', 'integer', 'float', 'boolean', 'json')
    )
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_config_key ON system_config(config_key);
CREATE INDEX idx_config_category ON system_config(category);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE system_config IS 'ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¨­å®šã‚’ç®¡ç†';
COMMENT ON COLUMN system_config.is_secret IS 'TRUE: æ©Ÿå¯†æƒ…å ±ï¼ˆæš—å·åŒ–æŽ¨å¥¨ï¼‰';

-- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®æŒ¿å…¥
INSERT INTO system_config (config_key, config_value, config_type, category, description) VALUES
    ('max_parallel_runs', '10', 'integer', 'execution', 'æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•°'),
    ('default_retention_days', '90', 'integer', 'storage', 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“ï¼ˆæ—¥ï¼‰'),
    ('enable_mlflow', 'true', 'boolean', 'tracking', 'MLflowãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æœ‰åŠ¹åŒ–'),
    ('enable_wandb', 'false', 'boolean', 'tracking', 'W&Bãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æœ‰åŠ¹åŒ–'),
    ('log_level', 'INFO', 'string', 'logging', 'ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«');
```

---

#### 2.10.2 tags (ã‚¿ã‚°)

**ç›®çš„**: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã§ä½¿ç”¨ã™ã‚‹ã‚¿ã‚°ã‚’ç®¡ç†

```sql
CREATE TABLE tags (
    -- ä¸»ã‚­ãƒ¼
    tag_id BIGSERIAL PRIMARY KEY,
    
    -- ã‚¿ã‚°æƒ…å ±
    tag_name VARCHAR(255) NOT NULL UNIQUE,
    tag_category VARCHAR(100),
    tag_description TEXT,
    
    -- ä½¿ç”¨çµ±è¨ˆ
    usage_count INTEGER DEFAULT 0,
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata JSONB DEFAULT '{}',
    
    -- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_tags_name ON tags(tag_name);
CREATE INDEX idx_tags_category ON tags(tag_category);
CREATE INDEX idx_tags_usage ON tags(usage_count DESC);

-- ã‚³ãƒ¡ãƒ³ãƒˆ
COMMENT ON TABLE tags IS 'ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã§ä½¿ç”¨ã™ã‚‹ã‚¿ã‚°ã‚’ç®¡ç†';
```

---

## 3. ERå›³

### 3.1 ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–¢ä¿‚å›³ï¼ˆå…¨ä½“ï¼‰

```mermaid
erDiagram
    users ||--o{ user_experiments : "has"
    users ||--o{ experiments : "creates"
    
    experiments ||--o{ runs : "contains"
    experiments ||--o{ user_experiments : "shared_with"
    
    runs ||--o{ models : "produces"
    runs ||--o{ metrics : "records"
    runs ||--o{ hyperparameters : "uses"
    runs ||--o{ artifacts : "generates"
    runs ||--o{ predictions : "makes"
    runs ||--o{ system_logs : "logs"
    runs }o--|| runs : "parent_of"
    
    models ||--|| model_registry : "registered_in"
    models ||--o{ metrics : "evaluated_by"
    models ||--o{ predictions : "makes"
    models }o--|| datasets : "trained_on"
    
    datasets ||--o{ dataset_features : "contains"
    datasets }o--|| datasets : "derived_from"
    
    hyperparameter_search_spaces ||--o{ hyperparameters : "defines"
    
    experiments {
        bigint experiment_id PK
        varchar experiment_name
        text experiment_description
        varchar status
        varchar lifecycle_stage
        jsonb tags
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    runs {
        bigint run_id PK
        uuid run_uuid UK
        bigint experiment_id FK
        bigint parent_run_id FK
        varchar run_name
        varchar run_fingerprint
        varchar status
        timestamp start_time
        timestamp end_time
        numeric execution_duration_seconds
        jsonb tags
        jsonb metadata
    }
    
    models {
        bigint model_id PK
        uuid model_uuid UK
        bigint run_id FK
        varchar model_name
        varchar model_type
        varchar framework
        jsonb model_config
        jsonb hyperparameters
        text model_file_path
        varchar status
    }
    
    model_registry {
        bigint registry_id PK
        bigint model_id FK
        varchar registered_model_name
        integer model_version
        varchar current_stage
        timestamp deployed_at
        jsonb production_metrics
    }
    
    datasets {
        bigint dataset_id PK
        uuid dataset_uuid UK
        varchar dataset_name
        varchar dataset_type
        text file_path
        varchar file_format
        bigint parent_dataset_id FK
        jsonb schema_definition
    }
    
    dataset_features {
        bigint feature_id PK
        bigint dataset_id FK
        varchar feature_name
        varchar feature_type
        varchar feature_role
        boolean is_target
        boolean is_exogenous
    }
    
    metrics {
        bigint metric_id PK
        bigint run_id FK
        bigint model_id FK
        varchar metric_name
        double metric_value
        integer metric_step
        varchar dataset_type
        timestamp timestamp
    }
    
    hyperparameters {
        bigint hyperparameter_id PK
        bigint run_id FK
        varchar param_key
        text param_value
        varchar param_type
    }
    
    artifacts {
        bigint artifact_id PK
        uuid artifact_uuid UK
        bigint run_id FK
        varchar artifact_name
        varchar artifact_type
        text storage_path
        varchar storage_type
    }
    
    predictions {
        bigint prediction_id PK
        bigint run_id FK
        bigint model_id FK
        timestamp prediction_date
        text unique_id
        integer horizon
        double predicted_value
        double actual_value
    }
    
    users {
        bigint user_id PK
        uuid user_uuid UK
        varchar username UK
        varchar email UK
        boolean is_active
        timestamp last_login_at
    }
```

---

### 3.2 ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–¢ä¿‚å›³ï¼ˆå®Ÿé¨“ãƒ»å®Ÿè¡Œãƒ»ãƒ¢ãƒ‡ãƒ«ï¼‰

```mermaid
erDiagram
    experiments ||--o{ runs : contains
    runs ||--o{ models : produces
    runs ||--o{ metrics : records
    runs ||--o{ hyperparameters : uses
    models ||--|| model_registry : registered_in
    models ||--o{ predictions : makes
    
    experiments {
        bigint experiment_id PK "å®Ÿé¨“ID"
        varchar experiment_name "å®Ÿé¨“å"
        varchar status "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"
        timestamp created_at "ä½œæˆæ—¥æ™‚"
    }
    
    runs {
        bigint run_id PK "å®Ÿè¡ŒID"
        uuid run_uuid UK "UUID"
        bigint experiment_id FK "å®Ÿé¨“ID"
        varchar run_fingerprint "ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆ"
        varchar status "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"
        timestamp start_time "é–‹å§‹æ™‚åˆ»"
        timestamp end_time "çµ‚äº†æ™‚åˆ»"
    }
    
    models {
        bigint model_id PK "ãƒ¢ãƒ‡ãƒ«ID"
        bigint run_id FK "å®Ÿè¡ŒID"
        varchar model_name "ãƒ¢ãƒ‡ãƒ«å"
        varchar model_type "ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥"
        jsonb model_config "ãƒ¢ãƒ‡ãƒ«è¨­å®š"
        text model_file_path "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"
    }
    
    model_registry {
        bigint registry_id PK "ãƒ¬ã‚¸ã‚¹ãƒˆãƒªID"
        bigint model_id FK "ãƒ¢ãƒ‡ãƒ«ID"
        varchar current_stage "ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸"
        integer model_version "ãƒãƒ¼ã‚¸ãƒ§ãƒ³"
    }
```

---

## 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ

### 4.1 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥

#### 4.1.1 B-Treeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

**ç”¨é€”**: ç¯„å›²æ¤œç´¢ã€ã‚½ãƒ¼ãƒˆã€ç­‰ä¾¡æ¯”è¼ƒ

```sql
-- å®Ÿé¨“æ¤œç´¢ã®æœ€é©åŒ–
CREATE INDEX idx_experiments_status_created 
    ON experiments(status, created_at DESC) 
    WHERE status = 'active';

-- å®Ÿè¡Œæ¤œç´¢ã®æœ€é©åŒ–
CREATE INDEX idx_runs_exp_status_start 
    ON runs(experiment_id, status, start_time DESC);

-- ãƒ¢ãƒ‡ãƒ«æ¤œç´¢ã®æœ€é©åŒ–
CREATE INDEX idx_models_type_created 
    ON models(model_type, created_at DESC);

-- è©•ä¾¡æŒ‡æ¨™æ¤œç´¢ã®æœ€é©åŒ–
CREATE INDEX idx_metrics_run_name_step 
    ON metrics(run_id, metric_name, metric_step);
```

---

#### 4.1.2 GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

**ç”¨é€”**: JSONBã€å…¨æ–‡æ¤œç´¢ã€é…åˆ—

```sql
-- JSONBæ¤œç´¢ã®æœ€é©åŒ–
CREATE INDEX idx_experiments_tags_gin 
    ON experiments USING GIN(tags);

CREATE INDEX idx_runs_metadata_gin 
    ON runs USING GIN(metadata);

CREATE INDEX idx_models_config_gin 
    ON models USING GIN(model_config);

-- ã‚¿ã‚°æ¤œç´¢ã®æœ€é©åŒ–
CREATE INDEX idx_experiments_tags_jsonb_path 
    ON experiments USING GIN(tags jsonb_path_ops);
```

---

#### 4.1.3 éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

**ç”¨é€”**: ç‰¹å®šæ¡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–

```sql
-- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå®Ÿé¨“ã®ã¿
CREATE INDEX idx_experiments_active 
    ON experiments(experiment_name) 
    WHERE status = 'active' AND deleted_at IS NULL;

-- å®Ÿè¡Œä¸­ã®Runã®ã¿
CREATE INDEX idx_runs_running 
    ON runs(start_time DESC) 
    WHERE status = 'running';

-- Productionã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿
CREATE INDEX idx_registry_production 
    ON model_registry(registered_model_name, model_version) 
    WHERE current_stage = 'production';
```

---

#### 4.1.4 è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

**ç”¨é€”**: è¤‡æ•°ã‚«ãƒ©ãƒ ã§ã®æ¤œç´¢æœ€é©åŒ–

```sql
-- å®Ÿé¨“å†…ã®å®Ÿè¡Œæ¤œç´¢
CREATE INDEX idx_runs_composite 
    ON runs(experiment_id, status, start_time DESC);

-- äºˆæ¸¬æ¤œç´¢
CREATE INDEX idx_predictions_composite 
    ON predictions(unique_id, prediction_date, horizon);

-- ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢
CREATE INDEX idx_metrics_composite 
    ON metrics(run_id, metric_name, dataset_type, metric_step);
```

---

### 4.2 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

#### 4.2.1 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰

```sql
-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è‚¥å¤§åŒ–ç¢ºèª
SELECT
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
    idx_scan AS index_scans,
    idx_tup_read AS tuples_read,
    idx_tup_fetch AS tuples_fetched
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC;

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ï¼ˆCONCURRENTä½¿ç”¨ã§ç„¡åœæ­¢ï¼‰
REINDEX INDEX CONCURRENTLY idx_runs_fingerprint;
REINDEX TABLE CONCURRENTLY runs;
```

---

#### 4.2.2 æœªä½¿ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¤œå‡º

```sql
-- ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ¤œå‡º
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
  AND idx_scan = 0
  AND indexrelname NOT LIKE 'pk_%'
ORDER BY pg_relation_size(indexrelid) DESC;
```

---

## 5. ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»

### 5.1 ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«

#### 5.1.1 Alembicè¨­å®š

```python
# alembic.ini
[alembic]
script_location = db/migrations
sqlalchemy.url = postgresql://user:pass@localhost/ts_forecast_system

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = INFO
handlers = console

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

---

#### 5.1.2 ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision = ${repr(up_revision)}
down_revision = ${repr(down_revision)}
branch_labels = ${repr(branch_labels)}
depends_on = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}
```

---

### 5.2 ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚§ãƒ¼ã‚º

#### Phase 1: åŸºæœ¬ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ

```python
"""Create base tables

Revision ID: 001
Create Date: 2025-11-03
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

def upgrade() -> None:
    # experiments ãƒ†ãƒ¼ãƒ–ãƒ«
    op.create_table(
        'experiments',
        sa.Column('experiment_id', sa.BigInteger(), autoincrement=True, nullable=False),
        sa.Column('experiment_name', sa.String(255), nullable=False),
        sa.Column('experiment_description', sa.Text(), nullable=True),
        sa.Column('status', sa.String(50), nullable=False, server_default='active'),
        sa.Column('tags', postgresql.JSONB(), nullable=False, server_default='{}'),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('updated_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.PrimaryKeyConstraint('experiment_id')
    )
    
    # runs ãƒ†ãƒ¼ãƒ–ãƒ«
    op.create_table(
        'runs',
        sa.Column('run_id', sa.BigInteger(), autoincrement=True, nullable=False),
        sa.Column('run_uuid', postgresql.UUID(), nullable=False, server_default=sa.text('gen_random_uuid()')),
        sa.Column('experiment_id', sa.BigInteger(), nullable=False),
        sa.Column('run_name', sa.String(255), nullable=False),
        sa.Column('run_fingerprint', sa.String(64), nullable=False),
        sa.Column('status', sa.String(50), nullable=False, server_default='running'),
        sa.Column('start_time', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.Column('end_time', sa.TIMESTAMP(), nullable=True),
        sa.Column('tags', postgresql.JSONB(), nullable=False, server_default='{}'),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('CURRENT_TIMESTAMP')),
        sa.ForeignKeyConstraint(['experiment_id'], ['experiments.experiment_id'], ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('run_id'),
        sa.UniqueConstraint('run_uuid')
    )
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    op.create_index('idx_experiments_name', 'experiments', ['experiment_name'])
    op.create_index('idx_runs_experiment_id', 'runs', ['experiment_id'])
    op.create_index('idx_runs_fingerprint', 'runs', ['run_fingerprint'])

def downgrade() -> None:
    op.drop_table('runs')
    op.drop_table('experiments')
```

---

#### Phase 2: ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«

```python
"""Create model and dataset tables

Revision ID: 002
Revises: 001
Create Date: 2025-11-03
"""

def upgrade() -> None:
    # models ãƒ†ãƒ¼ãƒ–ãƒ«
    op.create_table(
        'models',
        # ... (ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©)
    )
    
    # datasets ãƒ†ãƒ¼ãƒ–ãƒ«
    op.create_table(
        'datasets',
        # ... (ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©)
    )
    
    # dataset_features ãƒ†ãƒ¼ãƒ–ãƒ«
    op.create_table(
        'dataset_features',
        # ... (ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©)
    )

def downgrade() -> None:
    op.drop_table('dataset_features')
    op.drop_table('datasets')
    op.drop_table('models')
```

---

#### Phase 3: è©•ä¾¡ãƒ»äºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼‰

```python
"""Create metrics and predictions tables with partitioning

Revision ID: 003
Revises: 002
Create Date: 2025-11-03
"""

def upgrade() -> None:
    # predictions ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¦ªï¼‰
    op.execute("""
        CREATE TABLE predictions (
            prediction_id BIGSERIAL,
            run_id BIGINT NOT NULL,
            model_id BIGINT NOT NULL,
            prediction_date TIMESTAMP NOT NULL,
            unique_id TEXT NOT NULL,
            horizon INTEGER NOT NULL,
            predicted_value DOUBLE PRECISION NOT NULL,
            created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY (prediction_id, prediction_date),
            FOREIGN KEY (run_id) REFERENCES runs(run_id) ON DELETE CASCADE,
            FOREIGN KEY (model_id) REFERENCES models(model_id) ON DELETE CASCADE
        ) PARTITION BY RANGE (prediction_date);
    """)
    
    # å››åŠæœŸã”ã¨ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ
    for year in [2025, 2026]:
        for q in range(1, 5):
            start_month = (q - 1) * 3 + 1
            end_month = start_month + 3
            op.execute(f"""
                CREATE TABLE predictions_{year}_q{q} PARTITION OF predictions
                FOR VALUES FROM ('{year}-{start_month:02d}-01') 
                TO ('{year if end_month <= 12 else year+1}-{end_month if end_month <= 12 else 1:02d}-01');
            """)

def downgrade() -> None:
    op.execute("DROP TABLE predictions CASCADE;")
```

---

### 5.3 ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œæ‰‹é †

```bash
# 1. æ–°ã—ã„ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
alembic revision -m "description_of_changes"

# 2. è‡ªå‹•ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®å·®åˆ†æ¤œå‡ºï¼‰
alembic revision --autogenerate -m "auto_generated_changes"

# 3. ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
alembic upgrade head

# 4. ç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¸ã®ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
alembic upgrade <revision_id>

# 5. ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
alembic downgrade -1  # 1ã¤å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«æˆ»ã™
alembic downgrade base  # åˆæœŸçŠ¶æ…‹ã«æˆ»ã™

# 6. ç¾åœ¨ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
alembic current

# 7. ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´è¡¨ç¤º
alembic history --verbose

# 8. ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆæ‰‹å‹•ã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨­å®šï¼‰
alembic stamp head
```

---

## 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 6.1 ã‚¯ã‚¨ãƒªæœ€é©åŒ–

#### 6.1.1 EXPLAINã®æ´»ç”¨

```sql
-- ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®ç¢ºèª
EXPLAIN ANALYZE
SELECT 
    e.experiment_name,
    COUNT(r.run_id) AS total_runs,
    AVG(r.execution_duration_seconds) AS avg_duration
FROM experiments e
LEFT JOIN runs r ON e.experiment_id = r.experiment_id
WHERE e.status = 'active'
GROUP BY e.experiment_id, e.experiment_name
ORDER BY total_runs DESC
LIMIT 10;
```

---

#### 6.1.2 ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®æ¤œå‡º

```sql
-- pg_stat_statements æœ‰åŠ¹åŒ–
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªãƒˆãƒƒãƒ—10
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    max_exec_time,
    stddev_exec_time
FROM pg_stat_statements
WHERE query NOT LIKE '%pg_stat_statements%'
ORDER BY mean_exec_time DESC
LIMIT 10;
```

---

### 6.2 ãƒ†ãƒ¼ãƒ–ãƒ«æœ€é©åŒ–

#### 6.2.1 VACUUMè¨­å®š

```sql
-- è‡ªå‹•VACUUMè¨­å®šã®ç¢ºèª
SHOW autovacuum;
SHOW autovacuum_naptime;
SHOW autovacuum_vacuum_threshold;
SHOW autovacuum_analyze_threshold;

-- ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®VACUUMçµ±è¨ˆ
SELECT
    schemaname,
    relname,
    last_vacuum,
    last_autovacuum,
    vacuum_count,
    autovacuum_count,
    n_dead_tup,
    n_live_tup
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY n_dead_tup DESC;

-- æ‰‹å‹•VACUUMï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
VACUUM ANALYZE experiments;
VACUUM FULL ANALYZE runs;  -- ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ­ãƒƒã‚¯æ³¨æ„
```

---

#### 6.2.2 ANALYZEçµ±è¨ˆæ›´æ–°

```sql
-- çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
ANALYZE experiments;
ANALYZE runs;
ANALYZE models;

-- çµ±è¨ˆæƒ…å ±ã®ç¢ºèª
SELECT
    schemaname,
    tablename,
    last_analyze,
    last_autoanalyze,
    analyze_count,
    autoanalyze_count
FROM pg_stat_user_tables
WHERE schemaname = 'public';
```

---

### 6.3 æŽ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–

#### 6.3.1 SQLAlchemyæŽ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š

```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    'postgresql://user:password@localhost/ts_forecast_system',
    poolclass=QueuePool,
    pool_size=10,              # é€šå¸¸æŽ¥ç¶šæ•°
    max_overflow=20,           # æœ€å¤§è¿½åŠ æŽ¥ç¶šæ•°
    pool_timeout=30,           # æŽ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    pool_recycle=3600,         # æŽ¥ç¶šå†åˆ©ç”¨æ™‚é–“ï¼ˆç§’ï¼‰
    pool_pre_ping=True,        # æŽ¥ç¶šãƒã‚§ãƒƒã‚¯æœ‰åŠ¹åŒ–
    echo=False,                # SQLãƒ­ã‚°å‡ºåŠ›
    echo_pool=False,           # ãƒ—ãƒ¼ãƒ«ãƒ­ã‚°å‡ºåŠ›
)
```

---

#### 6.3.2 PgBouncerè¨­å®š

```ini
# /etc/pgbouncer/pgbouncer.ini
[databases]
ts_forecast_system = host=localhost port=5432 dbname=ts_forecast_system

[pgbouncer]
listen_addr = 127.0.0.1
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt

# Connection pooling settings
pool_mode = transaction
max_client_conn = 100
default_pool_size = 20
reserve_pool_size = 5
reserve_pool_timeout = 3

# Server lifetime
server_lifetime = 3600
server_idle_timeout = 600

# Logging
log_connections = 1
log_disconnections = 1
log_pooler_errors = 1
```

---

### 6.4 ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥

#### 6.4.1 ç¯„å›²ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°

```sql
-- predictions ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†

-- æ–°ã—ã„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¿½åŠ ï¼ˆè‡ªå‹•åŒ–æŽ¨å¥¨ï¼‰
CREATE TABLE predictions_2026_q1 PARTITION OF predictions
    FOR VALUES FROM ('2026-01-01') TO ('2026-04-01');

-- å¤ã„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®DETACHï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‰ï¼‰
ALTER TABLE predictions DETACH PARTITION predictions_2023_q1;

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å‰Šé™¤ï¼ˆãƒ‡ãƒ¼ã‚¿å‰Šé™¤ï¼‰
DROP TABLE predictions_2023_q1;

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®ç¢ºèª
SELECT
    parent.relname AS parent_table,
    child.relname AS partition_name,
    pg_get_expr(child.relpartbound, child.oid) AS partition_range
FROM pg_inherits
JOIN pg_class parent ON pg_inherits.inhparent = parent.oid
JOIN pg_class child ON pg_inherits.inhrelid = child.oid
WHERE parent.relname = 'predictions'
ORDER BY child.relname;
```

---

#### 6.4.2 è‡ªå‹•ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ

```python
"""
è‡ªå‹•ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
from datetime import datetime, timedelta
from sqlalchemy import text

def create_future_partitions(engine, table_name: str, months_ahead: int = 6):
    """
    å°†æ¥ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•ä½œæˆ
    
    Args:
        engine: SQLAlchemy engine
        table_name: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«
        months_ahead: ä½•ãƒ¶æœˆå…ˆã¾ã§ä½œæˆã™ã‚‹ã‹
    """
    current_date = datetime.now()
    
    for i in range(months_ahead):
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœŸé–“ã®è¨ˆç®—
        start_date = current_date + timedelta(days=30*i)
        end_date = start_date + timedelta(days=30)
        
        partition_name = f"{table_name}_{start_date.year}_{start_date.month:02d}"
        
        sql = f"""
        CREATE TABLE IF NOT EXISTS {partition_name} PARTITION OF {table_name}
        FOR VALUES FROM ('{start_date.strftime('%Y-%m-%d')}') 
        TO ('{end_date.strftime('%Y-%m-%d')}');
        """
        
        with engine.begin() as conn:
            conn.execute(text(sql))
            print(f"Created partition: {partition_name}")

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    from sqlalchemy import create_engine
    engine = create_engine('postgresql://user:pass@localhost/ts_forecast_system')
    
    # predictions ãƒ†ãƒ¼ãƒ–ãƒ«ã®6ãƒ¶æœˆå…ˆã¾ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ
    create_future_partitions(engine, 'predictions', months_ahead=6)
    
    # system_logs ãƒ†ãƒ¼ãƒ–ãƒ«ã®3ãƒ¶æœˆå…ˆã¾ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½œæˆ
    create_future_partitions(engine, 'system_logs', months_ahead=3)
```

---

### 6.5 ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

#### 6.5.1 ãƒžãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼

```sql
-- å®Ÿé¨“ã‚µãƒžãƒªãƒ¼ãƒ“ãƒ¥ãƒ¼
CREATE MATERIALIZED VIEW mv_experiment_summary AS
SELECT
    e.experiment_id,
    e.experiment_name,
    e.status,
    COUNT(r.run_id) AS total_runs,
    COUNT(CASE WHEN r.status = 'completed' THEN 1 END) AS completed_runs,
    COUNT(CASE WHEN r.status = 'failed' THEN 1 END) AS failed_runs,
    AVG(r.execution_duration_seconds) AS avg_duration,
    MAX(r.end_time) AS last_run_time,
    e.created_at
FROM experiments e
LEFT JOIN runs r ON e.experiment_id = r.experiment_id
WHERE e.deleted_at IS NULL
GROUP BY e.experiment_id, e.experiment_name, e.status, e.created_at;

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
CREATE UNIQUE INDEX idx_mv_exp_summary_id ON mv_experiment_summary(experiment_id);
CREATE INDEX idx_mv_exp_summary_name ON mv_experiment_summary(experiment_name);

-- å®šæœŸçš„ãªãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆcronã‚¸ãƒ§ãƒ–æŽ¨å¥¨ï¼‰
REFRESH MATERIALIZED VIEW CONCURRENTLY mv_experiment_summary;
```

---

#### 6.5.2 ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚µãƒžãƒªãƒ¼

```sql
CREATE MATERIALIZED VIEW mv_model_performance AS
SELECT
    m.model_id,
    m.model_name,
    m.model_type,
    m.framework,
    r.run_id,
    r.experiment_id,
    -- è©•ä¾¡æŒ‡æ¨™ã®é›†è¨ˆ
    MAX(CASE WHEN mt.metric_name = 'mae' AND mt.dataset_type = 'validation' 
        THEN mt.metric_value END) AS validation_mae,
    MAX(CASE WHEN mt.metric_name = 'rmse' AND mt.dataset_type = 'validation' 
        THEN mt.metric_value END) AS validation_rmse,
    MAX(CASE WHEN mt.metric_name = 'smape' AND mt.dataset_type = 'validation' 
        THEN mt.metric_value END) AS validation_smape,
    MAX(CASE WHEN mt.metric_name = 'mae' AND mt.dataset_type = 'test' 
        THEN mt.metric_value END) AS test_mae,
    MAX(CASE WHEN mt.metric_name = 'rmse' AND mt.dataset_type = 'test' 
        THEN mt.metric_value END) AS test_rmse,
    MAX(CASE WHEN mt.metric_name = 'smape' AND mt.dataset_type = 'test' 
        THEN mt.metric_value END) AS test_smape,
    m.training_duration_seconds,
    m.model_size_bytes,
    m.created_at
FROM models m
JOIN runs r ON m.run_id = r.run_id
LEFT JOIN metrics mt ON m.model_id = mt.model_id
WHERE m.status = 'evaluated'
GROUP BY m.model_id, m.model_name, m.model_type, m.framework, 
         r.run_id, r.experiment_id, m.training_duration_seconds, 
         m.model_size_bytes, m.created_at;

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE UNIQUE INDEX idx_mv_model_perf_id ON mv_model_performance(model_id);
CREATE INDEX idx_mv_model_perf_type ON mv_model_performance(model_type);
```

---

## 7. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

### 7.1 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ–¹é‡

#### 7.1.1 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¦ä»¶

| é …ç›® | è¦ä»¶ | å®Ÿè£… |
|-----|------|------|
| **RPO** (Recovery Point Objective) | 1æ™‚é–“ | ç¶™ç¶šçš„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– + å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— |
| **RTO** (Recovery Time Objective) | 4æ™‚é–“ | ãƒ›ãƒƒãƒˆã‚¹ã‚¿ãƒ³ãƒã‚¤ + PITR |
| **ä¿æŒæœŸé–“** | 30æ—¥é–“ï¼ˆãƒ•ãƒ«ï¼‰ã€90æ—¥é–“ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼‰ | è‡ªå‹•ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ |
| **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é »åº¦** | æ¯Žæ—¥ï¼ˆãƒ•ãƒ«ï¼‰ã€1æ™‚é–“ï¼ˆå·®åˆ†ï¼‰ | cronã‚¸ãƒ§ãƒ– |
| **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€** | ãƒ­ãƒ¼ã‚«ãƒ« + å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | S3/GCS/Azure Blob |

---

### 7.2 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

#### 7.2.1 ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```bash
#!/bin/bash
# full_backup.sh

set -e

# è¨­å®š
DB_NAME="ts_forecast_system"
DB_USER="postgres"
BACKUP_DIR="/var/backups/postgresql"
RETENTION_DAYS=30
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/full_${DB_NAME}_${DATE}.sql.gz"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p "${BACKUP_DIR}"

# ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
echo "[$(date)] Starting full backup..."
pg_dump -U "${DB_USER}" -Fc -Z9 "${DB_NAME}" | gzip > "${BACKUP_FILE}"

# ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼
if [ -f "${BACKUP_FILE}" ]; then
    echo "[$(date)] Backup completed: ${BACKUP_FILE}"
    echo "[$(date)] Backup size: $(du -h ${BACKUP_FILE} | cut -f1)"
else
    echo "[$(date)] ERROR: Backup failed!"
    exit 1
fi

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤
find "${BACKUP_DIR}" -name "full_${DB_NAME}_*.sql.gz" -mtime +${RETENTION_DAYS} -delete
echo "[$(date)] Old backups cleaned up (retention: ${RETENTION_DAYS} days)"

# å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¸ã®ã‚³ãƒ”ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# aws s3 cp "${BACKUP_FILE}" "s3://my-bucket/backups/"

echo "[$(date)] Backup process completed successfully"
```

---

#### 7.2.2 ç¶™ç¶šçš„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆWALï¼‰

```bash
#!/bin/bash
# wal_archive.sh

set -e

# è¨­å®š
ARCHIVE_DIR="/var/backups/postgresql/wal_archive"
WAL_FILE=$1
WAL_PATH=$2

# ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p "${ARCHIVE_DIR}"

# WALãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
if [ -f "${WAL_PATH}" ]; then
    gzip -c "${WAL_PATH}" > "${ARCHIVE_DIR}/${WAL_FILE}.gz"
    echo "[$(date)] WAL archived: ${WAL_FILE}"
else
    echo "[$(date)] ERROR: WAL file not found: ${WAL_PATH}"
    exit 1
fi
```

**postgresql.confè¨­å®š**:

```ini
# WALè¨­å®š
wal_level = replica
archive_mode = on
archive_command = '/path/to/wal_archive.sh %f %p'
archive_timeout = 300  # 5åˆ†

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
checkpoint_timeout = 5min
max_wal_size = 2GB
min_wal_size = 1GB
```

---

#### 7.2.3 å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```bash
#!/bin/bash
# incremental_backup.sh

set -e

# è¨­å®š
DB_NAME="ts_forecast_system"
DB_USER="postgres"
BACKUP_DIR="/var/backups/postgresql/incremental"
BASE_BACKUP_DIR="/var/backups/postgresql/base"
DATE=$(date +%Y%m%d_%H%M%S)

# ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å­˜åœ¨ç¢ºèª
if [ ! -d "${BASE_BACKUP_DIR}/base" ]; then
    echo "[$(date)] ERROR: Base backup not found. Run base_backup.sh first."
    exit 1
fi

# å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
mkdir -p "${BACKUP_DIR}"
pg_basebackup -U "${DB_USER}" -D "${BACKUP_DIR}/${DATE}" \
    -Fp -Xs -P -R --incremental="${BASE_BACKUP_DIR}/base/backup_manifest"

echo "[$(date)] Incremental backup completed: ${BACKUP_DIR}/${DATE}"
```

---

### 7.3 ãƒªã‚¹ãƒˆã‚¢æ‰‹é †

#### 7.3.1 ãƒ•ãƒ«ãƒªã‚¹ãƒˆã‚¢

```bash
#!/bin/bash
# restore_full.sh

set -e

BACKUP_FILE=$1
DB_NAME="ts_forecast_system"
DB_USER="postgres"

if [ -z "${BACKUP_FILE}" ]; then
    echo "Usage: $0 <backup_file.sql.gz>"
    exit 1
fi

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¦å†ä½œæˆ
echo "[$(date)] Dropping database..."
psql -U "${DB_USER}" -c "DROP DATABASE IF EXISTS ${DB_NAME};"
psql -U "${DB_USER}" -c "CREATE DATABASE ${DB_NAME};"

# ãƒªã‚¹ãƒˆã‚¢å®Ÿè¡Œ
echo "[$(date)] Restoring from backup..."
gunzip -c "${BACKUP_FILE}" | pg_restore -U "${DB_USER}" -d "${DB_NAME}" -Fc

echo "[$(date)] Restore completed successfully"

# çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
echo "[$(date)] Analyzing database..."
psql -U "${DB_USER}" -d "${DB_NAME}" -c "ANALYZE;"

echo "[$(date)] Database ready for use"
```

---

#### 7.3.2 PITRï¼ˆPoint-In-Time Recoveryï¼‰

```bash
#!/bin/bash
# restore_pitr.sh

set -e

BACKUP_DIR="/var/backups/postgresql/base"
WAL_ARCHIVE_DIR="/var/backups/postgresql/wal_archive"
TARGET_TIME=$1  # ä¾‹: '2025-11-03 14:30:00'
DATA_DIR="/var/lib/postgresql/15/main"

if [ -z "${TARGET_TIME}" ]; then
    echo "Usage: $0 'YYYY-MM-DD HH:MM:SS'"
    exit 1
fi

# PostgreSQLã®åœæ­¢
systemctl stop postgresql

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
mv "${DATA_DIR}" "${DATA_DIR}.old"

# ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ãƒªã‚¹ãƒˆã‚¢
cp -r "${BACKUP_DIR}/base" "${DATA_DIR}"

# recovery.confä½œæˆï¼ˆPostgreSQL 12+ã§ã¯ recovery.signalï¼‰
cat > "${DATA_DIR}/postgresql.auto.conf" <<EOF
restore_command = 'gunzip -c ${WAL_ARCHIVE_DIR}/%f.gz > %p'
recovery_target_time = '${TARGET_TIME}'
recovery_target_action = 'promote'
EOF

touch "${DATA_DIR}/recovery.signal"

# PostgreSQLã®èµ·å‹•
systemctl start postgresql

echo "[$(date)] PITR recovery initiated. Target time: ${TARGET_TIME}"
echo "[$(date)] Check PostgreSQL logs for recovery progress"
```

---

### 7.4 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç›£è¦–

#### 7.4.1 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒã‚§ãƒƒã‚¯

```sql
-- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ…‹ã®ç¢ºèª
SELECT
    pg_current_wal_lsn() AS current_wal_lsn,
    pg_walfile_name(pg_current_wal_lsn()) AS current_wal_file,
    pg_last_wal_receive_lsn() AS last_received_lsn,
    pg_last_wal_replay_lsn() AS last_replayed_lsn,
    (pg_current_wal_lsn() - pg_last_wal_replay_lsn())::bigint AS replication_lag_bytes;

-- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–çŠ¶æ…‹ã®ç¢ºèª
SELECT
    archived_count,
    last_archived_wal,
    last_archived_time,
    failed_count,
    last_failed_wal,
    last_failed_time
FROM pg_stat_archiver;
```

---

#### 7.4.2 ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼

```bash
#!/bin/bash
# verify_backup.sh

set -e

BACKUP_FILE=$1
TEMP_DB="ts_forecast_system_verify"
DB_USER="postgres"

if [ -z "${BACKUP_FILE}" ]; then
    echo "Usage: $0 <backup_file.sql.gz>"
    exit 1
fi

echo "[$(date)] Starting backup verification..."

# ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
psql -U "${DB_USER}" -c "DROP DATABASE IF EXISTS ${TEMP_DB};"
psql -U "${DB_USER}" -c "CREATE DATABASE ${TEMP_DB};"

# ãƒªã‚¹ãƒˆã‚¢å®Ÿè¡Œ
if gunzip -c "${BACKUP_FILE}" | pg_restore -U "${DB_USER}" -d "${TEMP_DB}" -Fc; then
    echo "[$(date)] Backup verification successful: ${BACKUP_FILE}"
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ç¢ºèª
    TABLE_COUNT=$(psql -U "${DB_USER}" -d "${TEMP_DB}" -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='public';")
    echo "[$(date)] Tables restored: ${TABLE_COUNT}"
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    psql -U "${DB_USER}" -c "DROP DATABASE ${TEMP_DB};"
    exit 0
else
    echo "[$(date)] ERROR: Backup verification failed!"
    psql -U "${DB_USER}" -c "DROP DATABASE IF EXISTS ${TEMP_DB};"
    exit 1
fi
```

---

## 8. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

### 8.1 ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

#### 8.1.1 ãƒ­ãƒ¼ãƒ«ãƒ»æ¨©é™ç®¡ç†

```sql
-- èª­ã¿å–ã‚Šå°‚ç”¨ãƒ­ãƒ¼ãƒ«
CREATE ROLE readonly_user;
GRANT CONNECT ON DATABASE ts_forecast_system TO readonly_user;
GRANT USAGE ON SCHEMA public TO readonly_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA public 
    GRANT SELECT ON TABLES TO readonly_user;

-- ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãƒ­ãƒ¼ãƒ«
CREATE ROLE data_scientist;
GRANT CONNECT ON DATABASE ts_forecast_system TO data_scientist;
GRANT USAGE ON SCHEMA public TO data_scientist;
GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA public TO data_scientist;
GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO data_scientist;

-- ç®¡ç†è€…ãƒ­ãƒ¼ãƒ«
CREATE ROLE admin_user WITH CREATEDB CREATEROLE;
GRANT ALL PRIVILEGES ON DATABASE ts_forecast_system TO admin_user;

-- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆä¾‹
CREATE USER alice WITH PASSWORD 'secure_password_here';
GRANT data_scientist TO alice;
```

---

#### 8.1.2 è¡Œãƒ¬ãƒ™ãƒ«ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆRLSï¼‰

```sql
-- RLSæœ‰åŠ¹åŒ–
ALTER TABLE experiments ENABLE ROW LEVEL SECURITY;

-- ãƒãƒªã‚·ãƒ¼ä½œæˆ: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®å®Ÿé¨“ã®ã¿å‚ç…§å¯èƒ½
CREATE POLICY user_experiments_policy ON experiments
    FOR SELECT
    USING (created_by = current_user OR current_user IN (SELECT user_id FROM users WHERE is_superuser = TRUE));

-- ãƒãƒªã‚·ãƒ¼ä½œæˆ: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®å®Ÿé¨“ã®ã¿æ›´æ–°å¯èƒ½
CREATE POLICY user_experiments_update_policy ON experiments
    FOR UPDATE
    USING (created_by = current_user)
    WITH CHECK (created_by = current_user);
```

---

### 8.2 ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–

#### 8.2.1 æŽ¥ç¶šæš—å·åŒ–ï¼ˆSSL/TLSï¼‰

**postgresql.confè¨­å®š**:

```ini
ssl = on
ssl_cert_file = '/etc/ssl/certs/server.crt'
ssl_key_file = '/etc/ssl/private/server.key'
ssl_ca_file = '/etc/ssl/certs/ca.crt'
ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'
ssl_prefer_server_ciphers = on
ssl_min_protocol_version = 'TLSv1.2'
```

**pg_hba.confè¨­å®š**:

```
# TYPE  DATABASE        USER            ADDRESS                 METHOD
hostssl all             all             0.0.0.0/0               md5
hostssl all             all             ::/0                    md5
```

---

#### 8.2.2 ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–ï¼ˆpgcryptoï¼‰

```sql
-- pgcryptoæ‹¡å¼µã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–ä¾‹
CREATE TABLE secure_config (
    config_id SERIAL PRIMARY KEY,
    config_key VARCHAR(255) NOT NULL,
    config_value_encrypted BYTEA NOT NULL,
    encryption_key_id VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æš—å·åŒ–é–¢æ•°
CREATE OR REPLACE FUNCTION encrypt_value(
    plain_text TEXT,
    encryption_key TEXT
) RETURNS BYTEA AS $$
BEGIN
    RETURN pgp_sym_encrypt(plain_text, encryption_key);
END;
$$ LANGUAGE plpgsql;

-- å¾©å·åŒ–é–¢æ•°
CREATE OR REPLACE FUNCTION decrypt_value(
    encrypted_data BYTEA,
    encryption_key TEXT
) RETURNS TEXT AS $$
BEGIN
    RETURN pgp_sym_decrypt(encrypted_data, encryption_key);
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨ä¾‹
INSERT INTO secure_config (config_key, config_value_encrypted, encryption_key_id)
VALUES ('api_key', encrypt_value('secret_api_key_value', 'encryption_password'), 'key_v1');

-- å¾©å·åŒ–ã—ã¦å–å¾—
SELECT config_key, decrypt_value(config_value_encrypted, 'encryption_password') AS config_value
FROM secure_config
WHERE config_key = 'api_key';
```

---

### 8.3 ç›£æŸ»ãƒ»ãƒ­ã‚®ãƒ³ã‚°

#### 8.3.1 ç›£æŸ»ãƒ­ã‚°è¨­å®š

**postgresql.confè¨­å®š**:

```ini
# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB

# ä½•ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ã‹
log_connections = on
log_disconnections = on
log_duration = on
log_line_prefix = '%t [%p]: user=%u,db=%d,app=%a,client=%h '
log_statement = 'all'  # ã¾ãŸã¯ 'ddl', 'mod', 'none'
log_min_duration_statement = 1000  # 1ç§’ä»¥ä¸Šã®ã‚¯ã‚¨ãƒªã‚’ãƒ­ã‚°

# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
log_error_verbosity = default
log_min_messages = warning
```

---

#### 8.3.2 ç›£æŸ»ãƒˆãƒªã‚¬ãƒ¼

```sql
-- ç›£æŸ»ãƒ­ã‚°ãƒˆãƒªã‚¬ãƒ¼é–¢æ•°
CREATE OR REPLACE FUNCTION audit.log_changes()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO audit.audit_logs (
            table_name, operation_type, record_id, new_values,
            user_id, user_ip
        ) VALUES (
            TG_TABLE_NAME, 'INSERT', NEW.experiment_id, to_jsonb(NEW),
            current_user, inet_client_addr()
        );
        RETURN NEW;
    ELSIF TG_OP = 'UPDATE' THEN
        INSERT INTO audit.audit_logs (
            table_name, operation_type, record_id,
            old_values, new_values, changed_fields,
            user_id, user_ip
        ) VALUES (
            TG_TABLE_NAME, 'UPDATE', NEW.experiment_id,
            to_jsonb(OLD), to_jsonb(NEW),
            jsonb_object_agg(key, value)
                FILTER (WHERE to_jsonb(OLD) -> key IS DISTINCT FROM to_jsonb(NEW) -> key),
            current_user, inet_client_addr()
        );
        RETURN NEW;
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO audit.audit_logs (
            table_name, operation_type, record_id, old_values,
            user_id, user_ip
        ) VALUES (
            TG_TABLE_NAME, 'DELETE', OLD.experiment_id, to_jsonb(OLD),
            current_user, inet_client_addr()
        );
        RETURN OLD;
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- ãƒˆãƒªã‚¬ãƒ¼ä½œæˆ
CREATE TRIGGER experiments_audit_trigger
    AFTER INSERT OR UPDATE OR DELETE ON experiments
    FOR EACH ROW
    EXECUTE FUNCTION audit.log_changes();

CREATE TRIGGER models_audit_trigger
    AFTER INSERT OR UPDATE OR DELETE ON models
    FOR EACH ROW
    EXECUTE FUNCTION audit.log_changes();
```

---

## 9. é‹ç”¨æ‰‹é †

### 9.1 æ—¥æ¬¡é‹ç”¨

#### 9.1.1 ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```sql
-- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶šæ•°ç¢ºèª
SELECT 
    count(*) AS total_connections,
    count(*) FILTER (WHERE state = 'active') AS active_connections,
    count(*) FILTER (WHERE state = 'idle') AS idle_connections
FROM pg_stat_activity
WHERE datname = 'ts_forecast_system';

-- ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
    n_live_tup AS row_count
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ˜ãƒ«ã‚¹ç¢ºèª
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
  AND idx_scan = 0
ORDER BY pg_relation_size(indexrelid) DESC;
```

---

#### 9.1.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ç›£è¦–

```sql
-- å®Ÿè¡Œä¸­ã®é•·æ™‚é–“ã‚¯ã‚¨ãƒª
SELECT
    pid,
    now() - query_start AS duration,
    usename,
    state,
    query
FROM pg_stat_activity
WHERE state != 'idle'
  AND now() - query_start > interval '5 minutes'
ORDER BY duration DESC;

-- ãƒ­ãƒƒã‚¯å¾…ã¡ç¢ºèª
SELECT
    blocked_locks.pid AS blocked_pid,
    blocked_activity.usename AS blocked_user,
    blocking_locks.pid AS blocking_pid,
    blocking_activity.usename AS blocking_user,
    blocked_activity.query AS blocked_statement,
    blocking_activity.query AS blocking_statement
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks 
    ON blocking_locks.locktype = blocked_locks.locktype
    AND blocking_locks.database IS NOT DISTINCT FROM blocked_locks.database
    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
    AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted;
```

---

### 9.2 é€±æ¬¡é‹ç”¨

#### 9.2.1 çµ±è¨ˆæƒ…å ±æ›´æ–°

```bash
#!/bin/bash
# weekly_maintenance.sh

set -e

DB_NAME="ts_forecast_system"
DB_USER="postgres"

echo "[$(date)] Starting weekly maintenance..."

# VACUUM ANALYZE
echo "[$(date)] Running VACUUM ANALYZE..."
psql -U "${DB_USER}" -d "${DB_NAME}" -c "VACUUM ANALYZE;"

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
echo "[$(date)] Checking for bloated indexes..."
psql -U "${DB_USER}" -d "${DB_NAME}" <<EOF
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND pg_relation_size(indexrelid) > 10485760  -- 10MBä»¥ä¸Š
ORDER BY pg_relation_size(indexrelid) DESC;
EOF

# ãƒžãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼ã®æ›´æ–°
echo "[$(date)] Refreshing materialized views..."
psql -U "${DB_USER}" -d "${DB_NAME}" -c "REFRESH MATERIALIZED VIEW CONCURRENTLY mv_experiment_summary;"
psql -U "${DB_USER}" -d "${DB_NAME}" -c "REFRESH MATERIALIZED VIEW CONCURRENTLY mv_model_performance;"

echo "[$(date)] Weekly maintenance completed"
```

---

### 9.3 æœˆæ¬¡é‹ç”¨

#### 9.3.1 å®¹é‡ç®¡ç†

```bash
#!/bin/bash
# monthly_capacity_report.sh

set -e

DB_NAME="ts_forecast_system"
DB_USER="postgres"
REPORT_FILE="/var/log/postgresql/capacity_report_$(date +%Y%m).txt"

echo "Database Capacity Report - $(date)" > "${REPORT_FILE}"
echo "========================================" >> "${REPORT_FILE}"

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
echo -e "\n## Database Size ##" >> "${REPORT_FILE}"
psql -U "${DB_USER}" -d "${DB_NAME}" -t -c "
SELECT pg_size_pretty(pg_database_size('${DB_NAME}'));
" >> "${REPORT_FILE}"

# ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãƒˆãƒƒãƒ—10
echo -e "\n## Top 10 Largest Tables ##" >> "${REPORT_FILE}"
psql -U "${DB_USER}" -d "${DB_NAME}" -t -c "
SELECT
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 10;
" >> "${REPORT_FILE}"

# æˆé•·çŽ‡ã®è¨ˆç®—ï¼ˆå‰æœˆæ¯”ï¼‰
echo -e "\n## Growth Rate ##" >> "${REPORT_FILE}"
# å®Ÿè£…çœç•¥ï¼ˆå‰æœˆã®ãƒ¬ãƒãƒ¼ãƒˆã¨æ¯”è¼ƒï¼‰

cat "${REPORT_FILE}"
```

---

### 9.4 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### 9.4.1 æŽ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
# æŽ¥ç¶šæ•°ä¸Šé™ç¢ºèª
psql -U postgres -c "SHOW max_connections;"

# ç¾åœ¨ã®æŽ¥ç¶šæ•°ç¢ºèª
psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"

# ã‚¢ã‚¤ãƒ‰ãƒ«æŽ¥ç¶šã®å¼·åˆ¶çµ‚äº†
psql -U postgres -c "
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE datname = 'ts_forecast_system'
  AND state = 'idle'
  AND state_change < now() - interval '1 hour';
"
```

---

#### 9.4.2 ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªå¯¾å¿œ

```sql
-- ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªã®ç‰¹å®š
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    rows
FROM pg_stat_statements
WHERE mean_exec_time > 1000  -- 1ç§’ä»¥ä¸Š
ORDER BY mean_exec_time DESC
LIMIT 10;

-- ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ç¢ºèª
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
SELECT ...;  -- è©²å½“ã‚¯ã‚¨ãƒª

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
CREATE INDEX idx_name ON table_name(column_name);
ANALYZE table_name;
```

---

## 10. ä»˜éŒ²

### 10.1 SQLAlchemy ãƒ¢ãƒ‡ãƒ«ä¾‹

```python
"""
SQLAlchemy ORM ãƒ¢ãƒ‡ãƒ«å®šç¾©
"""
from sqlalchemy import (
    Column, BigInteger, String, Text, Integer, Boolean, 
    TIMESTAMP, Numeric, ForeignKey, CheckConstraint, UniqueConstraint,
    Index
)
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
import uuid

Base = declarative_base()


class Experiment(Base):
    """å®Ÿé¨“ãƒ†ãƒ¼ãƒ–ãƒ«"""
    __tablename__ = 'experiments'
    
    experiment_id = Column(BigInteger, primary_key=True, autoincrement=True)
    experiment_name = Column(String(255), nullable=False)
    experiment_description = Column(Text)
    experiment_type = Column(String(50), nullable=False, default='training')
    status = Column(String(50), nullable=False, default='active')
    lifecycle_stage = Column(String(50), nullable=False, default='active')
    created_by = Column(String(255))
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp(), onupdate=func.current_timestamp())
    deleted_at = Column(TIMESTAMP)
    tags = Column(JSONB, nullable=False, server_default='{}')
    metadata = Column('metadata', JSONB, nullable=False, server_default='{}')
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—
    runs = relationship("Run", back_populates="experiment", cascade="all, delete-orphan")
    
    # åˆ¶ç´„
    __table_args__ = (
        CheckConstraint(status.in_(['active', 'archived', 'deleted']), name='ck_experiments_status'),
        CheckConstraint(lifecycle_stage.in_(['active', 'deleted', 'archived']), name='ck_experiments_lifecycle'),
        Index('idx_experiments_name', 'experiment_name'),
        Index('idx_experiments_created_at', 'created_at'),
    )


class Run(Base):
    """å®Ÿè¡Œãƒ†ãƒ¼ãƒ–ãƒ«"""
    __tablename__ = 'runs'
    
    run_id = Column(BigInteger, primary_key=True, autoincrement=True)
    run_uuid = Column(UUID(as_uuid=True), nullable=False, unique=True, default=uuid.uuid4)
    experiment_id = Column(BigInteger, ForeignKey('experiments.experiment_id', ondelete='CASCADE'), nullable=False)
    parent_run_id = Column(BigInteger, ForeignKey('runs.run_id', ondelete='SET NULL'))
    run_name = Column(String(255), nullable=False)
    run_description = Column(Text)
    run_fingerprint = Column(String(64), nullable=False)
    status = Column(String(50), nullable=False, default='running')
    lifecycle_stage = Column(String(50), nullable=False, default='active')
    start_time = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp())
    end_time = Column(TIMESTAMP)
    execution_duration_seconds = Column(Numeric(12, 3))
    source_type = Column(String(50))
    source_name = Column(Text)
    git_commit_hash = Column(String(40))
    git_branch = Column(String(255))
    user_id = Column(String(255))
    tags = Column(JSONB, nullable=False, server_default='{}')
    metadata = Column('metadata', JSONB, nullable=False, server_default='{}')
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp(), onupdate=func.current_timestamp())
    deleted_at = Column(TIMESTAMP)
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—
    experiment = relationship("Experiment", back_populates="runs")
    models = relationship("Model", back_populates="run", cascade="all, delete-orphan")
    metrics = relationship("Metric", back_populates="run", cascade="all, delete-orphan")
    hyperparameters = relationship("Hyperparameter", back_populates="run", cascade="all, delete-orphan")
    artifacts = relationship("Artifact", back_populates="run", cascade="all, delete-orphan")
    
    # åˆ¶ç´„
    __table_args__ = (
        CheckConstraint(status.in_(['running', 'completed', 'failed', 'killed', 'scheduled']), name='ck_runs_status'),
        UniqueConstraint('experiment_id', 'run_fingerprint', name='uq_run_fingerprint'),
        Index('idx_runs_experiment_id', 'experiment_id'),
        Index('idx_runs_fingerprint', 'run_fingerprint'),
        Index('idx_runs_status', 'status'),
    )


class Model(Base):
    """ãƒ¢ãƒ‡ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«"""
    __tablename__ = 'models'
    
    model_id = Column(BigInteger, primary_key=True, autoincrement=True)
    model_uuid = Column(UUID(as_uuid=True), nullable=False, unique=True, default=uuid.uuid4)
    run_id = Column(BigInteger, ForeignKey('runs.run_id', ondelete='CASCADE'), nullable=False)
    model_name = Column(String(255), nullable=False)
    model_type = Column(String(100), nullable=False)
    model_version = Column(String(50), nullable=False, default='1.0.0')
    framework = Column(String(50), nullable=False)
    framework_version = Column(String(50))
    model_config = Column(JSONB, nullable=False)
    hyperparameters = Column(JSONB, nullable=False, server_default='{}')
    training_dataset_id = Column(BigInteger, ForeignKey('datasets.dataset_id'))
    training_samples = Column(Integer)
    training_duration_seconds = Column(Numeric(12, 3))
    model_size_bytes = Column(BigInteger)
    model_file_path = Column(Text, nullable=False)
    checkpoint_path = Column(Text)
    status = Column(String(50), nullable=False, default='training')
    tags = Column(JSONB, nullable=False, server_default='{}')
    metadata = Column('metadata', JSONB, nullable=False, server_default='{}')
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.current_timestamp(), onupdate=func.current_timestamp())
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—
    run = relationship("Run", back_populates="models")
    registry = relationship("ModelRegistry", back_populates="model", uselist=False)
    
    # åˆ¶ç´„
    __table_args__ = (
        CheckConstraint(status.in_(['training', 'trained', 'evaluating', 'evaluated', 'deployed', 'archived', 'failed']), 
                       name='ck_models_status'),
        Index('idx_models_run_id', 'run_id'),
        Index('idx_models_type', 'model_type'),
    )
```

---

### 10.2 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from pathlib import Path

def init_database(database_url: str):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
    
    # ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
    engine = create_engine(database_url, echo=True)
    Session = sessionmaker(bind=engine)
    
    # ã‚¹ã‚­ãƒ¼ãƒžä½œæˆ
    with engine.begin() as conn:
        conn.execute(text("CREATE SCHEMA IF NOT EXISTS audit;"))
        conn.execute(text("CREATE SCHEMA IF NOT EXISTS staging;"))
        conn.execute(text("CREATE SCHEMA IF NOT EXISTS archive;"))
    
    # æ‹¡å¼µæ©Ÿèƒ½ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    with engine.begin() as conn:
        conn.execute(text("CREATE EXTENSION IF NOT EXISTS pg_stat_statements;"))
        conn.execute(text("CREATE EXTENSION IF NOT EXISTS pgcrypto;"))
        conn.execute(text("CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";"))
    
    # Alembicãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    import subprocess
    subprocess.run(["alembic", "upgrade", "head"], check=True)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
    with Session() as session:
        # system_config
        from models import SystemConfig
        default_configs = [
            SystemConfig(config_key='max_parallel_runs', config_value='10', config_type='integer'),
            SystemConfig(config_key='default_retention_days', config_value='90', config_type='integer'),
            SystemConfig(config_key='enable_mlflow', config_value='true', config_type='boolean'),
        ]
        session.add_all(default_configs)
        session.commit()
    
    print("Database initialization completed successfully!")

if __name__ == "__main__":
    database_url = os.getenv('DATABASE_URL', 'postgresql://postgres:password@localhost/ts_forecast_system')
    init_database(database_url)
```

---

### 10.3 ç”¨èªžé›†

| ç”¨èªž | èª¬æ˜Ž |
|-----|------|
| **B-Tree** | å¹³è¡¡æœ¨æ§‹é€ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚ç¯„å›²æ¤œç´¢ã«é©ã—ã¦ã„ã‚‹ |
| **GIN** | Generalized Inverted Indexã€‚JSONBã€é…åˆ—ã€å…¨æ–‡æ¤œç´¢ã«ä½¿ç”¨ |
| **MVCC** | Multi-Version Concurrency Controlã€‚PostgreSQLã®ä¸¦è¡Œåˆ¶å¾¡æ–¹å¼ |
| **WAL** | Write-Ahead Loggingã€‚ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãƒ­ã‚° |
| **VACUUM** | ä¸è¦ãªã‚¿ãƒ—ãƒ«ã®å‰Šé™¤ã¨ãƒ‡ã‚£ã‚¹ã‚¯é ˜åŸŸã®å›žåŽ |
| **ANALYZE** | ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–° |
| **REINDEX** | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†æ§‹ç¯‰ |
| **PITR** | Point-In-Time Recoveryã€‚ç‰¹å®šæ™‚ç‚¹ã¸ã®å¾©å…ƒ |
| **RLS** | Row Level Securityã€‚è¡Œãƒ¬ãƒ™ãƒ«ã®ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ |
| **Partition** | ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¤‡æ•°ã®ç‰©ç†çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«ã«åˆ†å‰² |

---

## ã¾ã¨ã‚

æœ¬ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆæ›¸ã§ã¯ã€æ™‚ç³»åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã«å¿…è¦ãªä»¥ä¸‹ã‚’å®šç¾©ã—ã¾ã—ãŸï¼š

### âœ… ä¸»è¦æˆæžœç‰©

1. **15ã®ã‚³ã‚¢ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©**
   - å®Ÿé¨“ç®¡ç†ï¼ˆexperiments, runsï¼‰
   - ãƒ¢ãƒ‡ãƒ«ç®¡ç†ï¼ˆmodels, model_registryï¼‰
   - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†ï¼ˆdatasets, dataset_featuresï¼‰
   - è©•ä¾¡æŒ‡æ¨™ï¼ˆmetrics, metric_summariesï¼‰
   - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆhyperparameters, hyperparameter_search_spacesï¼‰
   - ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆartifactsï¼‰
   - äºˆæ¸¬çµæžœï¼ˆpredictionsï¼‰
   - ãƒ­ã‚°ãƒ»ç›£æŸ»ï¼ˆsystem_logs, audit_logsï¼‰
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ï¼ˆusers, user_experimentsï¼‰
   - è¨­å®šï¼ˆsystem_config, tagsï¼‰

2. **åŒ…æ‹¬çš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ**
   - B-Treeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆç¯„å›²æ¤œç´¢ãƒ»ã‚½ãƒ¼ãƒˆï¼‰
   - GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆJSONBæ¤œç´¢ï¼‰
   - éƒ¨åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆç‰¹å®šæ¡ä»¶ã®æœ€é©åŒ–ï¼‰
   - è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆè¤‡æ•°ã‚«ãƒ©ãƒ æ¤œç´¢ï¼‰

3. **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥**
   - æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å››åŠæœŸãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
   - ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœˆæ¬¡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
   - è‡ªå‹•ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†

4. **å®Œå…¨ãªãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»**
   - Alembicè¨­å®š
   - ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   - ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †

5. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**
   - ã‚¯ã‚¨ãƒªæœ€é©åŒ–
   - æŽ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
   - ãƒžãƒ†ãƒªã‚¢ãƒ©ã‚¤ã‚ºãƒ‰ãƒ“ãƒ¥ãƒ¼

6. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ãƒªã‚¹ãƒˆã‚¢æˆ¦ç•¥**
   - ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ—¥æ¬¡ï¼‰
   - ç¶™ç¶šçš„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆWALï¼‰
   - PITRå¯¾å¿œ

7. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–**
   - ãƒ­ãƒ¼ãƒ«ãƒ»æ¨©é™ç®¡ç†
   - è¡Œãƒ¬ãƒ™ãƒ«ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
   - ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–
   - ç›£æŸ»ãƒ­ã‚°

8. **é‹ç”¨æ‰‹é †**
   - æ—¥æ¬¡ãƒ»é€±æ¬¡ãƒ»æœˆæ¬¡ã‚¿ã‚¹ã‚¯
   - ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
   - ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

---

**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®ç‰¹å¾´**:

- âœ… **æ‹¡å¼µæ€§**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
- âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹**: é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ
- âœ… **å¯ç”¨æ€§**: åŒ…æ‹¬çš„ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥
- âœ… **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: å¤šå±¤é˜²å¾¡ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ä¿è­·
- âœ… **ä¿å®ˆæ€§**: æ˜Žç¢ºãªé‹ç”¨æ‰‹é †ã¨ãƒ„ãƒ¼ãƒ«

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:

1. âœ… æœ¬è¨­è¨ˆæ›¸ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼
2. ðŸ“ Alembicãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
3. ðŸ§ª ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ
4. ðŸ”§ SQLAlchemy ORMãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
5. ðŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿæ–½
6. ðŸš€ æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™

---

**End of Document**
